{"cells":[{"cell_type":"markdown","metadata":{"id":"E8gl7muuoKRG"},"source":["# playground_s5e1"]},{"cell_type":"markdown","metadata":{"id":"dbDiLvhEokXA"},"source":["# ライブラリのインポート"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51203,"status":"ok","timestamp":1739942853464,"user":{"displayName":"金城雄太","userId":"17611660587199306335"},"user_tz":-540},"id":"kFTumYtrbSPp","outputId":"6c8c40e7-c1e2-448b-e3cf-c3f37a9e50ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 MB 17.4 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/4.1 MB 56.6 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 383.6/383.6 kB 20.6 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205.7/205.7 kB 12.0 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.0/45.0 kB 2.3 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.6/233.6 kB 15.4 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 5.1 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.6/97.6 kB 2.9 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.7/85.7 kB 3.6 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.9/228.9 kB 6.6 MB/s eta 0:00:00\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 4.5 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 8.8 MB/s eta 0:00:00\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 5.6 MB/s eta 0:00:00\n"]}],"source":["%%bash\n","pip uninstall lightgbm --yes\n","pip install lightgbm \\\n","    --no-binary lightgbm \\\n","    --no-cache lightgbm \\\n","    --config-settings=cmake.define.USE_CUDA=ON\n","\n","pip install catboost -qq\n","pip install seaborn_qqplot -qq\n","pip install japanize-matplotlib optuna mojimoji optuna pymysql -qq\n","pip install optuna -qq\n","pip install optuna-integration -qq\n","pip install category_encoders -qq\n","pip install skorch -qq\n","pip install shirokumas -qq\n","pip install shap -qq\n","pip install jaconv -qq\n","pip install hydra-core -qq\n","pip install pyyaml -qq"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11937,"status":"ok","timestamp":1739942869263,"user":{"displayName":"金城雄太","userId":"17611660587199306335"},"user_tz":-540},"id":"9r0NFWznonoI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7fe4c3ab-df45-4287-8fb1-3a910c8b9aa2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import sklearn as sk\n","import pickle\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from seaborn_qqplot import pplot\n","import catboost\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import log_loss\n","import functools\n","import datetime\n","from dateutil.parser import parse\n","import japanize_matplotlib\n","import optuna\n","import mojimoji as mjmj\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import xgboost as xgb\n","import lightgbm as lgb\n","import category_encoders as ce\n","from pathlib import Path\n","import os\n","import re\n","import pickle\n","import math\n","from math import sqrt\n","from sklearn import preprocessing\n","from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler ,LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.pipeline import Pipeline, make_pipeline\n","import skorch\n","import torch\n","from torch import nn\n","from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from skorch import NeuralNetRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import RFECV\n","import jaconv\n","import unicodedata\n","import hydra\n","import sys\n","from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor\n","from skorch import NeuralNetRegressor\n","from catboost import CatBoostRegressor\n","from omegaconf import OmegaConf, DictConfig\n","import yaml"]},{"cell_type":"markdown","metadata":{"id":"asVck4Tr6K1Z"},"source":["# 生データのデータフレーム化"]},{"cell_type":"code","source":["import pandas as pd\n","base_path = \"/content/drive/MyDrive/kaggle/playground_s5e1\"\n","row_tr_df = pd.read_csv(f\"{base_path}/data/train.csv\")\n","row_test_df = pd.read_csv(f\"{base_path}/data/test.csv\")\n","row_sample_df = pd.read_csv(f\"{base_path}/data/sample_submission.csv\", header=None)\n","\n","tr_df = row_tr_df.copy()\n","test_df = row_test_df.copy()\n","\n","for col in tr_df.keys():\n","    col_l = col.lower()\n","    tr_df.rename(columns={col: col_l}, inplace=True)\n","    test_df.rename(columns={col: col_l}, inplace=True)\n","\n","tr_df_cp = tr_df.copy()\n","test_df_cp = test_df.copy()"],"metadata":{"id":"1ar2_qiFRdBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_na(inp_df):\n","    df = inp_df.copy()\n","    for col in df.keys():\n","        global tmp_df\n","        print(col)\n","        tmp_lst = list(df[col].unique())\n","        tmp_df = pd.DataFrame(tmp_lst)\n","\n","        if tmp_df.isna().any()[0]:\n","            print(\"NaNあり\")\n","            n_nan= df[col].isna().sum()\n","            print(f\"NaNは{n_nan}個あります。\")\n","        else:\n","            print(\"NaNなし\")\n","        print(\"------------------\\n\")\n","\n","\n","def check_and_clean_na(inp_df, fill_na=False):\n","    df = inp_df.copy()\n","    for col in df.keys():\n","        global tmp_df\n","        print(col)\n","        tmp_lst = list(df[col].unique())\n","        tmp_df = pd.DataFrame(tmp_lst)\n","\n","        if tmp_df.isna().any()[0]:\n","            print(\"NaNあり\")\n","            if fill_na:\n","                if isinstance(df[col][0], str):\n","                    df[col] = df[col].fillna(df[col].mode()[0])\n","                else:\n","                    df[col] = df[col].fillna(df[col].mean())\n","            else:\n","                df = df.dropna(subset=[col])\n","        else:\n","            print(\"NaNなし\")\n","        print(\"------------------\\n\")\n","\n","    return df"],"metadata":{"id":"8XFpGZM3NEka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_df = tr_df_cp.copy()\n","test_df = test_df_cp.copy()\n","tr_df = check_and_clean_na(tr_df).reset_index(drop=True)\n","test_df = check_and_clean_na(test_df, fill_na=True).reset_index(drop=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Af74-VpVNEdR","executionInfo":{"status":"ok","timestamp":1738306727530,"user_tz":-540,"elapsed":799,"user":{"displayName":"金城雄太","userId":"17611660587199306335"}},"outputId":"20d3f257-ed2f-4df8-fd50-69831a73620d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["id\n","NaNなし\n","------------------\n","\n","date\n","NaNなし\n","------------------\n","\n","country\n","NaNなし\n","------------------\n","\n","store\n","NaNなし\n","------------------\n","\n","product\n","NaNなし\n","------------------\n","\n","num_sold\n","NaNあり\n","------------------\n","\n","id\n","NaNなし\n","------------------\n","\n","date\n","NaNなし\n","------------------\n","\n","country\n","NaNなし\n","------------------\n","\n","store\n","NaNなし\n","------------------\n","\n","product\n","NaNなし\n","------------------\n","\n"]}]},{"cell_type":"markdown","source":["# データの可視化"],"metadata":{"id":"dAwhWXGFbCgj"}},{"cell_type":"code","source":["def get_binned_data(x, col, bins=10, label_format='{:02}_{:.0f}-{:.0f}'):\n","\n","    # データ型チェック\n","    if type(x) not in (pd.Series, pd.DataFrame):\n","        x = pd.Series(x)\n","\n","    if x.isnull().values.any():\n","        print(col)\n","        print(x.unique())\n","        raise ValueError(f\"{col}にNaNが含まれています。NaNを削除してください。\")\n","\n","    uniq_type = type(x[0])\n","\n","    # ラベルが文字型の場合\n","    if uniq_type is str:\n","        binned_x = x\n","        return binned_x, \"notbinned\"\n","\n","    # ラベルが数字でunique数が10未満の場合は、文字型に変換する\n","    if len(x.unique()) < 6:\n","        binned_x = pd.Series([str(val) for val in x])\n","        return binned_x, \"notbinned\"\n","    else:\n","        if type(bins) is int:\n","            binned_value, bin_def = pd.qcut(x, bins, retbins=True, duplicates='drop')\n","        else:\n","            bin_def = bins\n","\n","\n","        labels = [label_format.format(i, bin_def[i], bin_def[i+1]) for i in range(len(bin_def)-1)]\n","\n","        if type(bins) is int:\n","            binned_x = pd.qcut(x, bins, labels=labels, duplicates='drop')\n","        else:\n","            binned_x = pd.cut(x, bins, labels=labels)\n","\n","        binned_x = pd.Series([str(val) for val in binned_x])\n","\n","        return binned_x, \"binned\"\n","\n","def meta_subplots(row_num, col_num=3):\n","    dpi = 100\n","    fig_x = 10\n","    fig_y = 8\n","\n","    if row_num * col_num == 1:\n","        dpi = 70\n","        fig_x = 5\n","        fig_y = 5\n","\n","    if row_num * col_num == 1:\n","        dpi = dpi\n","        fig_x = fig_x\n","        fig_y = fig_y\n","    elif row_num * col_num <= 3:\n","        dpi = dpi*1.25\n","        fig_x = fig_x*1.25\n","        fig_y = fig_y*1.25\n","\n","    elif row_num * col_num <= 6:\n","        dpi = dpi*1.5\n","        fig_x = fig_x*1.5\n","        fig_y = fig_y*1.5\n","\n","    elif row_num * col_num <= 9:\n","        dpi = dpi*1.75\n","        fig_x = fig_x*1.75\n","        fig_y = fig_y*1.75\n","\n","    elif row_num * col_num <= 12:\n","        dpi = dpi*2\n","        fig_x = fig_x*2\n","        fig_y = fig_y*2\n","\n","\n","    fig, axes = plt.subplots(row_num, col_num ,dpi = dpi,\n","    facecolor = \"white\",\n","    edgecolor = \"black\",\n","    linewidth= 15,\n","    figsize=(fig_x, fig_y))\n","    return fig, axes\n","\n","def cnvrt_hist(tr_df, test_df, trgt_y, trgt_cols, row_num, col_num, bins=10, return_df=False, show_fig=True):\n","    bin_values = {}\n","    notbinned_cols = []\n","    binned_cols = []\n","    tr_df_cp = tr_df.copy()\n","    test_df_cp = test_df.copy()\n","\n","    id_lst = list(tr_df_cp.id.values)\n","    id_lst += list(test_df_cp.id.values)\n","    id_lst = sorted(id_lst)\n","    add_binned_df = pd.DataFrame(id_lst, columns=[\"id\"])\n","    fig, axes = meta_subplots(row_num, col_num)\n","    if row_num * col_num > 1:\n","        axes = axes.flatten()\n","    if len(trgt_cols) > 0:\n","        cols = trgt_cols.copy()\n","    else:\n","        cols = tr_df.columns\n","\n","    if \"id\" in cols:\n","        cols = [col for col in cols if col != \"id\"]\n","\n","    for i, col in enumerate(cols):\n","        plot_map = {}\n","        if i == len(cols):\n","            break;\n","        if col == trgt_y:\n","            continue;\n","        # ビン分割\n","        tr_endid = tr_df_cp.index[-1]\n","        all_df = pd.concat([tr_df_cp, test_df_cp], ignore_index=True)\n","        all_values = all_df[col]\n","        all_binned_values, is_binned = get_binned_data(all_values, col, bins)\n","        if is_binned == \"binned\":\n","            print(col)\n","            binned_cols.append(col)\n","            add_binned_df[[f\"binned_{col}\"]] = all_binned_values\n","        all_df[f\"binned_{col}\"] = all_binned_values.values\n","        tr_binned_values = all_binned_values[:tr_endid+1]\n","        test_binned_values = all_binned_values[tr_endid+1:]\n","        tr_plot_data = tr_binned_values.value_counts() / tr_df_cp.shape[0]\n","        tr_plot_data = tr_plot_data.to_dict()\n","        tr_plot_data = sorted(tr_plot_data.items())\n","        tr_plot_data = pd.DataFrame(tr_plot_data, columns=[f\"binned_{col}\", \"train_val_rate\"])\n","        tr_plot_data = tr_plot_data.set_index(f\"binned_{col}\")\n","        test_plot_data = test_binned_values.value_counts() / test_df_cp.shape[0]\n","        test_plot_data = test_plot_data.to_dict()\n","        test_plot_data = sorted(test_plot_data.items())\n","        test_plot_data = pd.DataFrame(test_plot_data, columns=[f\"binned_{col}\", \"test_val_rate\"])\n","        test_plot_data = test_plot_data.set_index(f\"binned_{col}\")\n","        tr_plot_data = tr_plot_data.fillna(0)\n","        test_plot_data = test_plot_data.fillna(0)\n","        if col != trgt_y:\n","            tmp_df = all_df[[col, f\"binned_{col}\"]].copy()\n","            y_rate_df = all_df[[col, trgt_y, f\"binned_{col}\"]][:tr_endid+1].copy()\n","            y_rate_df[trgt_y] = y_rate_df[trgt_y].apply(lambda x: int(x) if not pd.isna(x) else x)\n","            # .loc[: trgt]で置換すると強制的にcategory型になる！！！\n","            y_rate_df[trgt_y] = y_rate_df[trgt_y].astype('float64')\n","            y_rate_df = y_rate_df.groupby(f\"binned_{col}\")[trgt_y].mean().reset_index()\n","            y_rate_df.rename(columns={f\"{trgt_y}\": f\"{trgt_y}_rate\"}, inplace=True)\n","            y_rate_df.set_index(f\"binned_{col}\", inplace=True)\n","            tr_plot_data = pd.merge(tr_plot_data, y_rate_df, left_index=True, right_index=True, how='left')\n","            tr_plot_data[f\"{trgt_y}_rate\"] = tr_plot_data[f\"{trgt_y}_rate\"].fillna(0)\n","        w = 0.4\n","        x_axis_labels = [i for i in range(len(tr_plot_data.index.tolist()))]\n","        if row_num * col_num > 1:\n","            ax = axes[i]\n","        else:\n","            ax = axes\n","        ax_tr_dict = tr_plot_data[[\"train_val_rate\"]].to_dict()[\"train_val_rate\"]\n","        ax_test_dict = test_plot_data[[\"test_val_rate\"]].to_dict()[\"test_val_rate\"]\n","        ax2_tr_dict = tr_plot_data[[f\"{trgt_y}_rate\"]].to_dict()[f\"{trgt_y}_rate\"]\n","        ax.bar(x_axis_labels, ax_tr_dict.values(), width=w, color='red')\n","        plt_test_x = test_plot_data.index.tolist()\n","        tmp_plt_test_x = [i for i in range(len(plt_test_x))]\n","        tmp_plt_test_x = [int(x)+ w for x in tmp_plt_test_x]\n","        ax.bar(tmp_plt_test_x, ax_test_dict.values(), width=w, color='blue')\n","        plot_label_x = list(ax_tr_dict.keys())\n","        tmp_plot_label_x = [i for i in range(len(plot_label_x))]\n","        tmp_plot_label_x = [int(x)+ w/2 for x in tmp_plot_label_x]\n","        bin_values[col] = plot_label_x\n","        ax.set_xticks(tmp_plot_label_x)\n","        ax.set_xticklabels(plot_label_x, rotation=30)\n","        ax.xaxis.set_tick_params(direction='out', labelsize=7, width=3, pad=3)\n","        upper_y = test_plot_data[\"test_val_rate\"].max() + 0.1\n","        ax.set_ylim([0, upper_y])\n","        ax.yaxis.set_tick_params(direction='out', labelsize=7, width=1.5, pad=5)\n","        ax.set_xlabel(f\"{col}\", fontsize=7.5)\n","        ax.axes.xaxis.set_ticklabels([])\n","        ax2 = ax.twinx()\n","        ax2.plot(x_axis_labels, ax2_tr_dict.values(), marker='o', color='g')\n","        ax2.yaxis.set_tick_params(direction='out', labelsize=7, width=1.5, pad=5)\n","        upper_y =tr_plot_data[f\"{trgt_y}_rate\"].max() + 0.1\n","        ax2.set_ylim([0, upper_y])\n","        ax2.set_ylabel(\"\")\n","    fig.suptitle(\"Train_Test_Hist\", fontsize=7.5, y=0.95)\n","    plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.93, wspace=0.3, hspace=0.35)\n","    if row_num * col_num > 1:\n","        for i in range(len(cols), len(axes.flatten())):\n","                fig.delaxes(axes.flatten()[i])\n","    if not show_fig:\n","        plt.close(fig)\n","    if return_df:\n","        if len(binned_cols) > 0:\n","            tr_df = pd.merge(tr_df, add_binned_df, on=\"id\", how=\"left\")\n","            test_df = pd.merge(test_df, add_binned_df, on=\"id\", how=\"left\")\n","        return tr_df, test_df, bin_values\n","    else:\n","        return bin_values"],"metadata":{"id":"icVWL4XXNEWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trgt_y = 'num_sold'\n","trgt_cols = []\n","tmp_tr, tmp_test, bin_values = cnvrt_hist_encd(tr_df=tr_df, test_df=test_df, trgt_y=trgt_y, trgt_cols=trgt_cols, row_num=5, col_num=4, bins=10, return_df=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":319},"id":"uZBPkM-a4dkr","executionInfo":{"status":"ok","timestamp":1737445592965,"user_tz":-540,"elapsed":12979,"user":{"displayName":"金城雄太","userId":"17611660587199306335"}},"outputId":"3d42fe38-32e7-470c-f41e-ebc5b7ea0446"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x800 with 9 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# 特徴量エンジニアリング"],"metadata":{"id":"n-YWXpt_EQ8W"}},{"cell_type":"code","source":["def clean_date(s):\n","    year, month, day = s.split(\"-\")\n","    year = int(year)\n","    month = int(month)\n","    day = int(day)\n","    return year, month, day\n","\n","tr_df[[\"year\", \"month\", \"day\"]] = tr_df[\"date\"].apply(clean_date).apply(pd.Series)\n","test_df[[\"year\", \"month\", \"day\"]] = test_df[\"date\"].apply(clean_date).apply(pd.Series)\n","\n","def add_season(s):\n","    if s in [12, 1, 2]:\n","        s = \"winter\"\n","    elif s in [3, 4, 5]:\n","        s = \"spring\"\n","    elif s in [3, 4, 5]:\n","        s = \"summer\"\n","    else:\n","        s = \"autumn\"\n","    return s\n","\n","def add_sin_cos_col(df, col, max_val):\n","    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n","    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n","    return df\n","\n","tr_df = add_sin_cos_col(tr_df, \"month\", 12)\n","test_df = add_sin_cos_col(test_df, \"month\", 12)\n","tr_df = add_sin_cos_col(tr_df,\"day\", 31)\n","test_df = add_sin_cos_col(test_df,\"day\", 31)\n","\n","tr_df[\"season\"] = tr_df[\"month\"].apply(add_season).apply(pd.Series)\n","tr_df['weekday'] = pd.to_datetime(tr_df['date']).dt.dayofweek\n","test_df[\"season\"] = test_df[\"month\"].apply(add_season).apply(pd.Series)\n","test_df['weekday'] = pd.to_datetime(test_df['date']).dt.dayofweek\n","tr_df = tr_df.sort_values(by=[\"country\", \"store\", \"product\", \"year\", \"month\", \"day\"], ascending=True).reset_index(drop=True)\n","test_df = test_df.sort_values(by=[\"country\", \"store\", \"product\", \"year\", \"month\", \"day\"], ascending=True).reset_index(drop=True)"],"metadata":{"id":"jSC7c832aAzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_tr_df_1 = tr_df.copy()\n","_test_df_1 = test_df.copy()"],"metadata":{"id":"lNeNLI03YJYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_df[\"lag_1_num_sold\"] = tr_df.groupby([\"country\", \"store\", \"product\"])[\"num_sold\"].shift(1)\n","tr_df[\"lag_7_num_sold\"] = tr_df.groupby([\"country\", \"store\", \"product\"])[\"num_sold\"].shift(7)\n","tr_df[\"lag_365_num_sold\"] = tr_df.groupby([\"country\", \"store\", \"product\"])[\"num_sold\"].shift(365)\n","tr_df[\"rolling_mean_num_sold_7\"] = tr_df.groupby([\"country\", \"store\", \"product\"])[\"num_sold\"].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n","tr_df[\"rolling_mean_num_sold_30\"] = tr_df.groupby([\"country\", \"store\", \"product\"])[\"num_sold\"].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n","tr_df[\"rolling_mean_num_sold_365\"] = tr_df.groupby([\"country\", \"store\", \"product\"])[\"num_sold\"].transform(lambda x: x.rolling(window=365, min_periods=1).mean())\n","\n","tr_df[\"lag_1_num_sold\"] = tr_df[\"lag_1_num_sold\"].fillna(tr_df[\"lag_1_num_sold\"].mean())\n","tr_df[\"lag_7_num_sold\"] = tr_df[\"lag_7_num_sold\"].fillna(tr_df[\"lag_7_num_sold\"].mean())\n","tr_df[\"lag_365_num_sold\"] = tr_df[\"lag_365_num_sold\"].fillna(tr_df[\"lag_365_num_sold\"].mean())\n","tr_df[\"rolling_mean_num_sold_7\"] = tr_df[\"rolling_mean_num_sold_7\"].fillna(tr_df[\"rolling_mean_num_sold_7\"].mean())\n","tr_df[\"rolling_mean_num_sold_30\"] = tr_df[\"rolling_mean_num_sold_30\"].fillna(tr_df[\"rolling_mean_num_sold_30\"].mean())\n","tr_df[\"rolling_mean_num_sold_365\"] = tr_df[\"rolling_mean_num_sold_365\"].fillna(tr_df[\"rolling_mean_num_sold_365\"].mean())\n"],"metadata":{"id":"glLxM3SsFEoo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_lags_test(inp_tr_df, inp_test_df, lag_col):\n","    tr_df = inp_tr_df.copy()\n","    test_df = inp_test_df.copy()\n","    a_df = tr_df.groupby([\"country\", \"store\", \"product\"])[f\"{lag_col}\"].agg([\"mean\", \"std\"]).reset_index(drop=True)\n","    b_df = tr_df[[\"country\", \"store\", \"product\"]].drop_duplicates(ignore_index=True)\n","    c_df = pd.concat([a_df, b_df], axis=1)\n","    test_df = pd.merge(test_df, c_df, on=[\"country\", \"store\", \"product\"], how=\"left\")\n","    test_df[\"mean\"] = test_df[\"mean\"].fillna(test_df[\"mean\"].mean())\n","    test_df[\"std\"] = test_df[\"std\"].fillna(test_df[\"mean\"].mean())\n","    test_df[f\"{lag_col}\"] = test_df.apply(\n","        lambda row: np.random.normal(row[\"mean\"], row[\"std\"], size=1)[0], axis=1).apply(lambda x: round(abs(x)))\n","    test_df.drop([\"mean\", \"std\"], axis=1, inplace=True)\n","    return test_df"],"metadata":{"id":"cp5CexT93Uws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_df = _tr_df.copy()\n","test_df = _test_df.copy()"],"metadata":{"id":"guejxAxrxxnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for lag_col in [\"lag_1_num_sold\", \"lag_7_num_sold\", \"lag_365_num_sold\", \"rolling_mean_num_sold_7\", \"rolling_mean_num_sold_30\", \"rolling_mean_num_sold_365\"]:\n","    test_df = add_lags_test(tr_df, test_df, lag_col)"],"metadata":{"id":"zLSHHdDwzSnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_trgtencd(inp_tr_df, inp_test_df, grby_cols, trgt_col):\n","\n","    # inp_tr_dfのkeyの結合\n","    if isinstance(grby_cols, str):\n","        grby_cols = [grby_cols]\n","    tr_df = inp_tr_df.copy()\n","    test_df = inp_test_df.copy()\n","    df = tr_df.groupby(grby_cols)[f\"{trgt_col}\"].mean()\n","    if len(grby_cols) > 0:\n","        grby_cols_key =   \"_\".join(grby_cols)\n","    else:\n","        grby_cols_key = grby_cols[0]\n","    df = df.rename(f\"{grby_cols_key}_mean_{trgt_col}\")\n","    df_index = df.index\n","    df_2 = pd.DataFrame(df.values, columns = [f\"{grby_cols_key}_mean_{trgt_col}\"])\n","    df_2[\"tmp_key\"] = df_index\n","    df_2[\"tmp_key\"] = df_2[\"tmp_key\"].apply(lambda x: \"_\".join(list(x)))\n","    tr_df[\"tmp_key\"] = tr_df[grby_cols].agg(\"_\".join, axis=1)\n","    test_df[\"tmp_key\"] = test_df[grby_cols].agg(\"_\".join, axis=1)\n","    tr_df = pd.merge(tr_df, df_2, on=[\"tmp_key\"], how='left')\n","    test_df = pd.merge(test_df, df_2, on=[\"tmp_key\"], how='left')\n","    tr_df[f\"{grby_cols_key}_mean_{trgt_col}\"] = tr_df[f\"{grby_cols_key}_mean_{trgt_col}\"].fillna(tr_df[f\"{grby_cols_key}_mean_{trgt_col}\"].mean())\n","    test_df[f\"{grby_cols_key}_mean_{trgt_col}\"] = test_df[f\"{grby_cols_key}_mean_{trgt_col}\"].fillna(test_df[f\"{grby_cols_key}_mean_{trgt_col}\"].mean())\n","    tr_df.drop(\"tmp_key\", axis=1, inplace=True)\n","    test_df.drop(\"tmp_key\", axis=1, inplace=True)\n","\n","    return tr_df, test_df"],"metadata":{"id":"qvr2YrfR1C3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cate_cols = [\"date\", \"country\", \"store\", \"product\", \"year\", \"month\", \"day\", \"season\", \"weekday\"]\n","tr_df[cate_cols] = tr_df[cate_cols].astype(\"category\")\n","test_df[cate_cols] = test_df[cate_cols].astype(\"category\")\n","\n","def cnvrt_cate_col_val(df, cate_cols):\n","    for col in cate_cols:\n","        df[col] = df[col].apply(lambda x: str(x) if not isinstance(x, str) else x)\n","    return df\n","\n","tr_df = cnvrt_cate_col_val(tr_df, cate_cols)\n","test_df = cnvrt_cate_col_val(test_df, cate_cols)"],"metadata":{"id":"70ow0VjiwDY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_df, test_df = add_trgtencd(tr_df, test_df, [\"country\", \"store\", \"product\"], \"num_sold\")"],"metadata":{"id":"JbGEO45F-HiV","executionInfo":{"status":"ok","timestamp":1738306809295,"user_tz":-540,"elapsed":1325,"user":{"displayName":"金城雄太","userId":"17611660587199306335"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"286dfa0a-2a42-4b61-ab80-deb073a3e579"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-eee980e65fd3>:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  df = tr_df.groupby(grby_cols)[f\"{trgt_col}\"].mean()\n"]}]},{"cell_type":"code","source":["tr_df, test_df = add_trgtencd(tr_df, test_df, [\"country\", \"store\", \"season\"], \"num_sold\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7vjG6CTnSQ4","executionInfo":{"status":"ok","timestamp":1738306812263,"user_tz":-540,"elapsed":1157,"user":{"displayName":"金城雄太","userId":"17611660587199306335"}},"outputId":"1405166d-a3f3-4ced-9858-52ddcf154ca0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-eee980e65fd3>:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  df = tr_df.groupby(grby_cols)[f\"{trgt_col}\"].mean()\n"]}]},{"cell_type":"code","source":["exp_id = 2\n","base_path = Path(f\"/content/drive/MyDrive/kaggle/playground_s5e1/\")\n","tmp_dir = base_path / \"data/cleaned_data\"\n","tmp_dir.mkdir(parents=True, exist_ok=True)\n","with open(base_path / f\"data/cleaned_data/cleaned_tr_df_exp_{exp_id}.pickle\", 'wb') as f:\n","    pickle.dump(tr_df, f)\n","with open(base_path / f\"data/cleaned_data/cleaned_test_df_exp_{exp_id}.pickle\", 'wb') as f:\n","    pickle.dump(test_df, f)"],"metadata":{"id":"CmfRHUym1WXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 学習の実行"],"metadata":{"id":"6IcmTP5PbUeq"}},{"cell_type":"code","source":["! python3 /content/drive/MyDrive/kaggle/playground_s5e1/utility/main.py 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoJZuFyH1kNB","executionInfo":{"status":"ok","timestamp":1739947365581,"user_tz":-540,"elapsed":38115,"user":{"displayName":"金城雄太","userId":"17611660587199306335"}},"outputId":"cdef9370-66b6-4bc6-ed40-267e3cc56a9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.82167\tvalidation_1-mae:66.81565\n","[9]\tvalidation_0-mae:54.86501\tvalidation_1-mae:55.39689\n","loss____: 0.07690477615701145\n","\u001b[32m[I 2025-02-19 06:42:12,370]\u001b[0m Trial 0 finished with value: 0.07690477615701145 and parameters: {}. Best is trial 0 with value: 0.07690477615701145.\u001b[0m\n","-------\n","0.07690477615701145\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.80439\tvalidation_1-mae:82.36919\n","[9]\tvalidation_0-mae:53.77243\tvalidation_1-mae:68.40461\n","loss____: 0.07299724941056963\n","\u001b[32m[I 2025-02-19 06:42:12,422]\u001b[0m Trial 0 finished with value: 0.07299724941056963 and parameters: {}. Best is trial 0 with value: 0.07299724941056963.\u001b[0m\n","-------\n","0.07299724941056963\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:12,502]\u001b[0m Trial 0 finished with value: 0.0743368402237952 and parameters: {'max_depth': 4, 'max_leaves': 96, 'learning_rate': 0.07587945476302646, 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 0 with value: 0.0743368402237952.\u001b[0m\n","xgbreg_TRIAL_1 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_2 strat...\n","\u001b[32m[I 2025-02-19 06:42:12,547]\u001b[0m A new study created in memory with name: no-name-13a28b21-1baa-44f4-a7ee-80476a6a9d84\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:12,547]\u001b[0m A new study created in memory with name: no-name-4d730024-eba7-4152-8270-ff56c9eddbe0\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:12,548]\u001b[0m A new study created in memory with name: no-name-c86ad912-ef9d-41c8-9afd-5954904fec3e\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'max_leaves': 64, 'learning_rate': 0.0737265320016441, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 1.0, 'colsample_bytree': 0.6}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.39819\tvalidation_1-mae:74.82862\n","[9]\tvalidation_0-mae:45.03078\tvalidation_1-mae:62.00832\n","loss____: 0.05987942261156586\n","\u001b[32m[I 2025-02-19 06:42:12,811]\u001b[0m Trial 0 finished with value: 0.05987942261156586 and parameters: {}. Best is trial 0 with value: 0.05987942261156586.\u001b[0m\n","-------\n","0.05987942261156586\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.43410\tvalidation_1-mae:72.82836\n","[9]\tvalidation_0-mae:45.02129\tvalidation_1-mae:59.62610\n","loss____: 0.0645944939686955\n","\u001b[32m[I 2025-02-19 06:42:12,870]\u001b[0m Trial 0 finished with value: 0.0645944939686955 and parameters: {}. Best is trial 0 with value: 0.0645944939686955.\u001b[0m\n","-------\n","0.0645944939686955\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.29225\tvalidation_1-mae:69.72052\n","[9]\tvalidation_0-mae:45.68513\tvalidation_1-mae:57.39347\n","loss____: 0.060973360128444704\n","\u001b[32m[I 2025-02-19 06:42:12,928]\u001b[0m Trial 0 finished with value: 0.060973360128444704 and parameters: {}. Best is trial 0 with value: 0.060973360128444704.\u001b[0m\n","-------\n","0.060973360128444704\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:12,995]\u001b[0m Trial 1 finished with value: 0.06181575890290202 and parameters: {'max_depth': 5, 'max_leaves': 64, 'learning_rate': 0.0737265320016441, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 1.0, 'colsample_bytree': 0.6}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_2 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_3 strat...\n","\u001b[32m[I 2025-02-19 06:42:13,047]\u001b[0m A new study created in memory with name: no-name-8c505fc4-4542-4603-a9da-6aa02f29604d\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:13,047]\u001b[0m A new study created in memory with name: no-name-6ee31bc8-c289-4405-98a8-6722f2cd03d2\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:13,047]\u001b[0m A new study created in memory with name: no-name-21fdde8f-d52e-44c3-a54e-debc322f5c06\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'max_leaves': 26, 'learning_rate': 0.0373818018663584, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.8}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.47919\tvalidation_1-mae:78.45423\n","[9]\tvalidation_0-mae:63.56278\tvalidation_1-mae:72.78529\n","loss____: 0.08530421326766426\n","\u001b[32m[I 2025-02-19 06:42:13,315]\u001b[0m Trial 0 finished with value: 0.08530421326766426 and parameters: {}. Best is trial 0 with value: 0.08530421326766426.\u001b[0m\n","-------\n","0.08530421326766426\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:76.23661\tvalidation_1-mae:73.56218\n","[9]\tvalidation_0-mae:64.03314\tvalidation_1-mae:68.73116\n","loss____: 0.09175889203553833\n","\u001b[32m[I 2025-02-19 06:42:13,366]\u001b[0m Trial 0 finished with value: 0.09175889203553833 and parameters: {}. Best is trial 0 with value: 0.09175889203553833.\u001b[0m\n","-------\n","0.09175889203553833\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:76.82162\tvalidation_1-mae:70.36561\n","[9]\tvalidation_0-mae:64.19333\tvalidation_1-mae:63.58335\n","loss____: 0.08412447478695186\n","\u001b[32m[I 2025-02-19 06:42:13,417]\u001b[0m Trial 0 finished with value: 0.08412447478695186 and parameters: {}. Best is trial 0 with value: 0.08412447478695186.\u001b[0m\n","-------\n","0.08412447478695186\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:13,493]\u001b[0m Trial 2 finished with value: 0.08706252669671814 and parameters: {'max_depth': 3, 'max_leaves': 26, 'learning_rate': 0.0373818018663584, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_3 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_4 strat...\n","\u001b[32m[I 2025-02-19 06:42:13,573]\u001b[0m A new study created in memory with name: no-name-804cdbb9-327a-4f3e-a63d-4e0b42fabda6\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:13,573]\u001b[0m A new study created in memory with name: no-name-237f0da2-d6f4-4b25-aa76-1a2c8cd5dae1\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:13,574]\u001b[0m A new study created in memory with name: no-name-60b7380b-2a79-4b4c-b8e3-3873de032a75\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'max_leaves': 36, 'learning_rate': 0.04297256589643226, 'reg_alpha': 0.5, 'reg_lambda': 0.6000000000000001, 'subsample': 0.6, 'colsample_bytree': 0.8}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:76.80054\tvalidation_1-mae:71.20322\n","[9]\tvalidation_0-mae:63.45293\tvalidation_1-mae:65.05384\n","loss____: 0.08446231386751246\n","\u001b[32m[I 2025-02-19 06:42:13,831]\u001b[0m Trial 0 finished with value: 0.08446231386751246 and parameters: {}. Best is trial 0 with value: 0.08446231386751246.\u001b[0m\n","-------\n","0.08446231386751246\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:77.38591\tvalidation_1-mae:64.70347\n","[9]\tvalidation_0-mae:62.89406\tvalidation_1-mae:57.92107\n","loss____: 0.0882969962758712\n","\u001b[32m[I 2025-02-19 06:42:13,881]\u001b[0m Trial 0 finished with value: 0.0882969962758712 and parameters: {}. Best is trial 0 with value: 0.0882969962758712.\u001b[0m\n","-------\n","0.0882969962758712\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.13417\tvalidation_1-mae:86.52013\n","[9]\tvalidation_0-mae:61.41999\tvalidation_1-mae:78.98577\n","loss____: 0.08253885536146258\n","\u001b[32m[I 2025-02-19 06:42:13,934]\u001b[0m Trial 0 finished with value: 0.08253885536146258 and parameters: {}. Best is trial 0 with value: 0.08253885536146258.\u001b[0m\n","-------\n","0.08253885536146258\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:14,025]\u001b[0m Trial 3 finished with value: 0.08509938850161541 and parameters: {'max_depth': 3, 'max_leaves': 36, 'learning_rate': 0.04297256589643226, 'reg_alpha': 0.5, 'reg_lambda': 0.6000000000000001, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_4 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_5 strat...\n","\u001b[32m[I 2025-02-19 06:42:14,074]\u001b[0m A new study created in memory with name: no-name-090efddd-dcd3-4a4b-832f-85695311ad03\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:14,074]\u001b[0m A new study created in memory with name: no-name-60bf45fb-d534-4033-9bcf-72d51c65d1d2\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:14,074]\u001b[0m A new study created in memory with name: no-name-51361382-aa80-446e-ac18-76f82e3167f2\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'max_leaves': 14, 'learning_rate': 0.06467903667112945, 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 1.0, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:71.92794\tvalidation_1-mae:81.47283\n","[9]\tvalidation_0-mae:45.03894\tvalidation_1-mae:72.81609\n","loss____: 0.06113494737418955\n","\u001b[32m[I 2025-02-19 06:42:14,314]\u001b[0m Trial 0 finished with value: 0.06113494737418955 and parameters: {}. Best is trial 0 with value: 0.06113494737418955.\u001b[0m\n","-------\n","0.06113494737418955\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.15162\tvalidation_1-mae:66.52176\n","[9]\tvalidation_0-mae:46.64258\tvalidation_1-mae:57.91257\n","loss____: 0.06613197529677213\n","\u001b[32m[I 2025-02-19 06:42:14,370]\u001b[0m Trial 0 finished with value: 0.06613197529677213 and parameters: {}. Best is trial 0 with value: 0.06613197529677213.\u001b[0m\n","-------\n","0.06613197529677213\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:71.73944\tvalidation_1-mae:82.96835\n","[9]\tvalidation_0-mae:45.21395\tvalidation_1-mae:74.60409\n","loss____: 0.0612489338742135\n","\u001b[32m[I 2025-02-19 06:42:14,426]\u001b[0m Trial 0 finished with value: 0.0612489338742135 and parameters: {}. Best is trial 0 with value: 0.0612489338742135.\u001b[0m\n","-------\n","0.0612489338742135\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:14,523]\u001b[0m Trial 4 finished with value: 0.06283861884839172 and parameters: {'max_depth': 4, 'max_leaves': 14, 'learning_rate': 0.06467903667112945, 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 1.0, 'colsample_bytree': 1.0}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_5 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_6 strat...\n","\u001b[32m[I 2025-02-19 06:42:14,577]\u001b[0m A new study created in memory with name: no-name-f2cf7518-e22a-4874-b000-cada0fa88948\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:14,577]\u001b[0m A new study created in memory with name: no-name-8f136415-7b16-49fd-9563-b4879b9b6330\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:14,578]\u001b[0m A new study created in memory with name: no-name-1a625197-a90f-4f77-9909-20659dba2c19\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'max_leaves': 37, 'learning_rate': 0.018790490260574548, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.7}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:76.72642\tvalidation_1-mae:74.82628\n","[9]\tvalidation_0-mae:69.25453\tvalidation_1-mae:71.36581\n","loss____: 0.09289115008872009\n","\u001b[32m[I 2025-02-19 06:42:14,826]\u001b[0m Trial 0 finished with value: 0.09289115008872009 and parameters: {}. Best is trial 0 with value: 0.09289115008872009.\u001b[0m\n","-------\n","0.09289115008872009\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.62225\tvalidation_1-mae:80.99137\n","[9]\tvalidation_0-mae:67.95418\tvalidation_1-mae:78.05178\n","loss____: 0.09862519315182895\n","\u001b[32m[I 2025-02-19 06:42:14,881]\u001b[0m Trial 0 finished with value: 0.09862519315182895 and parameters: {}. Best is trial 0 with value: 0.09862519315182895.\u001b[0m\n","-------\n","0.09862519315182895\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.02632\tvalidation_1-mae:84.94696\n","[9]\tvalidation_0-mae:67.73633\tvalidation_1-mae:81.65512\n","loss____: 0.0910480688847269\n","\u001b[32m[I 2025-02-19 06:42:14,936]\u001b[0m Trial 0 finished with value: 0.0910480688847269 and parameters: {}. Best is trial 0 with value: 0.0910480688847269.\u001b[0m\n","-------\n","0.0910480688847269\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:14,999]\u001b[0m Trial 5 finished with value: 0.09418813737509198 and parameters: {'max_depth': 5, 'max_leaves': 37, 'learning_rate': 0.018790490260574548, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_6 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_7 strat...\n","\u001b[32m[I 2025-02-19 06:42:15,042]\u001b[0m A new study created in memory with name: no-name-3e0408f2-30eb-43de-b928-19d81a152445\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:15,042]\u001b[0m A new study created in memory with name: no-name-10ef1369-5a2f-451e-aecf-11e6bc61e038\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:15,042]\u001b[0m A new study created in memory with name: no-name-c3200a5c-80e3-4726-9a7e-92ee51593eb3\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'max_leaves': 92, 'learning_rate': 0.03329019834400153, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.4, 'subsample': 0.8, 'colsample_bytree': 0.8}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.89918\tvalidation_1-mae:75.85680\n","[9]\tvalidation_0-mae:63.12244\tvalidation_1-mae:69.70962\n","loss____: 0.08381294607689832\n","\u001b[32m[I 2025-02-19 06:42:15,285]\u001b[0m Trial 0 finished with value: 0.08381294607689832 and parameters: {}. Best is trial 0 with value: 0.08381294607689832.\u001b[0m\n","-------\n","0.08381294607689832\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:76.41322\tvalidation_1-mae:72.17379\n","[9]\tvalidation_0-mae:63.61673\tvalidation_1-mae:65.61793\n","loss____: 0.09048652893667902\n","\u001b[32m[I 2025-02-19 06:42:15,340]\u001b[0m Trial 0 finished with value: 0.09048652893667902 and parameters: {}. Best is trial 0 with value: 0.09048652893667902.\u001b[0m\n","-------\n","0.09048652893667902\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:76.85918\tvalidation_1-mae:70.80031\n","[9]\tvalidation_0-mae:64.04694\tvalidation_1-mae:63.32493\n","loss____: 0.08382732594540884\n","\u001b[32m[I 2025-02-19 06:42:15,389]\u001b[0m Trial 0 finished with value: 0.08382732594540884 and parameters: {}. Best is trial 0 with value: 0.08382732594540884.\u001b[0m\n","-------\n","0.08382732594540884\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:15,455]\u001b[0m Trial 6 finished with value: 0.08604226698632873 and parameters: {'max_depth': 3, 'max_leaves': 92, 'learning_rate': 0.03329019834400153, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.4, 'subsample': 0.8, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_7 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_8 strat...\n","\u001b[32m[I 2025-02-19 06:42:15,500]\u001b[0m A new study created in memory with name: no-name-00a2d3fd-0e1a-41b8-bc0f-dd780089db56\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:15,501]\u001b[0m A new study created in memory with name: no-name-29ecfc8e-814f-405e-b008-f10a3cff2d0e\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:15,501]\u001b[0m A new study created in memory with name: no-name-144e5b44-d491-4f40-8885-d0c6f423ee7f\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'max_leaves': 98, 'learning_rate': 0.07976195410250031, 'reg_alpha': 0.7, 'reg_lambda': 0.7, 'subsample': 0.8, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:72.98279\tvalidation_1-mae:74.61252\n","[9]\tvalidation_0-mae:49.86204\tvalidation_1-mae:70.00765\n","loss____: 0.06565242326585367\n","\u001b[32m[I 2025-02-19 06:42:15,754]\u001b[0m Trial 0 finished with value: 0.06565242326585367 and parameters: {}. Best is trial 0 with value: 0.06565242326585367.\u001b[0m\n","-------\n","0.06565242326585367\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.88732\tvalidation_1-mae:68.62527\n","[9]\tvalidation_0-mae:50.15247\tvalidation_1-mae:63.94555\n","loss____: 0.07263617541982621\n","\u001b[32m[I 2025-02-19 06:42:15,809]\u001b[0m Trial 0 finished with value: 0.07263617541982621 and parameters: {}. Best is trial 0 with value: 0.07263617541982621.\u001b[0m\n","-------\n","0.07263617541982621\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:72.17158\tvalidation_1-mae:80.09300\n","[9]\tvalidation_0-mae:50.73613\tvalidation_1-mae:75.23026\n","loss____: 0.06804481301637118\n","\u001b[32m[I 2025-02-19 06:42:15,863]\u001b[0m Trial 0 finished with value: 0.06804481301637118 and parameters: {}. Best is trial 0 with value: 0.06804481301637118.\u001b[0m\n","-------\n","0.06804481301637118\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:15,935]\u001b[0m Trial 7 finished with value: 0.06877780390068368 and parameters: {'max_depth': 3, 'max_leaves': 98, 'learning_rate': 0.07976195410250031, 'reg_alpha': 0.7, 'reg_lambda': 0.7, 'subsample': 0.8, 'colsample_bytree': 1.0}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_8 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_9 strat...\n","\u001b[32m[I 2025-02-19 06:42:15,980]\u001b[0m A new study created in memory with name: no-name-9abeb7f0-55f5-4a1d-abb5-2bda9f61cbde\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:15,980]\u001b[0m A new study created in memory with name: no-name-606a03dd-3e2b-4488-a8f2-a2cf9b96091a\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:15,981]\u001b[0m A new study created in memory with name: no-name-757c05a9-6c99-4b8f-958b-12f7f76412bb\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'max_leaves': 27, 'learning_rate': 0.014070456001948426, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.7, 'colsample_bytree': 0.9}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:76.10414\tvalidation_1-mae:79.37775\n","[9]\tvalidation_0-mae:71.57699\tvalidation_1-mae:78.38640\n","loss____: 0.09629163922218709\n","\u001b[32m[I 2025-02-19 06:42:16,225]\u001b[0m Trial 0 finished with value: 0.09629163922218709 and parameters: {}. Best is trial 0 with value: 0.09629163922218709.\u001b[0m\n","-------\n","0.09629163922218709\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:77.53923\tvalidation_1-mae:70.37418\n","[9]\tvalidation_0-mae:72.39101\tvalidation_1-mae:68.49513\n","loss____: 0.10291017776172774\n","\u001b[32m[I 2025-02-19 06:42:16,276]\u001b[0m Trial 0 finished with value: 0.10291017776172774 and parameters: {}. Best is trial 0 with value: 0.10291017776172774.\u001b[0m\n","-------\n","0.10291017776172774\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.49414\tvalidation_1-mae:83.74213\n","[9]\tvalidation_0-mae:70.74584\tvalidation_1-mae:82.02282\n","loss____: 0.09474111303467424\n","\u001b[32m[I 2025-02-19 06:42:16,329]\u001b[0m Trial 0 finished with value: 0.09474111303467424 and parameters: {}. Best is trial 0 with value: 0.09474111303467424.\u001b[0m\n","-------\n","0.09474111303467424\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:16,399]\u001b[0m Trial 8 finished with value: 0.09798097667286303 and parameters: {'max_depth': 3, 'max_leaves': 27, 'learning_rate': 0.014070456001948426, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_9 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_10 strat...\n","\u001b[32m[I 2025-02-19 06:42:16,441]\u001b[0m A new study created in memory with name: no-name-5722f57c-f1be-464a-9d64-71ed960d7169\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:16,441]\u001b[0m A new study created in memory with name: no-name-baee2d25-695a-4a0c-a4f1-8bb63cd4d0cf\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:16,442]\u001b[0m A new study created in memory with name: no-name-0e4c2d9a-b3f3-4afa-bc53-920b7b1bebf4\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'max_leaves': 35, 'learning_rate': 0.058842647484242366, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 0.6, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.25027\tvalidation_1-mae:73.60065\n","[9]\tvalidation_0-mae:57.65983\tvalidation_1-mae:73.03482\n","loss____: 0.07628802747564012\n","\u001b[32m[I 2025-02-19 06:42:16,687]\u001b[0m Trial 0 finished with value: 0.07628802747564012 and parameters: {}. Best is trial 0 with value: 0.07628802747564012.\u001b[0m\n","-------\n","0.07628802747564012\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:75.77670\tvalidation_1-mae:70.92902\n","[9]\tvalidation_0-mae:58.21105\tvalidation_1-mae:69.55603\n","loss____: 0.08265014717574934\n","\u001b[32m[I 2025-02-19 06:42:16,740]\u001b[0m Trial 0 finished with value: 0.08265014717574934 and parameters: {}. Best is trial 0 with value: 0.08265014717574934.\u001b[0m\n","-------\n","0.08265014717574934\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.38200\tvalidation_1-mae:80.48900\n","[9]\tvalidation_0-mae:57.12081\tvalidation_1-mae:79.58210\n","loss____: 0.07622998268269758\n","\u001b[32m[I 2025-02-19 06:42:16,793]\u001b[0m Trial 0 finished with value: 0.07622998268269758 and parameters: {}. Best is trial 0 with value: 0.07622998268269758.\u001b[0m\n","-------\n","0.07622998268269758\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:16,862]\u001b[0m Trial 9 finished with value: 0.07838938577802901 and parameters: {'max_depth': 4, 'max_leaves': 35, 'learning_rate': 0.058842647484242366, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 0.6, 'colsample_bytree': 1.0}. Best is trial 1 with value: 0.06181575890290202.\u001b[0m\n","xgbreg_TRIAL_10 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_11 strat...\n","\u001b[32m[I 2025-02-19 06:42:16,910]\u001b[0m A new study created in memory with name: no-name-1618372a-44c6-497d-828c-a352562b383e\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:16,910]\u001b[0m A new study created in memory with name: no-name-f1d5ce3e-95fc-4db3-b362-b2d06bfcd1e0\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:16,910]\u001b[0m A new study created in memory with name: no-name-1bb85b6a-9ce8-4b7a-b7b1-6b8aff5e3cbc\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'max_leaves': 69, 'learning_rate': 0.09725833997090795, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 1.0, 'colsample_bytree': 0.5}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.46592\tvalidation_1-mae:67.68246\n","[9]\tvalidation_0-mae:41.84213\tvalidation_1-mae:48.86459\n","loss____: 0.0548501791860043\n","\u001b[32m[I 2025-02-19 06:42:17,183]\u001b[0m Trial 0 finished with value: 0.0548501791860043 and parameters: {}. Best is trial 0 with value: 0.0548501791860043.\u001b[0m\n","-------\n","0.0548501791860043\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.83573\tvalidation_1-mae:71.30636\n","[9]\tvalidation_0-mae:41.21217\tvalidation_1-mae:54.16190\n","loss____: 0.0586735802458217\n","\u001b[32m[I 2025-02-19 06:42:17,237]\u001b[0m Trial 0 finished with value: 0.0586735802458217 and parameters: {}. Best is trial 0 with value: 0.0586735802458217.\u001b[0m\n","-------\n","0.0586735802458217\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.76197\tvalidation_1-mae:72.47273\n","[9]\tvalidation_0-mae:41.43296\tvalidation_1-mae:54.49171\n","loss____: 0.05570172889127921\n","\u001b[32m[I 2025-02-19 06:42:17,291]\u001b[0m Trial 0 finished with value: 0.05570172889127921 and parameters: {}. Best is trial 0 with value: 0.05570172889127921.\u001b[0m\n","-------\n","0.05570172889127921\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:17,358]\u001b[0m Trial 10 finished with value: 0.05640849610770173 and parameters: {'max_depth': 5, 'max_leaves': 69, 'learning_rate': 0.09725833997090795, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 1.0, 'colsample_bytree': 0.5}. Best is trial 10 with value: 0.05640849610770173.\u001b[0m\n","xgbreg_TRIAL_11 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_12 strat...\n","\u001b[32m[I 2025-02-19 06:42:17,417]\u001b[0m A new study created in memory with name: no-name-f0bc2cb1-dd28-4ef8-97cf-53c98be0da4f\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:17,417]\u001b[0m A new study created in memory with name: no-name-b36a8622-8194-4a8b-a703-66ff38fd52dc\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:17,418]\u001b[0m A new study created in memory with name: no-name-b0448f17-d4e7-4cc5-aa0f-dc86dafb4f11\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'max_leaves': 67, 'learning_rate': 0.0997718927289372, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 1.0, 'colsample_bytree': 0.5}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.04416\tvalidation_1-mae:68.87132\n","[9]\tvalidation_0-mae:40.88808\tvalidation_1-mae:49.06791\n","loss____: 0.05411694373150273\n","\u001b[32m[I 2025-02-19 06:42:17,661]\u001b[0m Trial 0 finished with value: 0.05411694373150273 and parameters: {}. Best is trial 0 with value: 0.05411694373150273.\u001b[0m\n","-------\n","0.05411694373150273\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.72608\tvalidation_1-mae:70.75003\n","[9]\tvalidation_0-mae:40.61719\tvalidation_1-mae:53.69330\n","loss____: 0.057719383643195114\n","\u001b[32m[I 2025-02-19 06:42:17,713]\u001b[0m Trial 0 finished with value: 0.057719383643195114 and parameters: {}. Best is trial 0 with value: 0.057719383643195114.\u001b[0m\n","-------\n","0.057719383643195114\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.97417\tvalidation_1-mae:70.58049\n","[9]\tvalidation_0-mae:40.74806\tvalidation_1-mae:54.06478\n","loss____: 0.054602697646289805\n","\u001b[32m[I 2025-02-19 06:42:17,764]\u001b[0m Trial 0 finished with value: 0.054602697646289805 and parameters: {}. Best is trial 0 with value: 0.054602697646289805.\u001b[0m\n","-------\n","0.054602697646289805\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:17,856]\u001b[0m Trial 11 finished with value: 0.05547967500699588 and parameters: {'max_depth': 5, 'max_leaves': 67, 'learning_rate': 0.0997718927289372, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 1.0, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.05547967500699588.\u001b[0m\n","xgbreg_TRIAL_12 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_13 strat...\n","\u001b[32m[I 2025-02-19 06:42:17,902]\u001b[0m A new study created in memory with name: no-name-0cd22a88-4767-4f4b-afad-06fe36f883af\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:17,903]\u001b[0m A new study created in memory with name: no-name-85baff22-71de-4f39-9085-2589cb208bdf\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:17,903]\u001b[0m A new study created in memory with name: no-name-ac50e070-c2a5-41ab-8063-39b1176d4188\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'max_leaves': 70, 'learning_rate': 0.09956909219202137, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.5}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:72.77065\tvalidation_1-mae:78.27004\n","[9]\tvalidation_0-mae:41.61905\tvalidation_1-mae:57.15114\n","loss____: 0.056215326509964414\n","\u001b[32m[I 2025-02-19 06:42:18,185]\u001b[0m Trial 0 finished with value: 0.056215326509964414 and parameters: {}. Best is trial 0 with value: 0.056215326509964414.\u001b[0m\n","-------\n","0.056215326509964414\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.88463\tvalidation_1-mae:69.92408\n","[9]\tvalidation_0-mae:41.73711\tvalidation_1-mae:51.45000\n","loss____: 0.059098404343920816\n","\u001b[32m[I 2025-02-19 06:42:18,238]\u001b[0m Trial 0 finished with value: 0.059098404343920816 and parameters: {}. Best is trial 0 with value: 0.059098404343920816.\u001b[0m\n","-------\n","0.059098404343920816\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.74153\tvalidation_1-mae:72.62140\n","[9]\tvalidation_0-mae:41.74921\tvalidation_1-mae:52.54088\n","loss____: 0.05599318478335504\n","\u001b[32m[I 2025-02-19 06:42:18,293]\u001b[0m Trial 0 finished with value: 0.05599318478335504 and parameters: {}. Best is trial 0 with value: 0.05599318478335504.\u001b[0m\n","-------\n","0.05599318478335504\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:18,372]\u001b[0m Trial 12 finished with value: 0.05710230521241342 and parameters: {'max_depth': 5, 'max_leaves': 70, 'learning_rate': 0.09956909219202137, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.05547967500699588.\u001b[0m\n","xgbreg_TRIAL_13 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_14 strat...\n","\u001b[32m[I 2025-02-19 06:42:18,420]\u001b[0m A new study created in memory with name: no-name-c98fcf46-215b-4e21-95dc-4a80b5533fc3\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:18,421]\u001b[0m A new study created in memory with name: no-name-ffc3df20-3e1b-4bf2-b5e9-c6e9fdce0096\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:18,421]\u001b[0m A new study created in memory with name: no-name-915b2e5d-b28b-42b3-bfec-2e949ef6b0ce\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'max_leaves': 75, 'learning_rate': 0.09621294163161209, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.6}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.63043\tvalidation_1-mae:74.25235\n","[9]\tvalidation_0-mae:39.34953\tvalidation_1-mae:61.03239\n","loss____: 0.05208319651343713\n","\u001b[32m[I 2025-02-19 06:42:18,752]\u001b[0m Trial 0 finished with value: 0.05208319651343713 and parameters: {}. Best is trial 0 with value: 0.05208319651343713.\u001b[0m\n","-------\n","0.05208319651343713\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.50405\tvalidation_1-mae:75.63042\n","[9]\tvalidation_0-mae:39.37794\tvalidation_1-mae:59.25116\n","loss____: 0.056048772733806686\n","\u001b[32m[I 2025-02-19 06:42:18,820]\u001b[0m Trial 0 finished with value: 0.056048772733806686 and parameters: {}. Best is trial 0 with value: 0.056048772733806686.\u001b[0m\n","-------\n","0.056048772733806686\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.18076\tvalidation_1-mae:76.02941\n","[9]\tvalidation_0-mae:39.37866\tvalidation_1-mae:60.80653\n","loss____: 0.05343542778146938\n","\u001b[32m[I 2025-02-19 06:42:18,886]\u001b[0m Trial 0 finished with value: 0.05343542778146938 and parameters: {}. Best is trial 0 with value: 0.05343542778146938.\u001b[0m\n","-------\n","0.05343542778146938\n","-------\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:18,961]\u001b[0m Trial 13 finished with value: 0.05385579900957107 and parameters: {'max_depth': 5, 'max_leaves': 75, 'learning_rate': 0.09621294163161209, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 13 with value: 0.05385579900957107.\u001b[0m\n","xgbreg_TRIAL_14 done...\n","\n","◎ level_1_xgbreg_GLOBAL_TRIAL_15 strat...\n","\u001b[32m[I 2025-02-19 06:42:19,005]\u001b[0m A new study created in memory with name: no-name-3a4f4502-6c1e-4d5b-9d43-aaf4c66469bd\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:19,006]\u001b[0m A new study created in memory with name: no-name-5480d0eb-8321-4e75-a397-42d743d493c7\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:19,006]\u001b[0m A new study created in memory with name: no-name-f4fedd51-7f1f-4a7c-a5c8-23b0e963ed03\u001b[0m\n","xgbreg\n","///// now...level_1_xgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'max_leaves': 78, 'learning_rate': 0.08845327907879591, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.6}\n","\n","///// now...level_1_xgbreg_local_study_1_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:74.37500\tvalidation_1-mae:73.76089\n","[9]\tvalidation_0-mae:42.25459\tvalidation_1-mae:58.75572\n","loss____: 0.05623417034720897\n","\u001b[32m[I 2025-02-19 06:42:19,320]\u001b[0m Trial 0 finished with value: 0.05623417034720897 and parameters: {}. Best is trial 0 with value: 0.05623417034720897.\u001b[0m\n","-------\n","0.05623417034720897\n","-------\n","\n","///// now...level_1_xgbreg_local_study_2_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.07041\tvalidation_1-mae:78.68301\n","[9]\tvalidation_0-mae:41.07483\tvalidation_1-mae:65.12431\n","loss____: 0.0593680602881807\n","\u001b[32m[I 2025-02-19 06:42:19,396]\u001b[0m Trial 0 finished with value: 0.0593680602881807 and parameters: {}. Best is trial 0 with value: 0.0593680602881807.\u001b[0m\n","-------\n","0.0593680602881807\n","-------\n","\n","///// now...level_1_xgbreg_local_study_3_fold_cv /////\n","★ level_1_xgbreg_LOCAL_TRIAL_1 strat...\n","[0]\tvalidation_0-mae:73.69645\tvalidation_1-mae:76.24829\n","[9]\tvalidation_0-mae:41.39284\tvalidation_1-mae:61.83099\n","loss____: 0.05603918901840899\n","\u001b[32m[I 2025-02-19 06:42:19,468]\u001b[0m Trial 0 finished with value: 0.05603918901840899 and parameters: {}. Best is trial 0 with value: 0.05603918901840899.\u001b[0m\n","-------\n","0.05603918901840899\n","-------\n","設定したn_trials50に達することなくstudyを終了します。\n","15回目のトライアルで終了しました。\n","///// now...level_1_xgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:19,581]\u001b[0m Trial 14 finished with value: 0.057213806551266226 and parameters: {'max_depth': 5, 'max_leaves': 78, 'learning_rate': 0.08845327907879591, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 13 with value: 0.05385579900957107.\u001b[0m\n","xgbreg_TRIAL_15 done...\n","///// model with optimal parameters 訓練中... //////\n","\n","///// LV1_xgbreg_model_with_best_params /////\n","///// 1_fold_cv... /////\n","[0]\tvalidation_0-mae:71.44239\tvalidation_1-mae:90.71498\n","[9]\tvalidation_0-mae:40.30508\tvalidation_1-mae:74.72015\n","///// LV1_xgbreg_model_with_best_params /////\n","///// 2_fold_cv... /////\n","[0]\tvalidation_0-mae:74.86600\tvalidation_1-mae:67.68041\n","[9]\tvalidation_0-mae:41.94807\tvalidation_1-mae:53.20710\n","///// LV1_xgbreg_model_with_best_params /////\n","///// 3_fold_cv... /////\n","[0]\tvalidation_0-mae:74.53922\tvalidation_1-mae:73.85815\n","[9]\tvalidation_0-mae:41.90363\tvalidation_1-mae:57.74897\n","[862.6169  755.42554 790.597   736.06573 717.5713  733.64996 752.9297\n"," 776.27    700.7656  704.8837  690.68146 660.8132  673.1706  702.3576\n"," 757.94366 692.78    698.1846  676.85565 694.5252  685.47125 717.7085\n"," 766.93134 689.4339  725.1801  696.096   674.7869  715.74164 709.6015\n"," 757.84375 701.8965  709.55255 695.1665  693.9138  699.8798  721.1374\n"," 726.7767  704.6038  735.90186 663.59406 684.56866 696.2348  705.13776\n"," 722.5498  696.7149  720.9277  677.0761  700.8374  702.98663 725.5661\n"," 729.2172  695.1943  714.27734 695.13165 667.58295 693.4117  708.3217\n"," 755.12604 692.2513  711.27606 674.1779  666.90735 706.8911  708.89716\n"," 737.1319  700.9148  717.7996  664.48773 662.43896 705.51324 711.18536\n"," 773.731   691.75977 699.9587  672.43    696.77826 701.67975 696.36505\n"," 725.5427  696.4702  697.90045 674.8465  655.3776  713.4189  719.14825\n"," 753.4217  684.4878  698.11304 681.17584 690.26074 710.03735 683.8957\n"," 725.88556 694.5095  685.0153  671.3963  688.8448  696.48804 698.5572\n"," 736.4615  690.54755 707.02545 680.4113  670.1428  713.49146 692.02423\n"," 729.6873  687.8913  690.4614  667.85767 666.76044 704.1984  692.65607\n"," 750.6663  690.03107 693.6027  678.89734 659.2356  688.00433 693.87646\n"," 724.5113  688.13257 693.2615  669.06854 669.1489  708.45264 699.75244\n"," 734.8737  690.9404  701.6551  667.6755  691.2097  691.8531  706.5715\n"," 746.04767 692.3643  706.4702  657.5478  693.6801  701.642   695.7863\n"," 719.0723  690.88116 713.3576  676.3367  680.4428  715.8789  707.187\n"," 724.9065  692.3427  711.50977 685.81714 674.74054 680.73126 709.66583\n"," 758.93915 690.1055  717.7558  685.0706  677.2673  691.2197  711.90704\n"," 736.78094 695.312   717.63947 679.16943 680.8001  706.9726  692.363\n"," 742.5706  690.6941  699.69434 686.9018  683.76483 708.96954 705.6885\n"," 742.25977 678.0058  712.81854 679.5791  674.0782  699.15106 684.68134\n"," 727.17725 700.0551  752.8664  705.6841  710.09375 696.8953  700.7241\n"," 738.99884 719.8034  694.68585 689.0246  683.81866 705.38086 708.94745\n"," 744.33136 689.1233  692.0174  671.33905 684.6341  682.533   706.27374\n"," 746.43097 710.7674  694.62256 681.7253  651.7578  679.4233  709.54877\n"," 728.58716 691.59375 695.03705 675.61176 678.44525 691.6839  699.5096\n"," 727.58    697.77905 710.7805  671.6888  673.1645  714.2419  716.74286\n"," 738.6543  694.54645 713.9588  679.83844 669.1528  706.1648  725.097\n"," 737.9017  695.08954 719.48016 693.63275 670.5698  701.80194 706.66284\n"," 731.70215 696.6934  718.7233  690.1546  673.82806 703.9917  719.5826\n"," 777.28046 687.24945 699.7849  699.53613 670.6978  717.39105 736.7299\n"," 759.572   722.89087 720.0188  696.2399  679.51385 698.4195  703.84863\n"," 744.4043  695.90845 703.34955 674.88214 689.7473  694.18256 718.2264\n"," 746.0278  675.0692  732.2166  684.3122  688.8098  705.8693  719.77313\n"," 749.87274 692.5146  703.26465 687.1513  676.96564 721.1171  724.1807\n"," 758.34796 720.31836 710.64777 668.98865 696.9568  695.9463  699.7232\n"," 760.61694 697.09064 703.44525 694.49603 697.5425  695.28    713.8047\n"," 747.73065 694.5683  705.0083  693.7768  697.7546  708.1722  727.28467\n"," 758.0997  702.28534 715.1118  685.153   679.0518  714.11554 730.4362\n"," 755.9126  700.6953  720.1621  684.7869  699.0337  718.9003  731.93353\n"," 737.82916 710.58466 705.18976 693.8276  702.03705 713.5844  747.486\n"," 752.4058  695.9637  722.7536  699.72235 680.78253 714.8566  718.1328\n"," 783.92804 724.53107 742.6673  697.4868  696.5124  747.16486 745.21014\n"," 783.1473  765.1712  745.62555 715.06665 689.6966  694.86273 731.6766\n"," 749.2334  717.08527 712.6512  695.5704  675.06537 705.8991  702.5376\n"," 742.34515 740.6148  753.10455 697.88654 707.8548  720.4251  713.20215\n"," 751.5804  723.44305 758.1379  754.8597  800.7535  812.04663 785.02155\n"," 780.2248  810.0578  802.9124  765.1953  786.8996  748.0567  772.36285\n"," 822.4672  766.6765  776.59283 747.56537 735.7663  749.4265  747.9318\n"," 787.05743 750.90967 746.21515 732.3313  735.1091  728.95917 744.3362\n"," 802.48456 759.3737  739.3804  737.7243  721.3974  743.93524 760.6594\n"," 796.0185  770.57526 785.41016 741.0378  735.7347  750.7319  759.7292\n"," 795.00507 740.96674 767.8511  738.293   735.8724  750.1001  777.3284\n"," 795.8902  736.32294 756.18097 731.35156 733.8826  755.3509  760.2739\n"," 775.9446  751.1667  781.7036  732.60077 733.4847  756.8364  759.2092\n"," 803.4341  735.4178  769.1291  743.71735 763.29144 741.82837 755.32684\n"," 786.1941  735.11725 769.6713  750.7168  730.768   748.60425 765.80743\n"," 813.87286 743.15894 785.92096 730.5025  730.2959  760.5535  762.66437\n"," 793.5413  737.7393  762.63214 731.4268  728.076   757.6172  760.58124\n"," 770.43317 761.78906 794.5184  737.3016  738.142   755.74756 797.5545\n"," 822.094   751.86035 755.02167 736.3732  712.7834  754.126   766.15845\n"," 792.8127  734.95337 774.33636 742.3521  741.70703 756.89087 775.6142\n"," 809.33246 733.4261  765.6142  732.35693 731.1673  745.56354 779.09894\n"," 806.05023 744.051   784.7487  732.42065 733.2488  773.438   789.3632\n"," 795.521   775.487   796.63135 733.2591  752.1004  748.50604 754.79443\n"," 798.5459  745.45074 781.32837 724.09375 749.028   770.02435 767.86505\n"," 822.65674 764.7805  775.03656 734.64056 730.7741  755.4829  759.6162\n"," 791.11523 760.3491  795.85516 736.7669  734.6331  732.7183  758.91034\n"," 839.2552  753.7092  786.05054 735.44366 737.4074  760.2614  769.4567\n"," 832.4702  746.20654 803.04297 735.825   757.97974 748.237   782.5603\n"," 819.03    738.7114  794.7033  761.74945 730.9087  747.11597 742.4653\n"," 803.1382  743.16144 772.4177  732.5047  731.833   756.01385 753.29395\n"," 803.52466 730.97626 763.69257 729.4868  746.49316 753.99756 767.36456\n"," 795.87646 733.98126 759.3868  768.1633  762.1987  766.6678  791.541\n"," 811.1343  760.559   782.7901  736.618   735.0953  755.1817  768.34015\n"," 792.81396 767.3999  780.0102  741.32196 709.8105  737.5418  757.3599\n"," 834.8457  771.1592  745.37933 731.60175 743.4759  746.46686 772.4551\n"," 800.43085 772.8618  768.4695  736.0683  734.48975 757.441   756.7753\n"," 817.73505 750.29156 812.94354 737.4141  734.86163 792.77344 780.4318\n"," 832.5747  742.44305 765.96216 754.11774 735.2037  735.576   780.1575\n"," 828.7508  737.613   773.2129  733.4954  737.6486  745.9876  753.7688\n"," 834.44794 776.21014 798.499   729.52466 735.4644  780.84717 820.4865\n"," 814.65436 766.4721  749.9676  725.6159  727.54047 782.7261  799.3835\n"," 840.6302  783.98456 782.469   759.8338  727.40936 762.67426 775.4346\n"," 818.2598  749.34375 778.9699  725.43585 733.8055  748.7083  767.52344\n"," 810.7485  755.12115 768.33984 739.42303 736.2214  748.3033  756.9467\n"," 804.5838  757.26587 755.94025 735.8047  735.69794 734.4025  755.5112\n"," 816.59467 744.2414  771.96356 739.0003  737.4836  740.0202  756.3835\n"," 814.44464 761.6452  784.7131  722.5289  736.39624 753.05743 757.66705\n"," 784.41815 741.2277  784.0014  736.554   765.33954 757.3702  768.0113\n"," 796.90845 741.5965  747.063   739.9638  737.60187 731.2905  760.9422\n"," 793.16235 773.7767  761.71356 740.18646 738.83887 751.06616 772.0332\n"," 813.25146 759.281   778.3078  731.41876 736.6245  762.5034  758.35547\n"," 786.4109  759.20807 774.1536  732.66797 735.88574 758.9189  762.37726\n"," 798.7862  739.5612  741.3006  734.31354 744.4086  761.3597  785.3357\n"," 808.9419  773.72675 826.6132  730.2073  744.3874  771.1526  787.24927\n"," 799.9844  744.0826  782.8081  723.85596 721.79474 759.2334  767.91943\n"," 789.7307  732.8089  744.8742  732.35834 726.7209  751.15625 775.1548\n"," 791.7544  750.2317  748.2986  727.2078  757.3587  767.6736  823.1861\n"," 854.732   829.2795  792.05054 731.27435 741.1706  772.9429  800.4378\n"," 812.013   763.6148  781.5876  728.3     738.1535  741.41986 756.75757\n"," 787.1667  752.13184 756.9318  725.24347 733.6257  716.08594 773.1755\n"," 796.34674 731.99384 753.71094 725.68506 722.3853  744.18604 757.76154\n"," 794.9943  744.39703 787.4239  717.66455 738.55664 756.0337  739.8996\n"," 793.55927 742.77765 752.0034  730.1365  735.3221  735.47656 726.6572\n"," 788.7942  738.3869  732.71924 731.70123 730.06396 736.03973 733.3929\n"," 783.39777 736.0289  736.5025  728.93604 710.42303 727.13824 732.9694\n"," 776.4113  743.7265  754.9414  715.49774 742.6137  752.8472  767.4466\n"," 805.3118  728.55664 738.3762  732.804   727.82416 715.5555  744.0127\n"," 780.38763 726.6643  764.27765 703.02423 709.97736 729.36554 718.55743\n"," 790.62744 726.5884  732.0983  717.74786 721.5444  735.1701  733.0784\n"," 768.4649  723.8752  738.50586 702.834   731.2803  731.59406 728.6424\n"," 768.4021  752.72186 730.03094 718.6386  727.6414  725.8389  729.9129\n"," 773.2426  718.0175  722.9389  706.79596 707.3499  699.8488  743.97406\n"," 766.4175  720.0512  732.6655  714.0847  707.72095 710.03204 736.99854\n"," 763.82324 731.7082  725.33026 723.30664 705.85175 734.762   740.4455\n"," 763.1359  724.6596  748.4749  722.9771  713.90137 738.80853 732.3193\n"," 769.32    739.5427  735.69385 700.2129  716.1975  724.32355 751.9368\n"," 783.50653 722.116   749.6512  705.6751  711.4346  732.7225  743.44336\n"," 760.10297 729.7204  723.248   710.43066 710.6899  745.43536 746.41\n"," 778.44165 722.5503  730.5923  716.8907  722.353   749.85065 699.6058\n"," 780.1208  724.37085 743.2544  720.18066 708.1685  715.97186 713.61847\n"," 782.97363 723.491   751.3498  704.513   731.48145 741.76337 736.3934\n"," 778.6277  720.53326 764.34344 700.0318  711.9927  709.133   715.02545\n"," 749.2337  721.2656  726.8408  693.3636  711.47076 704.2458  712.84406\n"," 752.06976 720.0727  724.5192  727.0324  740.38324 736.8379  776.29706\n"," 806.38324 745.7204  789.4815  712.1169  711.8035  724.2104  735.9047\n"," 773.69775 720.37506 736.0378  697.6645  687.7215  708.55273 731.7065\n"," 751.1721  706.51855 730.8447  720.8316  688.77313 724.4528  746.49854\n"," 753.64777 736.36456 742.16486 722.1159  727.6509  742.3549  729.4212\n"," 748.2752  735.21857 727.72125 719.2745  717.3874  745.23    751.43677\n"," 780.7586  725.6155  752.1351  725.6968  718.3953  742.86884 728.5739\n"," 781.6106  723.438   724.1155  704.3405  723.13525 745.9453  732.3327\n"," 809.4424  716.1866  724.50977 726.9405  704.0694  749.111   745.42236\n"," 804.68524 724.6294  725.32544 728.4798  722.10223 751.62177 769.9432\n"," 804.5363  737.6885  748.5451  718.1779  724.3359  734.56665 760.1303\n"," 788.0801  726.0794  746.89746 716.7347  716.87006 731.2713  743.7015\n"," 779.76514 726.4407  737.1619  722.16455 695.3796  754.2791 ]\n","\tDone!\n","\n","///// lgbregの学習をSTART ! /////\n","///// now... level_1 //////\n","///// search for lgbreg /////\n","\u001b[32m[I 2025-02-19 06:42:20,317]\u001b[0m A new study created in RDB with name: study_ens_level_1_lgbreg\u001b[0m\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_1 strat...\n","\u001b[32m[I 2025-02-19 06:42:20,464]\u001b[0m A new study created in memory with name: no-name-43f08e3a-9c18-4b40-bdff-1a54d0722139\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:20,465]\u001b[0m A new study created in memory with name: no-name-bdd5e236-774a-401a-b95a-e3838753acc1\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:20,465]\u001b[0m A new study created in memory with name: no-name-be45f716-b832-43d7-9efa-b5f7a2508ecb\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 102, 'learning_rate': 0.05395030966670229, 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.6, 'colsample_bytree': 0.5}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.893887\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6483.94\teval's l2: 6353.37\n","loss____: 0.08396797643434417\n","\u001b[32m[I 2025-02-19 06:42:20,776]\u001b[0m Trial 0 finished with value: 0.08396797643434417 and parameters: {}. Best is trial 0 with value: 0.08396797643434417.\u001b[0m\n","-------\n","0.08396797643434417\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.182238\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6272.92\teval's l2: 8089.89\n","loss____: 0.08953134515305965\n","\u001b[32m[I 2025-02-19 06:42:20,807]\u001b[0m Trial 0 finished with value: 0.08953134515305965 and parameters: {}. Best is trial 0 with value: 0.08953134515305965.\u001b[0m\n","-------\n","0.08953134515305965\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000128 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.472895\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6444.27\teval's l2: 6913.94\n","loss____: 0.08339307940732071\n","\u001b[32m[I 2025-02-19 06:42:20,835]\u001b[0m Trial 0 finished with value: 0.08339307940732071 and parameters: {}. Best is trial 0 with value: 0.08339307940732071.\u001b[0m\n","-------\n","0.08339307940732071\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:20,935]\u001b[0m Trial 0 finished with value: 0.08563080033157484 and parameters: {'max_depth': 2, 'num_leaves': 102, 'learning_rate': 0.05395030966670229, 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 0 with value: 0.08563080033157484.\u001b[0m\n","lgbreg_TRIAL_1 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_2 strat...\n","\u001b[32m[I 2025-02-19 06:42:20,988]\u001b[0m A new study created in memory with name: no-name-27143bb4-a5d3-4bbe-8a15-dfd1e5e1a6da\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:20,988]\u001b[0m A new study created in memory with name: no-name-d8757110-493f-4dc7-ac54-f6c44dbb987b\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:20,988]\u001b[0m A new study created in memory with name: no-name-49380444-f5e3-4293-b339-ca98bc79ca41\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'num_leaves': 58, 'learning_rate': 0.051059032093947576, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 1.0, 'colsample_bytree': 0.6}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000103 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.561707\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5795.96\teval's l2: 6317.68\n","loss____: 0.07945226438544938\n","\u001b[32m[I 2025-02-19 06:42:21,242]\u001b[0m Trial 0 finished with value: 0.07945226438544938 and parameters: {}. Best is trial 0 with value: 0.07945226438544938.\u001b[0m\n","-------\n","0.07945226438544938\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000106 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.682814\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5754.35\teval's l2: 6225.02\n","loss____: 0.08510989792343421\n","\u001b[32m[I 2025-02-19 06:42:21,274]\u001b[0m Trial 0 finished with value: 0.08510989792343421 and parameters: {}. Best is trial 0 with value: 0.08510989792343421.\u001b[0m\n","-------\n","0.08510989792343421\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000102 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.373702\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5744.65\teval's l2: 6591.45\n","loss____: 0.07811171619005162\n","\u001b[32m[I 2025-02-19 06:42:21,306]\u001b[0m Trial 0 finished with value: 0.07811171619005162 and parameters: {}. Best is trial 0 with value: 0.07811171619005162.\u001b[0m\n","-------\n","0.07811171619005162\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:21,396]\u001b[0m Trial 1 finished with value: 0.08089129283297841 and parameters: {'max_depth': 5, 'num_leaves': 58, 'learning_rate': 0.051059032093947576, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 1.0, 'colsample_bytree': 0.6}. Best is trial 1 with value: 0.08089129283297841.\u001b[0m\n","lgbreg_TRIAL_2 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_3 strat...\n","\u001b[32m[I 2025-02-19 06:42:21,438]\u001b[0m A new study created in memory with name: no-name-9ceace14-f551-4295-9968-401900a54a9b\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:21,439]\u001b[0m A new study created in memory with name: no-name-f049a75e-9afe-4c4b-9493-df03bb2a62a2\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:21,439]\u001b[0m A new study created in memory with name: no-name-ca411733-b767-4c5f-b6d3-bab651b51eba\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 29, 'learning_rate': 0.02014847788415866, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.8}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.034602\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7891.65\teval's l2: 7385.94\n","loss____: 0.09279001634029407\n","\u001b[32m[I 2025-02-19 06:42:21,669]\u001b[0m Trial 0 finished with value: 0.09279001634029407 and parameters: {}. Best is trial 0 with value: 0.09279001634029407.\u001b[0m\n","-------\n","0.09279001634029407\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000123 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.835063\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7926.28\teval's l2: 6929.5\n","loss____: 0.09907265290939708\n","\u001b[32m[I 2025-02-19 06:42:21,696]\u001b[0m Trial 0 finished with value: 0.09907265290939708 and parameters: {}. Best is trial 0 with value: 0.09907265290939708.\u001b[0m\n","-------\n","0.09907265290939708\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000030 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 737.017301\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7678.96\teval's l2: 7989.91\n","loss____: 0.09117237611244104\n","\u001b[32m[I 2025-02-19 06:42:21,722]\u001b[0m Trial 0 finished with value: 0.09117237611244104 and parameters: {}. Best is trial 0 with value: 0.09117237611244104.\u001b[0m\n","-------\n","0.09117237611244104\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:21,808]\u001b[0m Trial 2 finished with value: 0.09434501512071074 and parameters: {'max_depth': 2, 'num_leaves': 29, 'learning_rate': 0.02014847788415866, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.08089129283297841.\u001b[0m\n","lgbreg_TRIAL_3 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_4 strat...\n","\u001b[32m[I 2025-02-19 06:42:21,854]\u001b[0m A new study created in memory with name: no-name-58afd225-7e4b-41d1-b0f7-b85067dec0ed\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:21,854]\u001b[0m A new study created in memory with name: no-name-2105ed72-b59e-451d-b1e5-eada29de3b9c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:21,854]\u001b[0m A new study created in memory with name: no-name-47b46e21-330f-4100-a9e2-4da82ae8250a\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 35, 'learning_rate': 0.023246728489504348, 'reg_alpha': 0.5, 'reg_lambda': 0.6000000000000001, 'subsample': 0.6, 'colsample_bytree': 0.8}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.322953\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7601.75\teval's l2: 7281.8\n","loss____: 0.09068067395582413\n","\u001b[32m[I 2025-02-19 06:42:22,096]\u001b[0m Trial 0 finished with value: 0.09068067395582413 and parameters: {}. Best is trial 0 with value: 0.09068067395582413.\u001b[0m\n","-------\n","0.09068067395582413\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.404844\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7547.39\teval's l2: 7835.52\n","loss____: 0.09716720336185264\n","\u001b[32m[I 2025-02-19 06:42:22,122]\u001b[0m Trial 0 finished with value: 0.09716720336185264 and parameters: {}. Best is trial 0 with value: 0.09716720336185264.\u001b[0m\n","-------\n","0.09716720336185264\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 732.660900\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7485.4\teval's l2: 8369.27\n","loss____: 0.09031639675847498\n","\u001b[32m[I 2025-02-19 06:42:22,149]\u001b[0m Trial 0 finished with value: 0.09031639675847498 and parameters: {}. Best is trial 0 with value: 0.09031639675847498.\u001b[0m\n","-------\n","0.09031639675847498\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:22,236]\u001b[0m Trial 3 finished with value: 0.09272142469205058 and parameters: {'max_depth': 2, 'num_leaves': 35, 'learning_rate': 0.023246728489504348, 'reg_alpha': 0.5, 'reg_lambda': 0.6000000000000001, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.08089129283297841.\u001b[0m\n","lgbreg_TRIAL_4 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_5 strat...\n","\u001b[32m[I 2025-02-19 06:42:22,315]\u001b[0m A new study created in memory with name: no-name-b73926e8-2d8b-4157-a488-b6840f9604a0\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:22,315]\u001b[0m A new study created in memory with name: no-name-97fbda0e-f298-4c75-adf0-4448c8490766\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:22,316]\u001b[0m A new study created in memory with name: no-name-99b5abbf-2c38-4977-a918-9b4caffcd732\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 23, 'learning_rate': 0.04050837781329675, 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 1.0, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.554787\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6024.44\teval's l2: 5729.64\n","loss____: 0.08013883913765445\n","\u001b[32m[I 2025-02-19 06:42:22,534]\u001b[0m Trial 0 finished with value: 0.08013883913765445 and parameters: {}. Best is trial 0 with value: 0.08013883913765445.\u001b[0m\n","-------\n","0.08013883913765445\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.603230\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5982.65\teval's l2: 6071.45\n","loss____: 0.08616860494683379\n","\u001b[32m[I 2025-02-19 06:42:22,561]\u001b[0m Trial 0 finished with value: 0.08616860494683379 and parameters: {}. Best is trial 0 with value: 0.08616860494683379.\u001b[0m\n","-------\n","0.08616860494683379\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.720877\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5938.27\teval's l2: 6154.32\n","loss____: 0.079121819620839\n","\u001b[32m[I 2025-02-19 06:42:22,590]\u001b[0m Trial 0 finished with value: 0.079121819620839 and parameters: {}. Best is trial 0 with value: 0.079121819620839.\u001b[0m\n","-------\n","0.079121819620839\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:22,685]\u001b[0m Trial 4 finished with value: 0.0818097545684424 and parameters: {'max_depth': 3, 'num_leaves': 23, 'learning_rate': 0.04050837781329675, 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 1.0, 'colsample_bytree': 1.0}. Best is trial 1 with value: 0.08089129283297841.\u001b[0m\n","lgbreg_TRIAL_5 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_6 strat...\n","\u001b[32m[I 2025-02-19 06:42:22,731]\u001b[0m A new study created in memory with name: no-name-2e721043-5c44-46ca-8e04-49d98638e370\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:22,731]\u001b[0m A new study created in memory with name: no-name-d0af1da8-e9df-4feb-aadd-72b66c420133\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:22,732]\u001b[0m A new study created in memory with name: no-name-d1cdc73a-c07e-4540-96ff-ff2c39e2f34a\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'num_leaves': 35, 'learning_rate': 0.012521954287060391, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.7}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000103 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.611303\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 8066.71\teval's l2: 9281.96\n","loss____: 0.09541488113636765\n","\u001b[32m[I 2025-02-19 06:42:22,963]\u001b[0m Trial 0 finished with value: 0.09541488113636765 and parameters: {}. Best is trial 0 with value: 0.09541488113636765.\u001b[0m\n","-------\n","0.09541488113636765\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000108 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.198385\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 8285.31\teval's l2: 7689.25\n","loss____: 0.1016084067960133\n","\u001b[32m[I 2025-02-19 06:42:22,993]\u001b[0m Trial 0 finished with value: 0.1016084067960133 and parameters: {}. Best is trial 0 with value: 0.1016084067960133.\u001b[0m\n","-------\n","0.1016084067960133\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000110 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.803922\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 8331.53\teval's l2: 7307.31\n","loss____: 0.0940870863076529\n","\u001b[32m[I 2025-02-19 06:42:23,021]\u001b[0m Trial 0 finished with value: 0.0940870863076529 and parameters: {}. Best is trial 0 with value: 0.0940870863076529.\u001b[0m\n","-------\n","0.0940870863076529\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:23,097]\u001b[0m Trial 5 finished with value: 0.0970367914133446 and parameters: {'max_depth': 4, 'num_leaves': 35, 'learning_rate': 0.012521954287060391, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 1 with value: 0.08089129283297841.\u001b[0m\n","lgbreg_TRIAL_6 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_7 strat...\n","\u001b[32m[I 2025-02-19 06:42:23,144]\u001b[0m A new study created in memory with name: no-name-661f7474-aa8b-47f4-9c6b-1a94d124051c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:23,144]\u001b[0m A new study created in memory with name: no-name-c08db020-a2f2-4f0f-befe-298af528c9e8\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:23,144]\u001b[0m A new study created in memory with name: no-name-e01b38e0-a32a-4826-a5d0-7b4aa1513976\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 95, 'learning_rate': 0.01814596135349025, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.4, 'subsample': 0.8, 'colsample_bytree': 0.8}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.642445\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7970.12\teval's l2: 7499.31\n","loss____: 0.09293940013075783\n","\u001b[32m[I 2025-02-19 06:42:23,406]\u001b[0m Trial 0 finished with value: 0.09293940013075783 and parameters: {}. Best is trial 0 with value: 0.09293940013075783.\u001b[0m\n","-------\n","0.09293940013075783\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.131488\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7866.95\teval's l2: 7849.56\n","loss____: 0.09943312421907714\n","\u001b[32m[I 2025-02-19 06:42:23,431]\u001b[0m Trial 0 finished with value: 0.09943312421907714 and parameters: {}. Best is trial 0 with value: 0.09943312421907714.\u001b[0m\n","-------\n","0.09943312421907714\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.461361\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7773.24\teval's l2: 9460.66\n","loss____: 0.09296759997593464\n","\u001b[32m[I 2025-02-19 06:42:23,458]\u001b[0m Trial 0 finished with value: 0.09296759997593464 and parameters: {}. Best is trial 0 with value: 0.09296759997593464.\u001b[0m\n","-------\n","0.09296759997593464\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:23,530]\u001b[0m Trial 6 finished with value: 0.09511337477525654 and parameters: {'max_depth': 2, 'num_leaves': 95, 'learning_rate': 0.01814596135349025, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.4, 'subsample': 0.8, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.08089129283297841.\u001b[0m\n","lgbreg_TRIAL_7 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_8 strat...\n","\u001b[32m[I 2025-02-19 06:42:23,588]\u001b[0m A new study created in memory with name: no-name-62cac20d-bfa1-4c22-b3ac-f73139d60ebc\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:23,589]\u001b[0m A new study created in memory with name: no-name-eadf5389-c555-46b7-944c-df0f3804708f\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:23,589]\u001b[0m A new study created in memory with name: no-name-272c0673-4bbe-41df-8f72-036e9c602eff\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 105, 'learning_rate': 0.05958443469672518, 'reg_alpha': 0.7, 'reg_lambda': 0.7, 'subsample': 0.8, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.788927\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5528.15\teval's l2: 5011.64\n","loss____: 0.0765574910548138\n","\u001b[32m[I 2025-02-19 06:42:23,868]\u001b[0m Trial 0 finished with value: 0.0765574910548138 and parameters: {}. Best is trial 0 with value: 0.0765574910548138.\u001b[0m\n","-------\n","0.0765574910548138\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.182238\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5566.21\teval's l2: 4593.63\n","loss____: 0.08140532459308145\n","\u001b[32m[I 2025-02-19 06:42:23,900]\u001b[0m Trial 0 finished with value: 0.08140532459308145 and parameters: {}. Best is trial 0 with value: 0.08140532459308145.\u001b[0m\n","-------\n","0.08140532459308145\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.403691\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5599.11\teval's l2: 4725.95\n","loss____: 0.07598457889524479\n","\u001b[32m[I 2025-02-19 06:42:23,939]\u001b[0m Trial 0 finished with value: 0.07598457889524479 and parameters: {}. Best is trial 0 with value: 0.07598457889524479.\u001b[0m\n","-------\n","0.07598457889524479\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:24,065]\u001b[0m Trial 7 finished with value: 0.07798246484771335 and parameters: {'max_depth': 2, 'num_leaves': 105, 'learning_rate': 0.05958443469672518, 'reg_alpha': 0.7, 'reg_lambda': 0.7, 'subsample': 0.8, 'colsample_bytree': 1.0}. Best is trial 7 with value: 0.07798246484771335.\u001b[0m\n","lgbreg_TRIAL_8 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_9 strat...\n","\u001b[32m[I 2025-02-19 06:42:24,120]\u001b[0m A new study created in memory with name: no-name-88d3f216-4a7e-4f9d-be51-7fb21ba18df7\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:24,121]\u001b[0m A new study created in memory with name: no-name-bc6a4e57-a40c-4af6-94d5-1daaaadac4f3\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:24,121]\u001b[0m A new study created in memory with name: no-name-ed92701a-0556-4cb6-9f3d-f8297c0827a1\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 30, 'learning_rate': 0.011097554561103107, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.7, 'colsample_bytree': 0.9}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000121 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.890427\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 8558.88\teval's l2: 7891.92\n","loss____: 0.097057604141271\n","\u001b[32m[I 2025-02-19 06:42:24,391]\u001b[0m Trial 0 finished with value: 0.097057604141271 and parameters: {}. Best is trial 0 with value: 0.097057604141271.\u001b[0m\n","-------\n","0.097057604141271\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.042676\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 8869.71\teval's l2: 6159.85\n","loss____: 0.10406623376497605\n","\u001b[32m[I 2025-02-19 06:42:24,420]\u001b[0m Trial 0 finished with value: 0.10406623376497605 and parameters: {}. Best is trial 0 with value: 0.10406623376497605.\u001b[0m\n","-------\n","0.10406623376497605\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 737.521338\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 8611.31\teval's l2: 7462.32\n","loss____: 0.09571223475180961\n","\u001b[32m[I 2025-02-19 06:42:24,448]\u001b[0m Trial 0 finished with value: 0.09571223475180961 and parameters: {}. Best is trial 0 with value: 0.09571223475180961.\u001b[0m\n","-------\n","0.09571223475180961\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:24,527]\u001b[0m Trial 8 finished with value: 0.09894535755268556 and parameters: {'max_depth': 2, 'num_leaves': 30, 'learning_rate': 0.011097554561103107, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 7 with value: 0.07798246484771335.\u001b[0m\n","lgbreg_TRIAL_9 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_10 strat...\n","\u001b[32m[I 2025-02-19 06:42:24,616]\u001b[0m A new study created in memory with name: no-name-05dfb4b5-3d56-43c9-8ea8-9fcd8cf7e054\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:24,616]\u001b[0m A new study created in memory with name: no-name-ad07c0cd-9912-41a2-8131-8c21fba3068e\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:24,616]\u001b[0m A new study created in memory with name: no-name-38bb248e-db69-4a66-9bd9-ffe39e6d8e9b\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 34, 'learning_rate': 0.03488960745139221, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 0.6, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.088812\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6677.59\teval's l2: 6759.73\n","loss____: 0.08496516555793635\n","\u001b[32m[I 2025-02-19 06:42:24,879]\u001b[0m Trial 0 finished with value: 0.08496516555793635 and parameters: {}. Best is trial 0 with value: 0.08496516555793635.\u001b[0m\n","-------\n","0.08496516555793635\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.780854\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6657.3\teval's l2: 7173.34\n","loss____: 0.09189397460872945\n","\u001b[32m[I 2025-02-19 06:42:24,906]\u001b[0m Trial 0 finished with value: 0.09189397460872945 and parameters: {}. Best is trial 0 with value: 0.09189397460872945.\u001b[0m\n","-------\n","0.09189397460872945\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.106113\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6882.25\teval's l2: 6043.75\n","loss____: 0.08542278876000634\n","\u001b[32m[I 2025-02-19 06:42:24,935]\u001b[0m Trial 0 finished with value: 0.08542278876000634 and parameters: {}. Best is trial 0 with value: 0.08542278876000634.\u001b[0m\n","-------\n","0.08542278876000634\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:25,035]\u001b[0m Trial 9 finished with value: 0.08742730964222405 and parameters: {'max_depth': 2, 'num_leaves': 34, 'learning_rate': 0.03488960745139221, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 0.6, 'colsample_bytree': 1.0}. Best is trial 7 with value: 0.07798246484771335.\u001b[0m\n","lgbreg_TRIAL_10 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_11 strat...\n","\u001b[32m[I 2025-02-19 06:42:25,080]\u001b[0m A new study created in memory with name: no-name-85f2f5c0-9e63-449a-b8b9-ef99efed2735\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:25,080]\u001b[0m A new study created in memory with name: no-name-30935278-50ce-43e9-be00-7e8c2debdd89\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:25,080]\u001b[0m A new study created in memory with name: no-name-cd13522f-2d10-4657-a859-2b6e30d426b8\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 68, 'learning_rate': 0.09322601292624808, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 737.087659\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3891.57\teval's l2: 3576.09\n","loss____: 0.06371227845806543\n","\u001b[32m[I 2025-02-19 06:42:25,348]\u001b[0m Trial 0 finished with value: 0.06371227845806543 and parameters: {}. Best is trial 0 with value: 0.06371227845806543.\u001b[0m\n","-------\n","0.06371227845806543\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.064591\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3873.21\teval's l2: 3494.84\n","loss____: 0.06701422779558917\n","\u001b[32m[I 2025-02-19 06:42:25,374]\u001b[0m Trial 0 finished with value: 0.06701422779558917 and parameters: {}. Best is trial 0 with value: 0.06701422779558917.\u001b[0m\n","-------\n","0.06701422779558917\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 732.923875\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3543.74\teval's l2: 5956.16\n","loss____: 0.06112142159186901\n","\u001b[32m[I 2025-02-19 06:42:25,399]\u001b[0m Trial 0 finished with value: 0.06112142159186901 and parameters: {}. Best is trial 0 with value: 0.06112142159186901.\u001b[0m\n","-------\n","0.06112142159186901\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:25,472]\u001b[0m Trial 10 finished with value: 0.06394930928184121 and parameters: {'max_depth': 3, 'num_leaves': 68, 'learning_rate': 0.09322601292624808, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}. Best is trial 10 with value: 0.06394930928184121.\u001b[0m\n","lgbreg_TRIAL_11 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_12 strat...\n","\u001b[32m[I 2025-02-19 06:42:25,517]\u001b[0m A new study created in memory with name: no-name-a135f100-cf1a-4fc1-8911-b2f5a74dba82\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:25,517]\u001b[0m A new study created in memory with name: no-name-8e1fca35-01d5-4f37-beaf-5dbf4eb4e382\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:25,517]\u001b[0m A new study created in memory with name: no-name-42f53a27-5088-4ddf-8b6f-5ab9a70c0176\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 70, 'learning_rate': 0.09944100570862976, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.726644\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3760.56\teval's l2: 3415.25\n","loss____: 0.06188583637665616\n","\u001b[32m[I 2025-02-19 06:42:25,774]\u001b[0m Trial 0 finished with value: 0.06188583637665616 and parameters: {}. Best is trial 0 with value: 0.06188583637665616.\u001b[0m\n","-------\n","0.06188583637665616\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000117 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.732411\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3716.65\teval's l2: 3434.19\n","loss____: 0.0650115872871508\n","\u001b[32m[I 2025-02-19 06:42:25,800]\u001b[0m Trial 0 finished with value: 0.0650115872871508 and parameters: {}. Best is trial 0 with value: 0.0650115872871508.\u001b[0m\n","-------\n","0.0650115872871508\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.498270\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3616.06\teval's l2: 4511.86\n","loss____: 0.06034158290423694\n","\u001b[32m[I 2025-02-19 06:42:25,828]\u001b[0m Trial 0 finished with value: 0.06034158290423694 and parameters: {}. Best is trial 0 with value: 0.06034158290423694.\u001b[0m\n","-------\n","0.06034158290423694\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:25,891]\u001b[0m Trial 11 finished with value: 0.06241300218934797 and parameters: {'max_depth': 3, 'num_leaves': 70, 'learning_rate': 0.09944100570862976, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}. Best is trial 11 with value: 0.06241300218934797.\u001b[0m\n","lgbreg_TRIAL_12 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_13 strat...\n","\u001b[32m[I 2025-02-19 06:42:25,939]\u001b[0m A new study created in memory with name: no-name-1ed1299b-5a32-4aa2-818e-e3b20f525534\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:25,939]\u001b[0m A new study created in memory with name: no-name-bf215dab-2f23-41c8-bb5d-6e82a08840b2\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:25,939]\u001b[0m A new study created in memory with name: no-name-e514560c-c042-47dd-89df-98c8881a30bc\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 67, 'learning_rate': 0.09890395348990107, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000111 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.809689\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3681.24\teval's l2: 4197.67\n","loss____: 0.06225108673166097\n","\u001b[32m[I 2025-02-19 06:42:26,202]\u001b[0m Trial 0 finished with value: 0.06225108673166097 and parameters: {}. Best is trial 0 with value: 0.06225108673166097.\u001b[0m\n","-------\n","0.06225108673166097\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.455594\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3629.44\teval's l2: 3968.74\n","loss____: 0.0644819752160643\n","\u001b[32m[I 2025-02-19 06:42:26,231]\u001b[0m Trial 0 finished with value: 0.0644819752160643 and parameters: {}. Best is trial 0 with value: 0.0644819752160643.\u001b[0m\n","-------\n","0.0644819752160643\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.693195\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3653.08\teval's l2: 3834.31\n","loss____: 0.059618572979232135\n","\u001b[32m[I 2025-02-19 06:42:26,264]\u001b[0m Trial 0 finished with value: 0.059618572979232135 and parameters: {}. Best is trial 0 with value: 0.059618572979232135.\u001b[0m\n","-------\n","0.059618572979232135\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:26,360]\u001b[0m Trial 12 finished with value: 0.06211721164231913 and parameters: {'max_depth': 3, 'num_leaves': 67, 'learning_rate': 0.09890395348990107, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 12 with value: 0.06211721164231913.\u001b[0m\n","lgbreg_TRIAL_13 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_14 strat...\n","\u001b[32m[I 2025-02-19 06:42:26,404]\u001b[0m A new study created in memory with name: no-name-49cf21b0-35e3-48c0-b6ec-e8dfe2042d6c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:26,404]\u001b[0m A new study created in memory with name: no-name-39b36153-29ae-4ee7-b834-25ffe2dbc3a5\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:26,405]\u001b[0m A new study created in memory with name: no-name-5ecc24c5-2c43-425d-8cf6-ada8d86be91c\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'num_leaves': 75, 'learning_rate': 0.0896977020441668, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.566321\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3506.18\teval's l2: 5117.88\n","loss____: 0.061291337093407845\n","\u001b[32m[I 2025-02-19 06:42:26,662]\u001b[0m Trial 0 finished with value: 0.061291337093407845 and parameters: {}. Best is trial 0 with value: 0.061291337093407845.\u001b[0m\n","-------\n","0.061291337093407845\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.459054\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3673.44\teval's l2: 3774.52\n","loss____: 0.06570126998885557\n","\u001b[32m[I 2025-02-19 06:42:26,688]\u001b[0m Trial 0 finished with value: 0.06570126998885557 and parameters: {}. Best is trial 0 with value: 0.06570126998885557.\u001b[0m\n","-------\n","0.06570126998885557\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.741638\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3648.79\teval's l2: 4179.8\n","loss____: 0.059975048364780514\n","\u001b[32m[I 2025-02-19 06:42:26,715]\u001b[0m Trial 0 finished with value: 0.059975048364780514 and parameters: {}. Best is trial 0 with value: 0.059975048364780514.\u001b[0m\n","-------\n","0.059975048364780514\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:26,813]\u001b[0m Trial 13 finished with value: 0.06232255181568131 and parameters: {'max_depth': 4, 'num_leaves': 75, 'learning_rate': 0.0896977020441668, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 12 with value: 0.06211721164231913.\u001b[0m\n","lgbreg_TRIAL_14 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_15 strat...\n","\u001b[32m[I 2025-02-19 06:42:26,867]\u001b[0m A new study created in memory with name: no-name-2d03d22b-a5b3-4e67-af32-f18e098e3bfb\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:26,868]\u001b[0m A new study created in memory with name: no-name-7a7c91a5-941c-42c1-a298-26da1eaf24cd\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:26,868]\u001b[0m A new study created in memory with name: no-name-b547cd99-0da5-4f71-b0bf-d20d5180a1a7\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'num_leaves': 50, 'learning_rate': 0.0744223022680386, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000109 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.355248\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4146.06\teval's l2: 4218.74\n","loss____: 0.06600784418548783\n","\u001b[32m[I 2025-02-19 06:42:27,159]\u001b[0m Trial 0 finished with value: 0.06600784418548783 and parameters: {}. Best is trial 0 with value: 0.06600784418548783.\u001b[0m\n","-------\n","0.06600784418548783\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.184544\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4124.03\teval's l2: 4373.18\n","loss____: 0.07009591395521096\n","\u001b[32m[I 2025-02-19 06:42:27,188]\u001b[0m Trial 0 finished with value: 0.07009591395521096 and parameters: {}. Best is trial 0 with value: 0.07009591395521096.\u001b[0m\n","-------\n","0.07009591395521096\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 736.712803\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4309.57\teval's l2: 3116.76\n","loss____: 0.0640924023126443\n","\u001b[32m[I 2025-02-19 06:42:27,215]\u001b[0m Trial 0 finished with value: 0.0640924023126443 and parameters: {}. Best is trial 0 with value: 0.0640924023126443.\u001b[0m\n","-------\n","0.0640924023126443\n","-------\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:27,308]\u001b[0m Trial 14 finished with value: 0.0667320534844477 and parameters: {'max_depth': 4, 'num_leaves': 50, 'learning_rate': 0.0744223022680386, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 12 with value: 0.06211721164231913.\u001b[0m\n","lgbreg_TRIAL_15 done...\n","\n","◎ level_1_lgbreg_GLOBAL_TRIAL_16 strat...\n","\u001b[32m[I 2025-02-19 06:42:27,351]\u001b[0m A new study created in memory with name: no-name-e4049dee-e6d2-415e-b436-599f4621e428\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:27,352]\u001b[0m A new study created in memory with name: no-name-734826b6-51c0-463b-bfb0-1d09276ed3f9\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:27,352]\u001b[0m A new study created in memory with name: no-name-ed183990-a09f-4d18-bb12-125304070311\u001b[0m\n","lgbreg\n","///// now...level_1_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'num_leaves': 83, 'learning_rate': 0.07230359677068766, 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9, 'colsample_bytree': 0.9}\n","\n","///// now...level_1_lgbreg_local_study_1_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000030 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.420992\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4282.3\teval's l2: 3859.85\n","loss____: 0.06644148766923935\n","\u001b[32m[I 2025-02-19 06:42:27,605]\u001b[0m Trial 0 finished with value: 0.06644148766923935 and parameters: {}. Best is trial 0 with value: 0.06644148766923935.\u001b[0m\n","-------\n","0.06644148766923935\n","-------\n","\n","///// now...level_1_lgbreg_local_study_2_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 733.640138\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4187.54\teval's l2: 4142.2\n","loss____: 0.06952853173743027\n","\u001b[32m[I 2025-02-19 06:42:27,633]\u001b[0m Trial 0 finished with value: 0.06952853173743027 and parameters: {}. Best is trial 0 with value: 0.06952853173743027.\u001b[0m\n","-------\n","0.06952853173743027\n","-------\n","\n","///// now...level_1_lgbreg_local_study_3_fold_cv /////\n","★ level_1_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.452134\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4198.28\teval's l2: 4275.06\n","loss____: 0.06465101808128705\n","\u001b[32m[I 2025-02-19 06:42:27,659]\u001b[0m Trial 0 finished with value: 0.06465101808128705 and parameters: {}. Best is trial 0 with value: 0.06465101808128705.\u001b[0m\n","-------\n","0.06465101808128705\n","-------\n","設定したn_trials50に達することなくstudyを終了します。\n","16回目のトライアルで終了しました。\n","///// now...level_1_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:27,751]\u001b[0m Trial 15 finished with value: 0.06687367916265223 and parameters: {'max_depth': 4, 'num_leaves': 83, 'learning_rate': 0.07230359677068766, 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 12 with value: 0.06211721164231913.\u001b[0m\n","lgbreg_TRIAL_16 done...\n","///// model with optimal parameters 訓練中... //////\n","\n","///// LV1_lgbreg_model_with_best_params /////\n","///// 1_fold_cv... /////\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 735.223760\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4267.56\teval's l2: 3964.31\n","///// LV1_lgbreg_model_with_best_params /////\n","///// 2_fold_cv... /////\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000030 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 734.673587\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4168.24\teval's l2: 4476.79\n","///// LV1_lgbreg_model_with_best_params /////\n","///// 3_fold_cv... /////\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 144\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 11\n","[LightGBM] [Info] Start training from score 737.608997\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4344.26\teval's l2: 3442.17\n","[801.75389773 724.22359015 724.22359015 724.22359015 724.22359015\n"," 746.29837585 773.73118003 801.75389773 724.22359015 724.22359015\n"," 724.22359015 724.22359015 746.29837585 773.73118003 801.75389773\n"," 724.22359015 724.22359015 724.22359015 724.22359015 745.53363634\n"," 771.1846968  799.2074145  724.22359015 724.22359015 724.22359015\n"," 724.22359015 745.53363634 771.1846968  800.98915822 724.22359015\n"," 724.22359015 724.22359015 724.22359015 746.29837585 773.73118003\n"," 801.75389773 724.22359015 724.22359015 724.22359015 724.22359015\n"," 746.29837585 773.73118003 801.75389773 724.22359015 724.22359015\n"," 724.22359015 724.22359015 745.53363634 772.96644052 799.2074145\n"," 724.22359015 724.22359015 724.22359015 724.22359015 745.53363634\n"," 771.1846968  799.2074145  724.22359015 724.22359015 716.1793178\n"," 716.1793178  738.83216872 772.72945337 800.75217106 716.1793178\n"," 716.1793178  716.1793178  716.1793178  738.83216872 772.72945337\n"," 800.75217106 716.1793178  716.1793178  716.1793178  716.1793178\n"," 738.06742921 771.96471386 798.20568783 716.1793178  716.1793178\n"," 716.1793178  716.1793178  738.06742921 770.18297014 798.20568783\n"," 716.1793178  716.1793178  716.1793178  716.1793178  738.83216872\n"," 762.25714342 790.27986112 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.90287912 762.25714342 790.27986112 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.90287912 762.25714342\n"," 789.51512161 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.13813961 759.71066019 787.73337789 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.13813961 761.49240391 789.51512161\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.90287912\n"," 762.25714342 790.27986112 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.90287912 762.25714342 790.27986112 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.13813961 759.71066019\n"," 787.73337789 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.13813961 759.71066019 787.73337789 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 762.25714342 790.27986112\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.90287912\n"," 762.25714342 790.27986112 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 761.49240391 789.51512161 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.13813961 759.71066019\n"," 787.73337789 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.13813961 762.25714342 790.27986112 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 762.25714342 790.27986112\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.90287912\n"," 762.25714342 789.51512161 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 759.71066019 787.73337789 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.13813961 761.49240391\n"," 789.51512161 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.90287912 762.25714342 790.27986112 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 762.25714342 790.27986112\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.13813961\n"," 759.71066019 787.73337789 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 759.71066019 787.73337789 714.42266612\n"," 714.42266612 714.42266612 714.42266612 738.07682142 767.47677133\n"," 795.49948903 716.1793178  716.1793178  716.1793178  716.1793178\n"," 738.07682142 767.47677133 795.49948903 716.1793178  716.1793178\n"," 716.1793178  716.1793178  738.07682142 766.71203182 794.73474952\n"," 716.1793178  716.1793178  716.1793178  716.1793178  737.31208191\n"," 764.9302881  792.9530058  716.1793178  716.1793178  716.1793178\n"," 716.1793178  737.31208191 766.71203182 801.75389773 722.97867947\n"," 722.97867947 722.97867947 722.97867947 745.05346516 773.73118003\n"," 801.75389773 722.97867947 722.97867947 722.97867947 722.97867947\n"," 745.05346516 773.73118003 801.75389773 722.97867947 722.97867947\n"," 722.97867947 722.97867947 744.28872565 771.1846968  799.2074145\n"," 722.97867947 722.97867947 722.97867947 722.97867947 744.28872565\n"," 771.1846968  800.98915822 722.97867947 722.97867947 722.97867947\n"," 722.97867947 745.05346516 773.73118003 801.75389773 722.97867947\n"," 722.97867947 722.97867947 722.97867947 745.05346516 773.73118003\n"," 801.75389773 722.97867947 722.97867947 722.97867947 722.97867947\n"," 744.28872565 772.96644052 799.2074145  722.97867947 722.97867947\n"," 722.97867947 722.97867947 744.28872565 771.1846968  799.2074145\n"," 722.97867947 722.97867947 722.97867947 722.97867947 746.29837585\n"," 773.73118003 801.75389773 724.22359015 724.22359015 724.22359015\n"," 724.22359015 746.29837585 773.73118003 801.75389773 724.22359015\n"," 724.22359015 724.22359015 724.22359015 746.29837585 772.96644052\n"," 800.98915822 724.22359015 724.22359015 724.22359015 724.22359015\n"," 745.53363634 771.1846968  799.2074145  724.22359015 724.22359015\n"," 724.22359015 724.22359015 745.53363634 772.96644052 801.75389773\n"," 724.22359015 724.22359015 724.22359015 724.22359015 746.29837585\n"," 773.73118003 801.75389773 724.22359015 724.22359015 724.22359015\n"," 724.22359015 746.29837585 773.73118003 801.75389773 724.22359015\n"," 724.22359015 724.22359015 724.22359015 745.53363634 771.1846968\n"," 799.2074145  724.22359015 724.22359015 724.22359015 724.22359015\n"," 745.53363634 771.1846968  799.2074145  724.22359015 724.22359015\n"," 724.22359015 724.22359015 746.29837585 773.73118003 801.75389773\n"," 724.22359015 724.22359015 724.22359015 724.22359015 746.29837585\n"," 773.73118003 801.75389773 724.22359015 724.22359015 724.22359015\n"," 724.22359015 745.53363634 772.96644052 800.98915822 724.22359015\n"," 724.22359015 724.22359015 724.22359015 745.53363634 771.1846968\n"," 799.2074145  724.22359015 724.22359015 724.22359015 716.1793178\n"," 738.83216872 772.72945337 800.75217106 716.1793178  716.1793178\n"," 716.1793178  716.1793178  738.83216872 772.72945337 800.75217106\n"," 716.1793178  716.1793178  716.1793178  716.1793178  738.06742921\n"," 771.96471386 799.98743155 716.1793178  716.1793178  716.1793178\n"," 716.1793178  738.06742921 770.18297014 798.20568783 716.1793178\n"," 716.1793178  716.1793178  716.1793178  738.06742921 772.72945337\n"," 790.27986112 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.90287912 762.25714342 790.27986112 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 762.25714342 790.27986112\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.13813961\n"," 759.71066019 787.73337789 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 759.71066019 789.51512161 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.90287912 762.25714342\n"," 790.27986112 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.90287912 762.25714342 790.27986112 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.13813961 759.71066019 787.73337789\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.13813961\n"," 759.71066019 787.73337789 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.90287912 762.25714342 790.27986112 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.90287912 762.25714342\n"," 790.27986112 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.90287912 761.49240391 789.51512161 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.13813961 759.71066019 787.73337789\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.13813961\n"," 761.49240391 790.27986112 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.90287912 762.25714342 790.27986112 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.90287912 762.25714342\n"," 790.27986112 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.13813961 759.71066019 787.73337789 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.13813961 759.71066019 789.51512161\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.90287912\n"," 762.25714342 790.27986112 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.90287912 762.25714342 790.27986112 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.13813961 761.49240391\n"," 787.73337789 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.13813961 759.71066019 787.73337789 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 767.47677133 795.49948903\n"," 716.1793178  716.1793178  716.1793178  716.1793178  738.07682142\n"," 767.47677133 795.49948903 716.1793178  716.1793178  716.1793178\n"," 716.1793178  738.07682142 767.47677133 794.73474952 716.1793178\n"," 716.1793178  716.1793178  716.1793178  737.31208191 764.9302881\n"," 792.9530058  716.1793178  716.1793178  716.1793178  716.1793178\n"," 737.31208191 766.71203182 794.73474952 722.97867947 722.97867947\n"," 722.97867947 722.97867947 745.05346516 773.73118003 801.75389773\n"," 722.97867947 722.97867947 722.97867947 722.97867947 745.05346516\n"," 773.73118003 801.75389773 722.97867947 722.97867947 722.97867947\n"," 722.97867947 744.28872565 771.1846968  799.2074145  722.97867947\n"," 722.97867947 722.97867947 722.97867947 744.28872565 771.1846968\n"," 799.2074145  722.97867947 722.97867947 722.97867947 722.97867947\n"," 745.05346516 773.73118003 801.75389773 722.97867947 722.97867947\n"," 722.97867947 722.97867947 745.05346516 773.73118003 801.75389773\n"," 722.97867947 722.97867947 722.97867947 722.97867947 744.28872565\n"," 772.96644052 800.98915822 722.97867947 722.97867947 722.97867947\n"," 722.97867947 744.28872565 771.1846968  799.2074145  722.97867947\n"," 722.97867947 722.97867947 722.97867947 744.28872565 773.73118003\n"," 801.75389773 724.22359015 724.22359015 724.22359015 724.22359015\n"," 746.29837585 773.73118003 801.75389773 724.22359015 724.22359015\n"," 724.22359015 724.22359015 746.29837585 773.73118003 800.98915822\n"," 724.22359015 724.22359015 724.22359015 724.22359015 745.53363634\n"," 771.1846968  799.2074145  724.22359015 724.22359015 724.22359015\n"," 724.22359015 745.53363634 772.96644052 800.98915822 724.22359015\n"," 724.22359015 724.22359015 724.22359015 746.29837585 773.73118003\n"," 801.75389773 724.22359015 724.22359015 724.22359015 724.22359015\n"," 746.29837585 773.73118003 801.75389773 724.22359015 724.22359015\n"," 724.22359015 724.22359015 745.53363634 771.1846968  799.2074145\n"," 724.22359015 724.22359015 724.22359015 724.22359015 745.53363634\n"," 771.1846968  799.2074145  724.22359015 724.22359015 724.22359015\n"," 724.22359015 746.29837585 773.73118003 801.75389773 724.22359015\n"," 724.22359015 724.22359015 724.22359015 746.29837585 773.73118003\n"," 801.75389773 724.22359015 724.22359015 724.22359015 724.22359015\n"," 746.29837585 772.96644052 800.98915822 724.22359015 724.22359015\n"," 724.22359015 724.22359015 745.53363634 771.1846968  799.2074145\n"," 724.22359015 724.22359015 724.22359015 724.22359015 738.83216872\n"," 772.72945337 800.75217106 716.1793178  716.1793178  716.1793178\n"," 716.1793178  738.83216872 772.72945337 800.75217106 716.1793178\n"," 716.1793178  716.1793178  716.1793178  738.83216872 771.96471386\n"," 799.98743155 716.1793178  716.1793178  716.1793178  716.1793178\n"," 738.06742921 770.18297014 798.20568783 716.1793178  716.1793178\n"," 716.1793178  716.1793178  738.06742921 771.96471386 800.75217106\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.90287912\n"," 762.25714342 790.27986112 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.90287912 762.25714342 790.27986112 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.13813961 759.71066019\n"," 787.73337789 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.13813961 759.71066019 787.73337789 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 762.25714342 790.27986112\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.90287912\n"," 762.25714342 790.27986112 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 761.49240391 787.73337789 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.13813961 759.71066019\n"," 787.73337789 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.90287912 762.25714342 790.27986112 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 762.25714342 790.27986112\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.90287912\n"," 762.25714342 789.51512161 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 759.71066019 787.73337789 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.13813961 761.49240391\n"," 789.51512161 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.90287912 762.25714342 790.27986112 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.90287912 762.25714342 790.27986112\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.13813961\n"," 759.71066019 787.73337789 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 759.71066019 787.73337789 714.42266612\n"," 714.42266612 714.42266612 714.42266612 736.90287912 762.25714342\n"," 790.27986112 714.42266612 714.42266612 714.42266612 714.42266612\n"," 736.90287912 762.25714342 790.27986112 714.42266612 714.42266612\n"," 714.42266612 714.42266612 736.13813961 761.49240391 789.51512161\n"," 714.42266612 714.42266612 714.42266612 714.42266612 736.13813961\n"," 759.71066019 787.73337789 714.42266612 714.42266612 714.42266612\n"," 714.42266612 736.13813961 762.25714342 795.49948903 716.1793178\n"," 716.1793178  716.1793178  716.1793178  738.07682142 767.47677133\n"," 795.49948903 716.1793178  716.1793178  716.1793178  716.1793178\n"," 738.07682142 767.47677133 795.49948903 716.1793178  716.1793178\n"," 716.1793178  716.1793178  737.31208191 764.9302881  792.9530058\n"," 716.1793178  716.1793178  716.1793178  716.1793178  737.31208191]\n","\tDone!\n","\n","///// catregの学習をSTART ! /////\n","///// now... level_1 //////\n","///// search for catreg /////\n","\u001b[32m[I 2025-02-19 06:42:28,590]\u001b[0m A new study created in RDB with name: study_ens_level_1_catreg\u001b[0m\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_1 strat...\n","\u001b[32m[I 2025-02-19 06:42:28,721]\u001b[0m A new study created in memory with name: no-name-f34c85eb-9ee6-48ab-a782-90d43bc99603\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:28,721]\u001b[0m A new study created in memory with name: no-name-979d2314-88d2-4ac1-acbf-778802963fd4\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:28,721]\u001b[0m A new study created in memory with name: no-name-6b997863-f200-4fa5-8e14-530323def589\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'learning_rate': 0.08927180304353628, 'l2_leaf_reg': 0.6000000000000001, 'subsample': 0.8}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1018165\ttest: 0.1010204\ttest1: 0.1038256\tbest: 0.1038256 (0)\ttotal: 48.6ms\tremaining: 437ms\n","1:\tlearn: 0.0971507\ttest: 0.0962259\ttest1: 0.0991193\tbest: 0.0991193 (1)\ttotal: 49.4ms\tremaining: 197ms\n","2:\tlearn: 0.0933039\ttest: 0.0921069\ttest1: 0.0953511\tbest: 0.0953511 (2)\ttotal: 50.1ms\tremaining: 117ms\n","3:\tlearn: 0.0896614\ttest: 0.0883599\ttest1: 0.0917225\tbest: 0.0917225 (3)\ttotal: 50.8ms\tremaining: 76.2ms\n","4:\tlearn: 0.0861025\ttest: 0.0846754\ttest1: 0.0881615\tbest: 0.0881615 (4)\ttotal: 51.3ms\tremaining: 51.3ms\n","5:\tlearn: 0.0827730\ttest: 0.0812973\ttest1: 0.0856290\tbest: 0.0856290 (5)\ttotal: 51.8ms\tremaining: 34.6ms\n","6:\tlearn: 0.0802094\ttest: 0.0786229\ttest1: 0.0831600\tbest: 0.0831600 (6)\ttotal: 52.4ms\tremaining: 22.4ms\n","7:\tlearn: 0.0777355\ttest: 0.0759693\ttest1: 0.0812911\tbest: 0.0812911 (7)\ttotal: 52.9ms\tremaining: 13.2ms\n","8:\tlearn: 0.0753076\ttest: 0.0732500\ttest1: 0.0780545\tbest: 0.0780545 (8)\ttotal: 53.5ms\tremaining: 5.94ms\n","9:\tlearn: 0.0729661\ttest: 0.0707842\ttest1: 0.0758441\tbest: 0.0758441 (9)\ttotal: 54ms\tremaining: 0us\n","\n","bestTest = 0.07584409138\n","bestIteration = 9\n","\n","loss____: 0.07022241429791226\n","\u001b[32m[I 2025-02-19 06:42:29,009]\u001b[0m Trial 0 finished with value: 0.07022241429791226 and parameters: {}. Best is trial 0 with value: 0.07022241429791226.\u001b[0m\n","-------\n","0.07022241429791226\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1017865\ttest: 0.1015979\ttest1: 0.0869053\tbest: 0.0869053 (0)\ttotal: 1.01ms\tremaining: 9.09ms\n","1:\tlearn: 0.0971243\ttest: 0.0968050\ttest1: 0.0829853\tbest: 0.0829853 (1)\ttotal: 1.85ms\tremaining: 7.4ms\n","2:\tlearn: 0.0926970\ttest: 0.0922703\ttest1: 0.0791390\tbest: 0.0791390 (2)\ttotal: 2.48ms\tremaining: 5.78ms\n","3:\tlearn: 0.0890039\ttest: 0.0884232\ttest1: 0.0761427\tbest: 0.0761427 (3)\ttotal: 2.97ms\tremaining: 4.45ms\n","4:\tlearn: 0.0854475\ttest: 0.0848373\ttest1: 0.0731039\tbest: 0.0731039 (4)\ttotal: 3.51ms\tremaining: 3.51ms\n","5:\tlearn: 0.0819999\ttest: 0.0812837\ttest1: 0.0696584\tbest: 0.0696584 (5)\ttotal: 4.02ms\tremaining: 2.68ms\n","6:\tlearn: 0.0791005\ttest: 0.0783159\ttest1: 0.0677700\tbest: 0.0677700 (6)\ttotal: 4.7ms\tremaining: 2.01ms\n","7:\tlearn: 0.0765466\ttest: 0.0755915\ttest1: 0.0663976\tbest: 0.0663976 (7)\ttotal: 5.41ms\tremaining: 1.35ms\n","8:\tlearn: 0.0744022\ttest: 0.0731139\ttest1: 0.0642116\tbest: 0.0642116 (8)\ttotal: 6.09ms\tremaining: 676us\n","9:\tlearn: 0.0718167\ttest: 0.0704833\ttest1: 0.0615399\tbest: 0.0615399 (9)\ttotal: 6.57ms\tremaining: 0us\n","\n","bestTest = 0.0615399155\n","bestIteration = 9\n","\n","loss____: 0.07183366054113394\n","\u001b[32m[I 2025-02-19 06:42:29,086]\u001b[0m Trial 0 finished with value: 0.07183366054113394 and parameters: {}. Best is trial 0 with value: 0.07183366054113394.\u001b[0m\n","-------\n","0.07183366054113394\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.0990703\ttest: 0.0987158\ttest1: 0.1139379\tbest: 0.1139379 (0)\ttotal: 1.49ms\tremaining: 13.4ms\n","1:\tlearn: 0.0946836\ttest: 0.0941802\ttest1: 0.1092719\tbest: 0.1092719 (1)\ttotal: 2.15ms\tremaining: 8.59ms\n","2:\tlearn: 0.0908032\ttest: 0.0898964\ttest1: 0.1038306\tbest: 0.1038306 (2)\ttotal: 2.96ms\tremaining: 6.91ms\n","3:\tlearn: 0.0871866\ttest: 0.0862175\ttest1: 0.1001367\tbest: 0.1001367 (3)\ttotal: 3.54ms\tremaining: 5.32ms\n","4:\tlearn: 0.0841367\ttest: 0.0829718\ttest1: 0.0968982\tbest: 0.0968982 (4)\ttotal: 4.06ms\tremaining: 4.06ms\n","5:\tlearn: 0.0809169\ttest: 0.0796126\ttest1: 0.0924770\tbest: 0.0924770 (5)\ttotal: 5.2ms\tremaining: 3.47ms\n","6:\tlearn: 0.0781988\ttest: 0.0767286\ttest1: 0.0895982\tbest: 0.0895982 (6)\ttotal: 5.72ms\tremaining: 2.45ms\n","7:\tlearn: 0.0760294\ttest: 0.0742604\ttest1: 0.0865524\tbest: 0.0865524 (7)\ttotal: 6.26ms\tremaining: 1.56ms\n","8:\tlearn: 0.0739383\ttest: 0.0717985\ttest1: 0.0830365\tbest: 0.0830365 (8)\ttotal: 6.88ms\tremaining: 763us\n","9:\tlearn: 0.0717791\ttest: 0.0694665\ttest1: 0.0800373\tbest: 0.0800373 (9)\ttotal: 7.35ms\tremaining: 0us\n","\n","bestTest = 0.08003728101\n","bestIteration = 9\n","\n","loss____: 0.06876653232256033\n","\u001b[32m[I 2025-02-19 06:42:29,160]\u001b[0m Trial 0 finished with value: 0.06876653232256033 and parameters: {}. Best is trial 0 with value: 0.06876653232256033.\u001b[0m\n","-------\n","0.06876653232256033\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:29,252]\u001b[0m Trial 0 finished with value: 0.07027420238720218 and parameters: {'max_depth': 2, 'learning_rate': 0.08927180304353628, 'l2_leaf_reg': 0.6000000000000001, 'subsample': 0.8}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_1 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_2 strat...\n","\u001b[32m[I 2025-02-19 06:42:29,325]\u001b[0m A new study created in memory with name: no-name-69877a39-529c-4efc-92fa-286ca9678534\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:29,325]\u001b[0m A new study created in memory with name: no-name-4a9f11df-28c2-490c-a9a6-5b8903fd7ad9\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:29,326]\u001b[0m A new study created in memory with name: no-name-06b8b6da-cfb3-456b-b1d3-46fc18a34caf\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'learning_rate': 0.01432169828911152, 'l2_leaf_reg': 0.3, 'subsample': 1.0}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1042381\ttest: 0.1041812\ttest1: 0.1068177\tbest: 0.1068177 (0)\ttotal: 1ms\tremaining: 9.03ms\n","1:\tlearn: 0.1035385\ttest: 0.1034550\ttest1: 0.1058540\tbest: 0.1058540 (1)\ttotal: 1.53ms\tremaining: 6.12ms\n","2:\tlearn: 0.1028051\ttest: 0.1026994\ttest1: 0.1048570\tbest: 0.1048570 (2)\ttotal: 2.07ms\tremaining: 4.83ms\n","3:\tlearn: 0.1021493\ttest: 0.1019827\ttest1: 0.1039871\tbest: 0.1039871 (3)\ttotal: 2.66ms\tremaining: 3.99ms\n","4:\tlearn: 0.1013450\ttest: 0.1011514\ttest1: 0.1031318\tbest: 0.1031318 (4)\ttotal: 3.06ms\tremaining: 3.06ms\n","5:\tlearn: 0.1006398\ttest: 0.1003940\ttest1: 0.1023558\tbest: 0.1023558 (5)\ttotal: 3.5ms\tremaining: 2.33ms\n","6:\tlearn: 0.0998545\ttest: 0.0995891\ttest1: 0.1015203\tbest: 0.1015203 (6)\ttotal: 3.94ms\tremaining: 1.69ms\n","7:\tlearn: 0.0991718\ttest: 0.0988921\ttest1: 0.1005870\tbest: 0.1005870 (7)\ttotal: 4.41ms\tremaining: 1.1ms\n","8:\tlearn: 0.0985553\ttest: 0.0982303\ttest1: 0.0996963\tbest: 0.0996963 (8)\ttotal: 4.82ms\tremaining: 535us\n","9:\tlearn: 0.0978045\ttest: 0.0974517\ttest1: 0.0989922\tbest: 0.0989922 (9)\ttotal: 5.22ms\tremaining: 0us\n","\n","bestTest = 0.09899223938\n","bestIteration = 9\n","\n","loss____: 0.09598284657864382\n","\u001b[32m[I 2025-02-19 06:42:29,512]\u001b[0m Trial 0 finished with value: 0.09598284657864382 and parameters: {}. Best is trial 0 with value: 0.09598284657864382.\u001b[0m\n","-------\n","0.09598284657864382\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1044524\ttest: 0.1044308\ttest1: 0.1030734\tbest: 0.1030734 (0)\ttotal: 854us\tremaining: 7.69ms\n","1:\tlearn: 0.1035927\ttest: 0.1035560\ttest1: 0.1022633\tbest: 0.1022633 (1)\ttotal: 1.28ms\tremaining: 5.14ms\n","2:\tlearn: 0.1027909\ttest: 0.1027800\ttest1: 0.1015364\tbest: 0.1015364 (2)\ttotal: 1.97ms\tremaining: 4.59ms\n","3:\tlearn: 0.1020992\ttest: 0.1020228\ttest1: 0.1007930\tbest: 0.1007930 (3)\ttotal: 2.54ms\tremaining: 3.81ms\n","4:\tlearn: 0.1013823\ttest: 0.1012553\ttest1: 0.1000482\tbest: 0.1000482 (4)\ttotal: 3ms\tremaining: 3ms\n","5:\tlearn: 0.1005993\ttest: 0.1004526\ttest1: 0.0993208\tbest: 0.0993208 (5)\ttotal: 3.45ms\tremaining: 2.3ms\n","6:\tlearn: 0.0999395\ttest: 0.0997452\ttest1: 0.0986572\tbest: 0.0986572 (6)\ttotal: 3.83ms\tremaining: 1.64ms\n","7:\tlearn: 0.0991531\ttest: 0.0989400\ttest1: 0.0979294\tbest: 0.0979294 (7)\ttotal: 4.41ms\tremaining: 1.1ms\n","8:\tlearn: 0.0984724\ttest: 0.0982110\ttest1: 0.0972220\tbest: 0.0972220 (8)\ttotal: 5.01ms\tremaining: 557us\n","9:\tlearn: 0.0977191\ttest: 0.0974195\ttest1: 0.0965188\tbest: 0.0965188 (9)\ttotal: 5.45ms\tremaining: 0us\n","\n","bestTest = 0.09651875729\n","bestIteration = 9\n","\n","loss____: 0.10209410121767941\n","\u001b[32m[I 2025-02-19 06:42:29,591]\u001b[0m Trial 0 finished with value: 0.10209410121767941 and parameters: {}. Best is trial 0 with value: 0.10209410121767941.\u001b[0m\n","-------\n","0.10209410121767941\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1036608\ttest: 0.1036300\ttest1: 0.1077033\tbest: 0.1077033 (0)\ttotal: 819us\tremaining: 7.37ms\n","1:\tlearn: 0.1028801\ttest: 0.1028217\ttest1: 0.1070052\tbest: 0.1070052 (1)\ttotal: 1.36ms\tremaining: 5.46ms\n","2:\tlearn: 0.1021033\ttest: 0.1020214\ttest1: 0.1063096\tbest: 0.1063096 (2)\ttotal: 1.93ms\tremaining: 4.5ms\n","3:\tlearn: 0.1015560\ttest: 0.1013691\ttest1: 0.1057573\tbest: 0.1057573 (3)\ttotal: 2.53ms\tremaining: 3.79ms\n","4:\tlearn: 0.1007815\ttest: 0.1005547\ttest1: 0.1049259\tbest: 0.1049259 (4)\ttotal: 2.96ms\tremaining: 2.96ms\n","5:\tlearn: 0.1000192\ttest: 0.0997664\ttest1: 0.1042377\tbest: 0.1042377 (5)\ttotal: 3.38ms\tremaining: 2.25ms\n","6:\tlearn: 0.0994761\ttest: 0.0991057\ttest1: 0.1035219\tbest: 0.1035219 (6)\ttotal: 3.79ms\tremaining: 1.62ms\n","7:\tlearn: 0.0987387\ttest: 0.0983533\ttest1: 0.1028634\tbest: 0.1028634 (7)\ttotal: 4.24ms\tremaining: 1.06ms\n","8:\tlearn: 0.0979805\ttest: 0.0975672\ttest1: 0.1020217\tbest: 0.1020217 (8)\ttotal: 4.8ms\tremaining: 533us\n","9:\tlearn: 0.0972542\ttest: 0.0968081\ttest1: 0.1012417\tbest: 0.1012417 (9)\ttotal: 5.29ms\tremaining: 0us\n","\n","bestTest = 0.101241693\n","bestIteration = 9\n","\n","loss____: 0.0942818300218121\n","\u001b[32m[I 2025-02-19 06:42:29,664]\u001b[0m Trial 0 finished with value: 0.0942818300218121 and parameters: {}. Best is trial 0 with value: 0.0942818300218121.\u001b[0m\n","-------\n","0.0942818300218121\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:29,744]\u001b[0m Trial 1 finished with value: 0.09745292593937845 and parameters: {'max_depth': 2, 'learning_rate': 0.01432169828911152, 'l2_leaf_reg': 0.3, 'subsample': 1.0}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_2 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_3 strat...\n","\u001b[32m[I 2025-02-19 06:42:29,799]\u001b[0m A new study created in memory with name: no-name-cc9b6c41-7aaf-4512-8b49-9e8b38552bff\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:29,799]\u001b[0m A new study created in memory with name: no-name-8c546b10-ddfc-4ba1-b05e-d04bdc652ac9\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:29,799]\u001b[0m A new study created in memory with name: no-name-df1e8c90-1f42-44d9-af0e-559b87a7c25f\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'learning_rate': 0.051059032093947576, 'l2_leaf_reg': 0.3, 'subsample': 1.0}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1012825\ttest: 0.1011523\ttest1: 0.1059167\tbest: 0.1059167 (0)\ttotal: 1.09ms\tremaining: 9.82ms\n","1:\tlearn: 0.0981568\ttest: 0.0978872\ttest1: 0.1021400\tbest: 0.1021400 (1)\ttotal: 1.91ms\tremaining: 7.63ms\n","2:\tlearn: 0.0953309\ttest: 0.0950240\ttest1: 0.0993032\tbest: 0.0993032 (2)\ttotal: 2.6ms\tremaining: 6.06ms\n","3:\tlearn: 0.0927174\ttest: 0.0923521\ttest1: 0.0963765\tbest: 0.0963765 (3)\ttotal: 3.87ms\tremaining: 5.81ms\n","4:\tlearn: 0.0904112\ttest: 0.0898694\ttest1: 0.0932363\tbest: 0.0932363 (4)\ttotal: 4.69ms\tremaining: 4.69ms\n","5:\tlearn: 0.0879895\ttest: 0.0873837\ttest1: 0.0899466\tbest: 0.0899466 (5)\ttotal: 5.27ms\tremaining: 3.52ms\n","6:\tlearn: 0.0860185\ttest: 0.0853091\ttest1: 0.0876774\tbest: 0.0876774 (6)\ttotal: 5.91ms\tremaining: 2.53ms\n","7:\tlearn: 0.0841097\ttest: 0.0833445\ttest1: 0.0854370\tbest: 0.0854370 (7)\ttotal: 6.52ms\tremaining: 1.63ms\n","8:\tlearn: 0.0822654\ttest: 0.0815205\ttest1: 0.0833151\tbest: 0.0833151 (8)\ttotal: 7.1ms\tremaining: 789us\n","9:\tlearn: 0.0806496\ttest: 0.0798273\ttest1: 0.0814321\tbest: 0.0814321 (9)\ttotal: 7.77ms\tremaining: 0us\n","\n","bestTest = 0.08143211143\n","bestIteration = 9\n","\n","loss____: 0.07830884272307047\n","\u001b[32m[I 2025-02-19 06:42:30,014]\u001b[0m Trial 0 finished with value: 0.07830884272307047 and parameters: {}. Best is trial 0 with value: 0.07830884272307047.\u001b[0m\n","-------\n","0.07830884272307047\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1040035\ttest: 0.1036089\ttest1: 0.0895030\tbest: 0.0895030 (0)\ttotal: 928us\tremaining: 8.36ms\n","1:\tlearn: 0.1008991\ttest: 0.1002290\ttest1: 0.0859659\tbest: 0.0859659 (1)\ttotal: 1.64ms\tremaining: 6.57ms\n","2:\tlearn: 0.0983579\ttest: 0.0973649\ttest1: 0.0834575\tbest: 0.0834575 (2)\ttotal: 2.4ms\tremaining: 5.6ms\n","3:\tlearn: 0.0957281\ttest: 0.0946380\ttest1: 0.0806932\tbest: 0.0806932 (3)\ttotal: 2.94ms\tremaining: 4.4ms\n","4:\tlearn: 0.0934935\ttest: 0.0922527\ttest1: 0.0784983\tbest: 0.0784983 (4)\ttotal: 3.52ms\tremaining: 3.52ms\n","5:\tlearn: 0.0910324\ttest: 0.0895500\ttest1: 0.0756387\tbest: 0.0756387 (5)\ttotal: 4.09ms\tremaining: 2.73ms\n","6:\tlearn: 0.0890315\ttest: 0.0872952\ttest1: 0.0735443\tbest: 0.0735443 (6)\ttotal: 4.59ms\tremaining: 1.97ms\n","7:\tlearn: 0.0867666\ttest: 0.0849326\ttest1: 0.0713484\tbest: 0.0713484 (7)\ttotal: 5.13ms\tremaining: 1.28ms\n","8:\tlearn: 0.0847238\ttest: 0.0828171\ttest1: 0.0695015\tbest: 0.0695015 (8)\ttotal: 5.73ms\tremaining: 636us\n","9:\tlearn: 0.0830443\ttest: 0.0810412\ttest1: 0.0678409\tbest: 0.0678409 (9)\ttotal: 6.24ms\tremaining: 0us\n","\n","bestTest = 0.06784087251\n","bestIteration = 9\n","\n","loss____: 0.08311744404616285\n","\u001b[32m[I 2025-02-19 06:42:30,086]\u001b[0m Trial 0 finished with value: 0.08311744404616285 and parameters: {}. Best is trial 0 with value: 0.08311744404616285.\u001b[0m\n","-------\n","0.08311744404616285\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1022643\ttest: 0.1020865\ttest1: 0.0982848\tbest: 0.0982848 (0)\ttotal: 1.33ms\tremaining: 11.9ms\n","1:\tlearn: 0.0993167\ttest: 0.0989648\ttest1: 0.0955455\tbest: 0.0955455 (1)\ttotal: 2.21ms\tremaining: 8.83ms\n","2:\tlearn: 0.0963675\ttest: 0.0959401\ttest1: 0.0927919\tbest: 0.0927919 (2)\ttotal: 3.17ms\tremaining: 7.4ms\n","3:\tlearn: 0.0934252\ttest: 0.0928875\ttest1: 0.0901562\tbest: 0.0901562 (3)\ttotal: 4.05ms\tremaining: 6.08ms\n","4:\tlearn: 0.0910904\ttest: 0.0904847\ttest1: 0.0879582\tbest: 0.0879582 (4)\ttotal: 4.85ms\tremaining: 4.85ms\n","5:\tlearn: 0.0885633\ttest: 0.0877965\ttest1: 0.0856132\tbest: 0.0856132 (5)\ttotal: 5.76ms\tremaining: 3.84ms\n","6:\tlearn: 0.0863986\ttest: 0.0854850\ttest1: 0.0836299\tbest: 0.0836299 (6)\ttotal: 6.41ms\tremaining: 2.75ms\n","7:\tlearn: 0.0841705\ttest: 0.0831451\ttest1: 0.0816237\tbest: 0.0816237 (7)\ttotal: 6.99ms\tremaining: 1.75ms\n","8:\tlearn: 0.0823393\ttest: 0.0811660\ttest1: 0.0799680\tbest: 0.0799680 (8)\ttotal: 7.57ms\tremaining: 841us\n","9:\tlearn: 0.0806692\ttest: 0.0794298\ttest1: 0.0784170\tbest: 0.0784170 (9)\ttotal: 8.17ms\tremaining: 0us\n","\n","bestTest = 0.07841700445\n","bestIteration = 9\n","\n","loss____: 0.07728158551904703\n","\u001b[32m[I 2025-02-19 06:42:30,163]\u001b[0m Trial 0 finished with value: 0.07728158551904703 and parameters: {}. Best is trial 0 with value: 0.07728158551904703.\u001b[0m\n","-------\n","0.07728158551904703\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:30,244]\u001b[0m Trial 2 finished with value: 0.0795692907627601 and parameters: {'max_depth': 3, 'learning_rate': 0.051059032093947576, 'l2_leaf_reg': 0.3, 'subsample': 1.0}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_3 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_4 strat...\n","\u001b[32m[I 2025-02-19 06:42:30,309]\u001b[0m A new study created in memory with name: no-name-98ec47d8-67ae-4c5b-8dee-177541db7cad\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:30,309]\u001b[0m A new study created in memory with name: no-name-d44da1ad-fe0b-458b-a2a7-c6f30192e9ca\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:30,309]\u001b[0m A new study created in memory with name: no-name-f2c008b0-93d4-4a19-b3a4-96ad3065544c\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'learning_rate': 0.016305687346221478, 'l2_leaf_reg': 0.3, 'subsample': 0.6}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1036180\ttest: 0.1036385\ttest1: 0.1125536\tbest: 0.1125536 (0)\ttotal: 1.35ms\tremaining: 12.1ms\n","1:\tlearn: 0.1026375\ttest: 0.1025851\ttest1: 0.1113637\tbest: 0.1113637 (1)\ttotal: 2.49ms\tremaining: 9.95ms\n","2:\tlearn: 0.1016811\ttest: 0.1016184\ttest1: 0.1103214\tbest: 0.1103214 (2)\ttotal: 3.47ms\tremaining: 8.1ms\n","3:\tlearn: 0.1007538\ttest: 0.1006848\ttest1: 0.1093176\tbest: 0.1093176 (3)\ttotal: 4.34ms\tremaining: 6.51ms\n","4:\tlearn: 0.0997118\ttest: 0.0996100\ttest1: 0.1081757\tbest: 0.1081757 (4)\ttotal: 5.09ms\tremaining: 5.09ms\n","5:\tlearn: 0.0987471\ttest: 0.0986497\ttest1: 0.1070859\tbest: 0.1070859 (5)\ttotal: 5.99ms\tremaining: 3.99ms\n","6:\tlearn: 0.0977064\ttest: 0.0975922\ttest1: 0.1060057\tbest: 0.1060057 (6)\ttotal: 6.99ms\tremaining: 3ms\n","7:\tlearn: 0.0968322\ttest: 0.0965802\ttest1: 0.1048226\tbest: 0.1048226 (7)\ttotal: 8.87ms\tremaining: 2.22ms\n","8:\tlearn: 0.0958124\ttest: 0.0954748\ttest1: 0.1036397\tbest: 0.1036397 (8)\ttotal: 9.83ms\tremaining: 1.09ms\n","9:\tlearn: 0.0950013\ttest: 0.0947101\ttest1: 0.1027603\tbest: 0.1027603 (9)\ttotal: 10.6ms\tremaining: 0us\n","\n","bestTest = 0.1027602558\n","bestIteration = 9\n","\n","loss____: 0.09418606207084854\n","\u001b[32m[I 2025-02-19 06:42:30,537]\u001b[0m Trial 0 finished with value: 0.09418606207084854 and parameters: {}. Best is trial 0 with value: 0.09418606207084854.\u001b[0m\n","-------\n","0.09418606207084854\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1045671\ttest: 0.1045102\ttest1: 0.1011017\tbest: 0.1011017 (0)\ttotal: 1.2ms\tremaining: 10.8ms\n","1:\tlearn: 0.1035053\ttest: 0.1034309\ttest1: 0.1001421\tbest: 0.1001421 (1)\ttotal: 2.19ms\tremaining: 8.74ms\n","2:\tlearn: 0.1025017\ttest: 0.1023990\ttest1: 0.0991176\tbest: 0.0991176 (2)\ttotal: 2.94ms\tremaining: 6.86ms\n","3:\tlearn: 0.1014894\ttest: 0.1013007\ttest1: 0.0981104\tbest: 0.0981104 (3)\ttotal: 3.65ms\tremaining: 5.48ms\n","4:\tlearn: 0.1004616\ttest: 0.1001650\ttest1: 0.0969546\tbest: 0.0969546 (4)\ttotal: 4.36ms\tremaining: 4.36ms\n","5:\tlearn: 0.0994189\ttest: 0.0991011\ttest1: 0.0960110\tbest: 0.0960110 (5)\ttotal: 5.3ms\tremaining: 3.53ms\n","6:\tlearn: 0.0985192\ttest: 0.0981748\ttest1: 0.0950048\tbest: 0.0950048 (6)\ttotal: 6.18ms\tremaining: 2.65ms\n","7:\tlearn: 0.0976207\ttest: 0.0972747\ttest1: 0.0942222\tbest: 0.0942222 (7)\ttotal: 6.9ms\tremaining: 1.72ms\n","8:\tlearn: 0.0966079\ttest: 0.0962243\ttest1: 0.0933032\tbest: 0.0933032 (8)\ttotal: 7.64ms\tremaining: 849us\n","9:\tlearn: 0.0955843\ttest: 0.0951311\ttest1: 0.0922751\tbest: 0.0922751 (9)\ttotal: 8.45ms\tremaining: 0us\n","\n","bestTest = 0.09227509418\n","bestIteration = 9\n","\n","loss____: 0.0995863213772364\n","\u001b[32m[I 2025-02-19 06:42:30,611]\u001b[0m Trial 0 finished with value: 0.0995863213772364 and parameters: {}. Best is trial 0 with value: 0.0995863213772364.\u001b[0m\n","-------\n","0.0995863213772364\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1025974\ttest: 0.1025648\ttest1: 0.1140216\tbest: 0.1140216 (0)\ttotal: 1.19ms\tremaining: 10.7ms\n","1:\tlearn: 0.1016435\ttest: 0.1015603\ttest1: 0.1129247\tbest: 0.1129247 (1)\ttotal: 2.23ms\tremaining: 8.9ms\n","2:\tlearn: 0.1007622\ttest: 0.1006336\ttest1: 0.1119233\tbest: 0.1119233 (2)\ttotal: 3.2ms\tremaining: 7.47ms\n","3:\tlearn: 0.0996905\ttest: 0.0995031\ttest1: 0.1106953\tbest: 0.1106953 (3)\ttotal: 4.16ms\tremaining: 6.24ms\n","4:\tlearn: 0.0985807\ttest: 0.0983561\ttest1: 0.1094496\tbest: 0.1094496 (4)\ttotal: 5.16ms\tremaining: 5.16ms\n","5:\tlearn: 0.0976950\ttest: 0.0974328\ttest1: 0.1084228\tbest: 0.1084228 (5)\ttotal: 5.98ms\tremaining: 3.99ms\n","6:\tlearn: 0.0968505\ttest: 0.0965476\ttest1: 0.1074771\tbest: 0.1074771 (6)\ttotal: 6.77ms\tremaining: 2.9ms\n","7:\tlearn: 0.0959155\ttest: 0.0955893\ttest1: 0.1065027\tbest: 0.1065027 (7)\ttotal: 7.67ms\tremaining: 1.92ms\n","8:\tlearn: 0.0950217\ttest: 0.0946931\ttest1: 0.1055924\tbest: 0.1055924 (8)\ttotal: 8.47ms\tremaining: 941us\n","9:\tlearn: 0.0941594\ttest: 0.0938221\ttest1: 0.1047073\tbest: 0.1047073 (9)\ttotal: 9.13ms\tremaining: 0us\n","\n","bestTest = 0.1047073154\n","bestIteration = 9\n","\n","loss____: 0.09220012040117889\n","\u001b[32m[I 2025-02-19 06:42:30,692]\u001b[0m Trial 0 finished with value: 0.09220012040117889 and parameters: {}. Best is trial 0 with value: 0.09220012040117889.\u001b[0m\n","-------\n","0.09220012040117889\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:30,763]\u001b[0m Trial 3 finished with value: 0.09532416794975462 and parameters: {'max_depth': 4, 'learning_rate': 0.016305687346221478, 'l2_leaf_reg': 0.3, 'subsample': 0.6}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_4 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_5 strat...\n","\u001b[32m[I 2025-02-19 06:42:30,824]\u001b[0m A new study created in memory with name: no-name-850e9b03-dbd8-47d3-903d-12d5b2ab678a\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:30,825]\u001b[0m A new study created in memory with name: no-name-7b67158e-816c-4d0e-8f57-cfb71a1143a3\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:30,825]\u001b[0m A new study created in memory with name: no-name-9567876a-6ab4-4157-b969-175a27038ecb\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'learning_rate': 0.03347776308515933, 'l2_leaf_reg': 0.5, 'subsample': 0.7}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1040301\ttest: 0.1038968\ttest1: 0.1015499\tbest: 0.1015499 (0)\ttotal: 1.02ms\tremaining: 9.22ms\n","1:\tlearn: 0.1022085\ttest: 0.1020340\ttest1: 0.0997262\tbest: 0.0997262 (1)\ttotal: 1.79ms\tremaining: 7.16ms\n","2:\tlearn: 0.1004654\ttest: 0.1002148\ttest1: 0.0980755\tbest: 0.0980755 (2)\ttotal: 2.41ms\tremaining: 5.62ms\n","3:\tlearn: 0.0988363\ttest: 0.0985233\ttest1: 0.0965138\tbest: 0.0965138 (3)\ttotal: 2.94ms\tremaining: 4.42ms\n","4:\tlearn: 0.0971281\ttest: 0.0967298\ttest1: 0.0945366\tbest: 0.0945366 (4)\ttotal: 3.58ms\tremaining: 3.58ms\n","5:\tlearn: 0.0955850\ttest: 0.0950700\ttest1: 0.0927034\tbest: 0.0927034 (5)\ttotal: 4.08ms\tremaining: 2.72ms\n","6:\tlearn: 0.0940521\ttest: 0.0934907\ttest1: 0.0909498\tbest: 0.0909498 (6)\ttotal: 4.6ms\tremaining: 1.97ms\n","7:\tlearn: 0.0928957\ttest: 0.0919354\ttest1: 0.0892791\tbest: 0.0892791 (7)\ttotal: 5.12ms\tremaining: 1.28ms\n","8:\tlearn: 0.0914863\ttest: 0.0903760\ttest1: 0.0879040\tbest: 0.0879040 (8)\ttotal: 5.63ms\tremaining: 626us\n","9:\tlearn: 0.0898822\ttest: 0.0887133\ttest1: 0.0860898\tbest: 0.0860898 (9)\ttotal: 6.14ms\tremaining: 0us\n","\n","bestTest = 0.08608976847\n","bestIteration = 9\n","\n","loss____: 0.08653520525084399\n","\u001b[32m[I 2025-02-19 06:42:31,057]\u001b[0m Trial 0 finished with value: 0.08653520525084399 and parameters: {}. Best is trial 0 with value: 0.08653520525084399.\u001b[0m\n","-------\n","0.08653520525084399\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1042034\ttest: 0.1040704\ttest1: 0.0983702\tbest: 0.0983702 (0)\ttotal: 1.08ms\tremaining: 9.75ms\n","1:\tlearn: 0.1024713\ttest: 0.1022821\ttest1: 0.0964726\tbest: 0.0964726 (1)\ttotal: 1.88ms\tremaining: 7.51ms\n","2:\tlearn: 0.1006343\ttest: 0.1002481\ttest1: 0.0948582\tbest: 0.0948582 (2)\ttotal: 2.51ms\tremaining: 5.86ms\n","3:\tlearn: 0.0989637\ttest: 0.0984629\ttest1: 0.0930410\tbest: 0.0930410 (3)\ttotal: 3.02ms\tremaining: 4.53ms\n","4:\tlearn: 0.0974380\ttest: 0.0968809\ttest1: 0.0914487\tbest: 0.0914487 (4)\ttotal: 3.53ms\tremaining: 3.53ms\n","5:\tlearn: 0.0957503\ttest: 0.0951030\ttest1: 0.0899769\tbest: 0.0899769 (5)\ttotal: 4.01ms\tremaining: 2.68ms\n","6:\tlearn: 0.0943495\ttest: 0.0935942\ttest1: 0.0884330\tbest: 0.0884330 (6)\ttotal: 5.29ms\tremaining: 2.27ms\n","7:\tlearn: 0.0932497\ttest: 0.0919950\ttest1: 0.0868035\tbest: 0.0868035 (7)\ttotal: 5.81ms\tremaining: 1.45ms\n","8:\tlearn: 0.0917901\ttest: 0.0904280\ttest1: 0.0852032\tbest: 0.0852032 (8)\ttotal: 6.34ms\tremaining: 704us\n","9:\tlearn: 0.0902805\ttest: 0.0888228\ttest1: 0.0838580\tbest: 0.0838580 (9)\ttotal: 6.8ms\tremaining: 0us\n","\n","bestTest = 0.08385795754\n","bestIteration = 9\n","\n","loss____: 0.09226725085149057\n","\u001b[32m[I 2025-02-19 06:42:31,132]\u001b[0m Trial 0 finished with value: 0.09226725085149057 and parameters: {}. Best is trial 0 with value: 0.09226725085149057.\u001b[0m\n","-------\n","0.09226725085149057\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1027749\ttest: 0.1027334\ttest1: 0.1076261\tbest: 0.1076261 (0)\ttotal: 939us\tremaining: 8.45ms\n","1:\tlearn: 0.1010505\ttest: 0.1009340\ttest1: 0.1059081\tbest: 0.1059081 (1)\ttotal: 1.59ms\tremaining: 6.38ms\n","2:\tlearn: 0.0993930\ttest: 0.0991775\ttest1: 0.1041446\tbest: 0.1041446 (2)\ttotal: 2.16ms\tremaining: 5.05ms\n","3:\tlearn: 0.0977845\ttest: 0.0974744\ttest1: 0.1028270\tbest: 0.1028270 (3)\ttotal: 3.68ms\tremaining: 5.52ms\n","4:\tlearn: 0.0960767\ttest: 0.0957454\ttest1: 0.1013245\tbest: 0.1013245 (4)\ttotal: 4.23ms\tremaining: 4.23ms\n","5:\tlearn: 0.0946057\ttest: 0.0942674\ttest1: 0.1001365\tbest: 0.1001365 (5)\ttotal: 4.79ms\tremaining: 3.19ms\n","6:\tlearn: 0.0930698\ttest: 0.0927643\ttest1: 0.0987025\tbest: 0.0987025 (6)\ttotal: 5.42ms\tremaining: 2.32ms\n","7:\tlearn: 0.0917081\ttest: 0.0913235\ttest1: 0.0970547\tbest: 0.0970547 (7)\ttotal: 6.03ms\tremaining: 1.51ms\n","8:\tlearn: 0.0901936\ttest: 0.0897295\ttest1: 0.0957384\tbest: 0.0957384 (8)\ttotal: 7.38ms\tremaining: 819us\n","9:\tlearn: 0.0889098\ttest: 0.0883325\ttest1: 0.0942832\tbest: 0.0942832 (9)\ttotal: 7.92ms\tremaining: 0us\n","\n","bestTest = 0.09428324967\n","bestIteration = 9\n","\n","loss____: 0.08632125395147329\n","\u001b[32m[I 2025-02-19 06:42:31,215]\u001b[0m Trial 0 finished with value: 0.08632125395147329 and parameters: {}. Best is trial 0 with value: 0.08632125395147329.\u001b[0m\n","-------\n","0.08632125395147329\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:31,305]\u001b[0m Trial 4 finished with value: 0.08837457001793596 and parameters: {'max_depth': 2, 'learning_rate': 0.03347776308515933, 'l2_leaf_reg': 0.5, 'subsample': 0.7}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_5 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_6 strat...\n","\u001b[32m[I 2025-02-19 06:42:31,355]\u001b[0m A new study created in memory with name: no-name-49bbf277-8b30-4955-9cbc-731b69176aa9\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:31,355]\u001b[0m A new study created in memory with name: no-name-33d923f5-ba3c-45b8-a8db-9c2ff8dcd17c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:31,355]\u001b[0m A new study created in memory with name: no-name-35aadd89-0968-4708-96fe-46495dfb3745\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'learning_rate': 0.013787764619353767, 'l2_leaf_reg': 0.4, 'subsample': 0.7}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1032847\ttest: 0.1032197\ttest1: 0.1113323\tbest: 0.1113323 (0)\ttotal: 1.21ms\tremaining: 10.9ms\n","1:\tlearn: 0.1024592\ttest: 0.1023408\ttest1: 0.1103656\tbest: 0.1103656 (1)\ttotal: 2.01ms\tremaining: 8.05ms\n","2:\tlearn: 0.1016573\ttest: 0.1014983\ttest1: 0.1094298\tbest: 0.1094298 (2)\ttotal: 2.83ms\tremaining: 6.6ms\n","3:\tlearn: 0.1008673\ttest: 0.1006637\ttest1: 0.1084937\tbest: 0.1084937 (3)\ttotal: 3.66ms\tremaining: 5.49ms\n","4:\tlearn: 0.1000363\ttest: 0.0998073\ttest1: 0.1076323\tbest: 0.1076323 (4)\ttotal: 4.39ms\tremaining: 4.39ms\n","5:\tlearn: 0.0992169\ttest: 0.0989643\ttest1: 0.1067856\tbest: 0.1067856 (5)\ttotal: 5.05ms\tremaining: 3.37ms\n","6:\tlearn: 0.0984079\ttest: 0.0981169\ttest1: 0.1059057\tbest: 0.1059057 (6)\ttotal: 5.63ms\tremaining: 2.42ms\n","7:\tlearn: 0.0976411\ttest: 0.0973291\ttest1: 0.1050909\tbest: 0.1050909 (7)\ttotal: 6.26ms\tremaining: 1.56ms\n","8:\tlearn: 0.0968395\ttest: 0.0965094\ttest1: 0.1041127\tbest: 0.1041127 (8)\ttotal: 6.9ms\tremaining: 766us\n","9:\tlearn: 0.0961739\ttest: 0.0958280\ttest1: 0.1034776\tbest: 0.1034776 (9)\ttotal: 7.75ms\tremaining: 0us\n","\n","bestTest = 0.1034775862\n","bestIteration = 9\n","\n","loss____: 0.09505225364973097\n","\u001b[32m[I 2025-02-19 06:42:31,570]\u001b[0m Trial 0 finished with value: 0.09505225364973097 and parameters: {}. Best is trial 0 with value: 0.09505225364973097.\u001b[0m\n","-------\n","0.09505225364973097\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1039913\ttest: 0.1039094\ttest1: 0.1057329\tbest: 0.1057329 (0)\ttotal: 1.01ms\tremaining: 9.09ms\n","1:\tlearn: 0.1031024\ttest: 0.1029722\ttest1: 0.1048896\tbest: 0.1048896 (1)\ttotal: 1.64ms\tremaining: 6.57ms\n","2:\tlearn: 0.1022745\ttest: 0.1020713\ttest1: 0.1041376\tbest: 0.1041376 (2)\ttotal: 3.14ms\tremaining: 7.33ms\n","3:\tlearn: 0.1014372\ttest: 0.1011700\ttest1: 0.1033789\tbest: 0.1033789 (3)\ttotal: 3.77ms\tremaining: 5.65ms\n","4:\tlearn: 0.1006741\ttest: 0.1003484\ttest1: 0.1026320\tbest: 0.1026320 (4)\ttotal: 4.35ms\tremaining: 4.35ms\n","5:\tlearn: 0.0998962\ttest: 0.0994755\ttest1: 0.1018333\tbest: 0.1018333 (5)\ttotal: 5.03ms\tremaining: 3.35ms\n","6:\tlearn: 0.0990926\ttest: 0.0986348\ttest1: 0.1010663\tbest: 0.1010663 (6)\ttotal: 5.74ms\tremaining: 2.46ms\n","7:\tlearn: 0.0982935\ttest: 0.0977610\ttest1: 0.1003441\tbest: 0.1003441 (7)\ttotal: 6.33ms\tremaining: 1.58ms\n","8:\tlearn: 0.0974827\ttest: 0.0969321\ttest1: 0.0995889\tbest: 0.0995889 (8)\ttotal: 6.89ms\tremaining: 765us\n","9:\tlearn: 0.0967015\ttest: 0.0961201\ttest1: 0.0988763\tbest: 0.0988763 (9)\ttotal: 7.51ms\tremaining: 0us\n","\n","bestTest = 0.09887632144\n","bestIteration = 9\n","\n","loss____: 0.10133111342057906\n","\u001b[32m[I 2025-02-19 06:42:31,646]\u001b[0m Trial 0 finished with value: 0.10133111342057906 and parameters: {}. Best is trial 0 with value: 0.10133111342057906.\u001b[0m\n","-------\n","0.10133111342057906\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1044802\ttest: 0.1044690\ttest1: 0.1069307\tbest: 0.1069307 (0)\ttotal: 1.68ms\tremaining: 15.1ms\n","1:\tlearn: 0.1035901\ttest: 0.1035167\ttest1: 0.1060161\tbest: 0.1060161 (1)\ttotal: 2.4ms\tremaining: 9.6ms\n","2:\tlearn: 0.1027695\ttest: 0.1026253\ttest1: 0.1051655\tbest: 0.1051655 (2)\ttotal: 3.28ms\tremaining: 7.66ms\n","3:\tlearn: 0.1021090\ttest: 0.1018265\ttest1: 0.1044007\tbest: 0.1044007 (3)\ttotal: 3.93ms\tremaining: 5.89ms\n","4:\tlearn: 0.1012743\ttest: 0.1009788\ttest1: 0.1036563\tbest: 0.1036563 (4)\ttotal: 4.63ms\tremaining: 4.63ms\n","5:\tlearn: 0.1004297\ttest: 0.1001155\ttest1: 0.1028323\tbest: 0.1028323 (5)\ttotal: 5.3ms\tremaining: 3.54ms\n","6:\tlearn: 0.0997220\ttest: 0.0993904\ttest1: 0.1021980\tbest: 0.1021980 (6)\ttotal: 5.99ms\tremaining: 2.57ms\n","7:\tlearn: 0.0989174\ttest: 0.0985482\ttest1: 0.1013620\tbest: 0.1013620 (7)\ttotal: 6.72ms\tremaining: 1.68ms\n","8:\tlearn: 0.0981233\ttest: 0.0977276\ttest1: 0.1006292\tbest: 0.1006292 (8)\ttotal: 7.35ms\tremaining: 816us\n","9:\tlearn: 0.0973386\ttest: 0.0969210\ttest1: 0.0998721\tbest: 0.0998721 (9)\ttotal: 7.93ms\tremaining: 0us\n","\n","bestTest = 0.09987212907\n","bestIteration = 9\n","\n","loss____: 0.09392340125286748\n","\u001b[32m[I 2025-02-19 06:42:31,720]\u001b[0m Trial 0 finished with value: 0.09392340125286748 and parameters: {}. Best is trial 0 with value: 0.09392340125286748.\u001b[0m\n","-------\n","0.09392340125286748\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:31,792]\u001b[0m Trial 5 finished with value: 0.09676892277439249 and parameters: {'max_depth': 3, 'learning_rate': 0.013787764619353767, 'l2_leaf_reg': 0.4, 'subsample': 0.7}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_6 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_7 strat...\n","\u001b[32m[I 2025-02-19 06:42:31,838]\u001b[0m A new study created in memory with name: no-name-a1f8d107-b74a-4a3d-b697-e6028f0035b0\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:31,839]\u001b[0m A new study created in memory with name: no-name-c5e36ad0-3ae6-4246-be3e-e8e1858af1f9\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:31,839]\u001b[0m A new study created in memory with name: no-name-67b032df-8504-4f6f-8a38-2290f65612c7\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'learning_rate': 0.06097839109531514, 'l2_leaf_reg': 0.3, 'subsample': 0.8}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1018530\ttest: 0.1017057\ttest1: 0.0949160\tbest: 0.0949160 (0)\ttotal: 1.32ms\tremaining: 11.9ms\n","1:\tlearn: 0.0982568\ttest: 0.0979239\ttest1: 0.0916971\tbest: 0.0916971 (1)\ttotal: 2.05ms\tremaining: 8.21ms\n","2:\tlearn: 0.0952333\ttest: 0.0945980\ttest1: 0.0889491\tbest: 0.0889491 (2)\ttotal: 3.05ms\tremaining: 7.11ms\n","3:\tlearn: 0.0919870\ttest: 0.0912534\ttest1: 0.0861561\tbest: 0.0861561 (3)\ttotal: 3.93ms\tremaining: 5.89ms\n","4:\tlearn: 0.0889850\ttest: 0.0881560\ttest1: 0.0835440\tbest: 0.0835440 (4)\ttotal: 4.67ms\tremaining: 4.67ms\n","5:\tlearn: 0.0859727\ttest: 0.0849964\ttest1: 0.0808963\tbest: 0.0808963 (5)\ttotal: 5.36ms\tremaining: 3.57ms\n","6:\tlearn: 0.0836375\ttest: 0.0825838\ttest1: 0.0789109\tbest: 0.0789109 (6)\ttotal: 6.12ms\tremaining: 2.62ms\n","7:\tlearn: 0.0813840\ttest: 0.0801109\ttest1: 0.0765614\tbest: 0.0765614 (7)\ttotal: 6.8ms\tremaining: 1.7ms\n","8:\tlearn: 0.0793554\ttest: 0.0777767\ttest1: 0.0745712\tbest: 0.0745712 (8)\ttotal: 7.47ms\tremaining: 830us\n","9:\tlearn: 0.0774770\ttest: 0.0757828\ttest1: 0.0726479\tbest: 0.0726479 (9)\ttotal: 8.92ms\tremaining: 0us\n","\n","bestTest = 0.07264793255\n","bestIteration = 9\n","\n","loss____: 0.07428992996425396\n","\u001b[32m[I 2025-02-19 06:42:32,051]\u001b[0m Trial 0 finished with value: 0.07428992996425396 and parameters: {}. Best is trial 0 with value: 0.07428992996425396.\u001b[0m\n","-------\n","0.07428992996425396\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1034134\ttest: 0.1032303\ttest1: 0.0859462\tbest: 0.0859462 (0)\ttotal: 1.32ms\tremaining: 11.9ms\n","1:\tlearn: 0.0997089\ttest: 0.0993834\ttest1: 0.0829231\tbest: 0.0829231 (1)\ttotal: 1.96ms\tremaining: 7.84ms\n","2:\tlearn: 0.0963210\ttest: 0.0957958\ttest1: 0.0802226\tbest: 0.0802226 (2)\ttotal: 2.66ms\tremaining: 6.21ms\n","3:\tlearn: 0.0928440\ttest: 0.0920962\ttest1: 0.0772340\tbest: 0.0772340 (3)\ttotal: 3.28ms\tremaining: 4.92ms\n","4:\tlearn: 0.0897767\ttest: 0.0889537\ttest1: 0.0745677\tbest: 0.0745677 (4)\ttotal: 3.88ms\tremaining: 3.88ms\n","5:\tlearn: 0.0865827\ttest: 0.0856964\ttest1: 0.0719457\tbest: 0.0719457 (5)\ttotal: 4.45ms\tremaining: 2.97ms\n","6:\tlearn: 0.0841170\ttest: 0.0832103\ttest1: 0.0701198\tbest: 0.0701198 (6)\ttotal: 5.12ms\tremaining: 2.19ms\n","7:\tlearn: 0.0815028\ttest: 0.0805478\ttest1: 0.0678913\tbest: 0.0678913 (7)\ttotal: 5.71ms\tremaining: 1.43ms\n","8:\tlearn: 0.0789781\ttest: 0.0779648\ttest1: 0.0660863\tbest: 0.0660863 (8)\ttotal: 6.3ms\tremaining: 700us\n","9:\tlearn: 0.0771297\ttest: 0.0760972\ttest1: 0.0644188\tbest: 0.0644188 (9)\ttotal: 6.9ms\tremaining: 0us\n","\n","bestTest = 0.06441884896\n","bestIteration = 9\n","\n","loss____: 0.0780323701950658\n","\u001b[32m[I 2025-02-19 06:42:32,122]\u001b[0m Trial 0 finished with value: 0.0780323701950658 and parameters: {}. Best is trial 0 with value: 0.0780323701950658.\u001b[0m\n","-------\n","0.0780323701950658\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1025384\ttest: 0.1020466\ttest1: 0.1013059\tbest: 0.1013059 (0)\ttotal: 1.11ms\tremaining: 9.96ms\n","1:\tlearn: 0.0993704\ttest: 0.0982268\ttest1: 0.0971054\tbest: 0.0971054 (1)\ttotal: 1.85ms\tremaining: 7.41ms\n","2:\tlearn: 0.0960265\ttest: 0.0946152\ttest1: 0.0931408\tbest: 0.0931408 (2)\ttotal: 2.79ms\tremaining: 6.5ms\n","3:\tlearn: 0.0929230\ttest: 0.0912182\ttest1: 0.0893442\tbest: 0.0893442 (3)\ttotal: 3.59ms\tremaining: 5.39ms\n","4:\tlearn: 0.0905644\ttest: 0.0883208\ttest1: 0.0861954\tbest: 0.0861954 (4)\ttotal: 4.22ms\tremaining: 4.22ms\n","5:\tlearn: 0.0876306\ttest: 0.0851561\ttest1: 0.0828885\tbest: 0.0828885 (5)\ttotal: 4.87ms\tremaining: 3.25ms\n","6:\tlearn: 0.0855687\ttest: 0.0829070\ttest1: 0.0802323\tbest: 0.0802323 (6)\ttotal: 5.65ms\tremaining: 2.42ms\n","7:\tlearn: 0.0832828\ttest: 0.0802630\ttest1: 0.0776075\tbest: 0.0776075 (7)\ttotal: 6.28ms\tremaining: 1.57ms\n","8:\tlearn: 0.0809105\ttest: 0.0778429\ttest1: 0.0745927\tbest: 0.0745927 (8)\ttotal: 6.88ms\tremaining: 764us\n","9:\tlearn: 0.0791106\ttest: 0.0759626\ttest1: 0.0721895\tbest: 0.0721895 (9)\ttotal: 7.5ms\tremaining: 0us\n","\n","bestTest = 0.07218946509\n","bestIteration = 9\n","\n","loss____: 0.07359022342049387\n","\u001b[32m[I 2025-02-19 06:42:32,198]\u001b[0m Trial 0 finished with value: 0.07359022342049387 and parameters: {}. Best is trial 0 with value: 0.07359022342049387.\u001b[0m\n","-------\n","0.07359022342049387\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:32,278]\u001b[0m Trial 6 finished with value: 0.07530417452660454 and parameters: {'max_depth': 3, 'learning_rate': 0.06097839109531514, 'l2_leaf_reg': 0.3, 'subsample': 0.8}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_7 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_8 strat...\n","\u001b[32m[I 2025-02-19 06:42:32,330]\u001b[0m A new study created in memory with name: no-name-9494a52f-790a-48bf-a7dd-cf598d59a105\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:32,330]\u001b[0m A new study created in memory with name: no-name-56d9b3db-2cf7-4b9c-9dd2-5fc433626744\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:32,331]\u001b[0m A new study created in memory with name: no-name-4415f424-b480-4007-8337-26d5cc5c883f\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'learning_rate': 0.011128853174905732, 'l2_leaf_reg': 0.6000000000000001, 'subsample': 0.6}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1050208\ttest: 0.1049437\ttest1: 0.1015834\tbest: 0.1015834 (0)\ttotal: 1.04ms\tremaining: 9.37ms\n","1:\tlearn: 0.1044311\ttest: 0.1042071\ttest1: 0.1008473\tbest: 0.1008473 (1)\ttotal: 1.66ms\tremaining: 6.66ms\n","2:\tlearn: 0.1037874\ttest: 0.1034793\ttest1: 0.1001731\tbest: 0.1001731 (2)\ttotal: 2.42ms\tremaining: 5.64ms\n","3:\tlearn: 0.1031121\ttest: 0.1027499\ttest1: 0.0995012\tbest: 0.0995012 (3)\ttotal: 3ms\tremaining: 4.5ms\n","4:\tlearn: 0.1025226\ttest: 0.1020983\ttest1: 0.0987931\tbest: 0.0987931 (4)\ttotal: 3.61ms\tremaining: 3.61ms\n","5:\tlearn: 0.1018657\ttest: 0.1014063\ttest1: 0.0980992\tbest: 0.0980992 (5)\ttotal: 4.19ms\tremaining: 2.79ms\n","6:\tlearn: 0.1013604\ttest: 0.1008561\ttest1: 0.0975477\tbest: 0.0975477 (6)\ttotal: 4.85ms\tremaining: 2.08ms\n","7:\tlearn: 0.1007531\ttest: 0.1001621\ttest1: 0.0969067\tbest: 0.0969067 (7)\ttotal: 5.7ms\tremaining: 1.43ms\n","8:\tlearn: 0.1001603\ttest: 0.0995249\ttest1: 0.0964208\tbest: 0.0964208 (8)\ttotal: 6.44ms\tremaining: 715us\n","9:\tlearn: 0.0995575\ttest: 0.0988478\ttest1: 0.0957996\tbest: 0.0957996 (9)\ttotal: 7.11ms\tremaining: 0us\n","\n","bestTest = 0.09579964745\n","bestIteration = 9\n","\n","loss____: 0.09673187027064545\n","\u001b[32m[I 2025-02-19 06:42:32,521]\u001b[0m Trial 0 finished with value: 0.09673187027064545 and parameters: {}. Best is trial 0 with value: 0.09673187027064545.\u001b[0m\n","-------\n","0.09673187027064545\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1050849\ttest: 0.1050010\ttest1: 0.1007891\tbest: 0.1007891 (0)\ttotal: 962us\tremaining: 8.66ms\n","1:\tlearn: 0.1043799\ttest: 0.1042519\ttest1: 0.1001156\tbest: 0.1001156 (1)\ttotal: 1.61ms\tremaining: 6.42ms\n","2:\tlearn: 0.1036774\ttest: 0.1035016\ttest1: 0.0994426\tbest: 0.0994426 (2)\ttotal: 2.3ms\tremaining: 5.38ms\n","3:\tlearn: 0.1030829\ttest: 0.1028455\ttest1: 0.0987971\tbest: 0.0987971 (3)\ttotal: 2.94ms\tremaining: 4.42ms\n","4:\tlearn: 0.1023992\ttest: 0.1021433\ttest1: 0.0981013\tbest: 0.0981013 (4)\ttotal: 3.52ms\tremaining: 3.52ms\n","5:\tlearn: 0.1016831\ttest: 0.1014085\ttest1: 0.0974025\tbest: 0.0974025 (5)\ttotal: 4.12ms\tremaining: 2.75ms\n","6:\tlearn: 0.1010803\ttest: 0.1007884\ttest1: 0.0968607\tbest: 0.0968607 (6)\ttotal: 4.86ms\tremaining: 2.08ms\n","7:\tlearn: 0.1004612\ttest: 0.1001066\ttest1: 0.0961837\tbest: 0.0961837 (7)\ttotal: 5.44ms\tremaining: 1.36ms\n","8:\tlearn: 0.0998089\ttest: 0.0994307\ttest1: 0.0954849\tbest: 0.0954849 (8)\ttotal: 6ms\tremaining: 666us\n","9:\tlearn: 0.0991575\ttest: 0.0987309\ttest1: 0.0948521\tbest: 0.0948521 (9)\ttotal: 6.64ms\tremaining: 0us\n","\n","bestTest = 0.09485214815\n","bestIteration = 9\n","\n","loss____: 0.10318317940669723\n","\u001b[32m[I 2025-02-19 06:42:32,597]\u001b[0m Trial 0 finished with value: 0.10318317940669723 and parameters: {}. Best is trial 0 with value: 0.10318317940669723.\u001b[0m\n","-------\n","0.10318317940669723\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1041088\ttest: 0.1040889\ttest1: 0.1094009\tbest: 0.1094009 (0)\ttotal: 950us\tremaining: 8.56ms\n","1:\tlearn: 0.1034627\ttest: 0.1034024\ttest1: 0.1086344\tbest: 0.1086344 (1)\ttotal: 1.58ms\tremaining: 6.34ms\n","2:\tlearn: 0.1027845\ttest: 0.1026809\ttest1: 0.1078371\tbest: 0.1078371 (2)\ttotal: 2.32ms\tremaining: 5.41ms\n","3:\tlearn: 0.1021285\ttest: 0.1020015\ttest1: 0.1070766\tbest: 0.1070766 (3)\ttotal: 2.94ms\tremaining: 4.41ms\n","4:\tlearn: 0.1014795\ttest: 0.1013269\ttest1: 0.1063203\tbest: 0.1063203 (4)\ttotal: 3.53ms\tremaining: 3.53ms\n","5:\tlearn: 0.1008318\ttest: 0.1006064\ttest1: 0.1054753\tbest: 0.1054753 (5)\ttotal: 4.09ms\tremaining: 2.73ms\n","6:\tlearn: 0.1001760\ttest: 0.0998861\ttest1: 0.1046700\tbest: 0.1046700 (6)\ttotal: 4.64ms\tremaining: 1.99ms\n","7:\tlearn: 0.0995856\ttest: 0.0992462\ttest1: 0.1039557\tbest: 0.1039557 (7)\ttotal: 5.2ms\tremaining: 1.3ms\n","8:\tlearn: 0.0989930\ttest: 0.0986251\ttest1: 0.1032530\tbest: 0.1032530 (8)\ttotal: 5.8ms\tremaining: 644us\n","9:\tlearn: 0.0984685\ttest: 0.0980840\ttest1: 0.1026470\tbest: 0.1026470 (9)\ttotal: 6.43ms\tremaining: 0us\n","\n","bestTest = 0.102647045\n","bestIteration = 9\n","\n","loss____: 0.09523597121334665\n","\u001b[32m[I 2025-02-19 06:42:32,672]\u001b[0m Trial 0 finished with value: 0.09523597121334665 and parameters: {}. Best is trial 0 with value: 0.09523597121334665.\u001b[0m\n","-------\n","0.09523597121334665\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:32,762]\u001b[0m Trial 7 finished with value: 0.09838367363022978 and parameters: {'max_depth': 3, 'learning_rate': 0.011128853174905732, 'l2_leaf_reg': 0.6000000000000001, 'subsample': 0.6}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_8 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_9 strat...\n","\u001b[32m[I 2025-02-19 06:42:32,806]\u001b[0m A new study created in memory with name: no-name-ed6db7eb-05f8-40b3-9c54-42471885b15b\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:32,806]\u001b[0m A new study created in memory with name: no-name-512ef6c6-21bc-42c2-94ec-9dceeac23248\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:32,807]\u001b[0m A new study created in memory with name: no-name-143f56f9-a276-4670-ba60-4ccf6b258fc4\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'learning_rate': 0.08889667907018929, 'l2_leaf_reg': 0.7, 'subsample': 1.0}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.0981318\ttest: 0.0978660\ttest1: 0.1197509\tbest: 0.1197509 (0)\ttotal: 784us\tremaining: 7.06ms\n","1:\tlearn: 0.0937813\ttest: 0.0931361\ttest1: 0.1138999\tbest: 0.1138999 (1)\ttotal: 1.28ms\tremaining: 5.12ms\n","2:\tlearn: 0.0902698\ttest: 0.0892450\ttest1: 0.1088099\tbest: 0.1088099 (2)\ttotal: 1.93ms\tremaining: 4.5ms\n","3:\tlearn: 0.0877798\ttest: 0.0861685\ttest1: 0.1051886\tbest: 0.1051886 (3)\ttotal: 2.46ms\tremaining: 3.69ms\n","4:\tlearn: 0.0841748\ttest: 0.0824402\ttest1: 0.1007512\tbest: 0.1007512 (4)\ttotal: 3.73ms\tremaining: 3.73ms\n","5:\tlearn: 0.0810099\ttest: 0.0791977\ttest1: 0.0964753\tbest: 0.0964753 (5)\ttotal: 4.2ms\tremaining: 2.8ms\n","6:\tlearn: 0.0782874\ttest: 0.0763612\ttest1: 0.0927048\tbest: 0.0927048 (6)\ttotal: 4.65ms\tremaining: 1.99ms\n","7:\tlearn: 0.0758901\ttest: 0.0738621\ttest1: 0.0893766\tbest: 0.0893766 (7)\ttotal: 5.18ms\tremaining: 1.29ms\n","8:\tlearn: 0.0737691\ttest: 0.0716515\ttest1: 0.0865514\tbest: 0.0865514 (8)\ttotal: 5.58ms\tremaining: 620us\n","9:\tlearn: 0.0713338\ttest: 0.0691804\ttest1: 0.0834541\tbest: 0.0834541 (9)\ttotal: 6.04ms\tremaining: 0us\n","\n","bestTest = 0.08345414661\n","bestIteration = 9\n","\n","loss____: 0.07044439172888314\n","\u001b[32m[I 2025-02-19 06:42:32,993]\u001b[0m Trial 0 finished with value: 0.07044439172888314 and parameters: {}. Best is trial 0 with value: 0.07044439172888314.\u001b[0m\n","-------\n","0.07044439172888314\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1018582\ttest: 0.1017600\ttest1: 0.0931100\tbest: 0.0931100 (0)\ttotal: 978us\tremaining: 8.8ms\n","1:\tlearn: 0.0973800\ttest: 0.0970427\ttest1: 0.0888877\tbest: 0.0888877 (1)\ttotal: 1.45ms\tremaining: 5.81ms\n","2:\tlearn: 0.0932056\ttest: 0.0926668\ttest1: 0.0851327\tbest: 0.0851327 (2)\ttotal: 2.12ms\tremaining: 4.95ms\n","3:\tlearn: 0.0893581\ttest: 0.0887250\ttest1: 0.0817316\tbest: 0.0817316 (3)\ttotal: 2.75ms\tremaining: 4.12ms\n","4:\tlearn: 0.0854900\ttest: 0.0847030\ttest1: 0.0789375\tbest: 0.0789375 (4)\ttotal: 3.15ms\tremaining: 3.15ms\n","5:\tlearn: 0.0823306\ttest: 0.0815487\ttest1: 0.0766819\tbest: 0.0766819 (5)\ttotal: 3.57ms\tremaining: 2.38ms\n","6:\tlearn: 0.0795022\ttest: 0.0786512\ttest1: 0.0739532\tbest: 0.0739532 (6)\ttotal: 3.98ms\tremaining: 1.71ms\n","7:\tlearn: 0.0767129\ttest: 0.0755395\ttest1: 0.0713957\tbest: 0.0713957 (7)\ttotal: 4.41ms\tremaining: 1.1ms\n","8:\tlearn: 0.0743844\ttest: 0.0731796\ttest1: 0.0692061\tbest: 0.0692061 (8)\ttotal: 4.85ms\tremaining: 538us\n","9:\tlearn: 0.0722058\ttest: 0.0707472\ttest1: 0.0675146\tbest: 0.0675146 (9)\ttotal: 5.29ms\tremaining: 0us\n","\n","bestTest = 0.06751461058\n","bestIteration = 9\n","\n","loss____: 0.07316948203375556\n","\u001b[32m[I 2025-02-19 06:42:33,059]\u001b[0m Trial 0 finished with value: 0.07316948203375556 and parameters: {}. Best is trial 0 with value: 0.07316948203375556.\u001b[0m\n","-------\n","0.07316948203375556\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.0999374\ttest: 0.0994063\ttest1: 0.1040673\tbest: 0.1040673 (0)\ttotal: 836us\tremaining: 7.53ms\n","1:\tlearn: 0.0955991\ttest: 0.0948707\ttest1: 0.1002318\tbest: 0.1002318 (1)\ttotal: 1.31ms\tremaining: 5.23ms\n","2:\tlearn: 0.0914802\ttest: 0.0906111\ttest1: 0.0961717\tbest: 0.0961717 (2)\ttotal: 1.88ms\tremaining: 4.4ms\n","3:\tlearn: 0.0878711\ttest: 0.0868774\ttest1: 0.0925293\tbest: 0.0925293 (3)\ttotal: 2.39ms\tremaining: 3.58ms\n","4:\tlearn: 0.0839269\ttest: 0.0827524\ttest1: 0.0883357\tbest: 0.0883357 (4)\ttotal: 2.82ms\tremaining: 2.82ms\n","5:\tlearn: 0.0808103\ttest: 0.0793592\ttest1: 0.0850702\tbest: 0.0850702 (5)\ttotal: 3.24ms\tremaining: 2.16ms\n","6:\tlearn: 0.0776997\ttest: 0.0761577\ttest1: 0.0816115\tbest: 0.0816115 (6)\ttotal: 3.66ms\tremaining: 1.57ms\n","7:\tlearn: 0.0752093\ttest: 0.0735737\ttest1: 0.0792935\tbest: 0.0792935 (7)\ttotal: 4.06ms\tremaining: 1.01ms\n","8:\tlearn: 0.0731031\ttest: 0.0713885\ttest1: 0.0772287\tbest: 0.0772287 (8)\ttotal: 4.51ms\tremaining: 500us\n","9:\tlearn: 0.0705692\ttest: 0.0687794\ttest1: 0.0744058\tbest: 0.0744058 (9)\ttotal: 4.89ms\tremaining: 0us\n","\n","bestTest = 0.07440575052\n","bestIteration = 9\n","\n","loss____: 0.06745124695384766\n","\u001b[32m[I 2025-02-19 06:42:33,127]\u001b[0m Trial 0 finished with value: 0.06745124695384766 and parameters: {}. Best is trial 0 with value: 0.06745124695384766.\u001b[0m\n","-------\n","0.06745124695384766\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:33,232]\u001b[0m Trial 8 finished with value: 0.07035504023882878 and parameters: {'max_depth': 2, 'learning_rate': 0.08889667907018929, 'l2_leaf_reg': 0.7, 'subsample': 1.0}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_9 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_10 strat...\n","\u001b[32m[I 2025-02-19 06:42:33,280]\u001b[0m A new study created in memory with name: no-name-ad765bd3-aae5-4c92-ac9a-114ef59cf82d\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:33,280]\u001b[0m A new study created in memory with name: no-name-61d41930-4fc3-4e32-b752-d2b2d491e18b\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:33,280]\u001b[0m A new study created in memory with name: no-name-2413fa65-977e-4ded-8edc-7df05fc8fa70\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'learning_rate': 0.012521954287060391, 'l2_leaf_reg': 0.6000000000000001, 'subsample': 0.8}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1044087\ttest: 0.1043881\ttest1: 0.1058488\tbest: 0.1058488 (0)\ttotal: 905us\tremaining: 8.14ms\n","1:\tlearn: 0.1036530\ttest: 0.1036146\ttest1: 0.1051108\tbest: 0.1051108 (1)\ttotal: 1.57ms\tremaining: 6.29ms\n","2:\tlearn: 0.1029454\ttest: 0.1028866\ttest1: 0.1044117\tbest: 0.1044117 (2)\ttotal: 2.17ms\tremaining: 5.06ms\n","3:\tlearn: 0.1022293\ttest: 0.1021548\ttest1: 0.1037092\tbest: 0.1037092 (3)\ttotal: 2.68ms\tremaining: 4.02ms\n","4:\tlearn: 0.1015580\ttest: 0.1014395\ttest1: 0.1030150\tbest: 0.1030150 (4)\ttotal: 3.15ms\tremaining: 3.15ms\n","5:\tlearn: 0.1008473\ttest: 0.1007001\ttest1: 0.1022016\tbest: 0.1022016 (5)\ttotal: 3.64ms\tremaining: 2.43ms\n","6:\tlearn: 0.1002075\ttest: 0.1000769\ttest1: 0.1015834\tbest: 0.1015834 (6)\ttotal: 4.13ms\tremaining: 1.77ms\n","7:\tlearn: 0.0995708\ttest: 0.0994552\ttest1: 0.1009243\tbest: 0.1009243 (7)\ttotal: 4.61ms\tremaining: 1.15ms\n","8:\tlearn: 0.0989556\ttest: 0.0987863\ttest1: 0.1002750\tbest: 0.1002750 (8)\ttotal: 5.07ms\tremaining: 563us\n","9:\tlearn: 0.0982684\ttest: 0.0980889\ttest1: 0.0995433\tbest: 0.0995433 (9)\ttotal: 5.53ms\tremaining: 0us\n","\n","bestTest = 0.0995433134\n","bestIteration = 9\n","\n","loss____: 0.09653154097825391\n","\u001b[32m[I 2025-02-19 06:42:33,477]\u001b[0m Trial 0 finished with value: 0.09653154097825391 and parameters: {}. Best is trial 0 with value: 0.09653154097825391.\u001b[0m\n","-------\n","0.09653154097825391\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1064134\ttest: 0.1063878\ttest1: 0.0923716\tbest: 0.0923716 (0)\ttotal: 940us\tremaining: 8.46ms\n","1:\tlearn: 0.1057343\ttest: 0.1056904\ttest1: 0.0918519\tbest: 0.0918519 (1)\ttotal: 1.66ms\tremaining: 6.62ms\n","2:\tlearn: 0.1050489\ttest: 0.1049711\ttest1: 0.0913146\tbest: 0.0913146 (2)\ttotal: 2.46ms\tremaining: 5.73ms\n","3:\tlearn: 0.1043812\ttest: 0.1042771\ttest1: 0.0907963\tbest: 0.0907963 (3)\ttotal: 3.09ms\tremaining: 4.63ms\n","4:\tlearn: 0.1036348\ttest: 0.1035102\ttest1: 0.0902703\tbest: 0.0902703 (4)\ttotal: 3.71ms\tremaining: 3.71ms\n","5:\tlearn: 0.1029085\ttest: 0.1027638\ttest1: 0.0897824\tbest: 0.0897824 (5)\ttotal: 4.37ms\tremaining: 2.91ms\n","6:\tlearn: 0.1022274\ttest: 0.1020658\ttest1: 0.0892641\tbest: 0.0892641 (6)\ttotal: 4.94ms\tremaining: 2.12ms\n","7:\tlearn: 0.1016423\ttest: 0.1014138\ttest1: 0.0887144\tbest: 0.0887144 (7)\ttotal: 5.42ms\tremaining: 1.35ms\n","8:\tlearn: 0.1012565\ttest: 0.1010190\ttest1: 0.0883450\tbest: 0.0883450 (8)\ttotal: 5.97ms\tremaining: 663us\n","9:\tlearn: 0.1005747\ttest: 0.1003161\ttest1: 0.0878893\tbest: 0.0878893 (9)\ttotal: 6.45ms\tremaining: 0us\n","\n","bestTest = 0.08788934497\n","bestIteration = 9\n","\n","loss____: 0.10363151819732806\n","\u001b[32m[I 2025-02-19 06:42:33,554]\u001b[0m Trial 0 finished with value: 0.10363151819732806 and parameters: {}. Best is trial 0 with value: 0.10363151819732806.\u001b[0m\n","-------\n","0.10363151819732806\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1051138\ttest: 0.1050932\ttest1: 0.1036933\tbest: 0.1036933 (0)\ttotal: 927us\tremaining: 8.35ms\n","1:\tlearn: 0.1044666\ttest: 0.1043700\ttest1: 0.1029806\tbest: 0.1029806 (1)\ttotal: 1.54ms\tremaining: 6.16ms\n","2:\tlearn: 0.1037606\ttest: 0.1036325\ttest1: 0.1022016\tbest: 0.1022016 (2)\ttotal: 2.08ms\tremaining: 4.86ms\n","3:\tlearn: 0.1031272\ttest: 0.1029168\ttest1: 0.1015144\tbest: 0.1015144 (3)\ttotal: 2.66ms\tremaining: 3.99ms\n","4:\tlearn: 0.1024205\ttest: 0.1021983\ttest1: 0.1007761\tbest: 0.1007761 (4)\ttotal: 3.45ms\tremaining: 3.45ms\n","5:\tlearn: 0.1017338\ttest: 0.1014937\ttest1: 0.1000318\tbest: 0.1000318 (5)\ttotal: 4.08ms\tremaining: 2.72ms\n","6:\tlearn: 0.1010864\ttest: 0.1008329\ttest1: 0.0993600\tbest: 0.0993600 (6)\ttotal: 4.6ms\tremaining: 1.97ms\n","7:\tlearn: 0.1004990\ttest: 0.1001679\ttest1: 0.0987117\tbest: 0.0987117 (7)\ttotal: 5.11ms\tremaining: 1.28ms\n","8:\tlearn: 0.0998702\ttest: 0.0994712\ttest1: 0.0980194\tbest: 0.0980194 (8)\ttotal: 5.64ms\tremaining: 627us\n","9:\tlearn: 0.0992069\ttest: 0.0987964\ttest1: 0.0972847\tbest: 0.0972847 (9)\ttotal: 6.17ms\tremaining: 0us\n","\n","bestTest = 0.0972846575\n","bestIteration = 9\n","\n","loss____: 0.0952082427960912\n","\u001b[32m[I 2025-02-19 06:42:33,631]\u001b[0m Trial 0 finished with value: 0.0952082427960912 and parameters: {}. Best is trial 0 with value: 0.0952082427960912.\u001b[0m\n","-------\n","0.0952082427960912\n","-------\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:33,708]\u001b[0m Trial 9 finished with value: 0.09845710065722439 and parameters: {'max_depth': 2, 'learning_rate': 0.012521954287060391, 'l2_leaf_reg': 0.6000000000000001, 'subsample': 0.8}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_10 done...\n","\n","◎ level_1_catreg_GLOBAL_TRIAL_11 strat...\n","\u001b[32m[I 2025-02-19 06:42:33,794]\u001b[0m A new study created in memory with name: no-name-e922df7a-7861-45ff-9a00-8f82eb2de497\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:33,795]\u001b[0m A new study created in memory with name: no-name-0ae5726b-09fc-491b-a12b-22815f3ec25c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:33,795]\u001b[0m A new study created in memory with name: no-name-dc30c40b-1aca-4e83-a022-ab9e9babaad6\u001b[0m\n","catreg\n","///// now...level_1_catreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'learning_rate': 0.029739065775707358, 'l2_leaf_reg': 0.7, 'subsample': 0.9}\n","\n","///// now...level_1_catreg_local_study_1_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1029195\ttest: 0.1028281\ttest1: 0.1033260\tbest: 0.1033260 (0)\ttotal: 2.57ms\tremaining: 23.1ms\n","1:\tlearn: 0.1009920\ttest: 0.1008222\ttest1: 0.1014908\tbest: 0.1014908 (1)\ttotal: 3.98ms\tremaining: 15.9ms\n","2:\tlearn: 0.0992185\ttest: 0.0989296\ttest1: 0.0997544\tbest: 0.0997544 (2)\ttotal: 5ms\tremaining: 11.7ms\n","3:\tlearn: 0.0973888\ttest: 0.0970276\ttest1: 0.0979504\tbest: 0.0979504 (3)\ttotal: 6.06ms\tremaining: 9.1ms\n","4:\tlearn: 0.0956709\ttest: 0.0952882\ttest1: 0.0963870\tbest: 0.0963870 (4)\ttotal: 7.02ms\tremaining: 7.02ms\n","5:\tlearn: 0.0940286\ttest: 0.0935826\ttest1: 0.0948566\tbest: 0.0948566 (5)\ttotal: 7.74ms\tremaining: 5.16ms\n","6:\tlearn: 0.0924485\ttest: 0.0919796\ttest1: 0.0933236\tbest: 0.0933236 (6)\ttotal: 8.98ms\tremaining: 3.85ms\n","7:\tlearn: 0.0910618\ttest: 0.0904766\ttest1: 0.0920733\tbest: 0.0920733 (7)\ttotal: 9.62ms\tremaining: 2.4ms\n","8:\tlearn: 0.0895835\ttest: 0.0889329\ttest1: 0.0906357\tbest: 0.0906357 (8)\ttotal: 10.4ms\tremaining: 1.15ms\n","9:\tlearn: 0.0879603\ttest: 0.0871802\ttest1: 0.0890978\tbest: 0.0890978 (9)\ttotal: 11.5ms\tremaining: 0us\n","\n","bestTest = 0.08909776973\n","bestIteration = 9\n","\n","loss____: 0.08563751361433906\n","\u001b[32m[I 2025-02-19 06:42:34,053]\u001b[0m Trial 0 finished with value: 0.08563751361433906 and parameters: {}. Best is trial 0 with value: 0.08563751361433906.\u001b[0m\n","-------\n","0.08563751361433906\n","-------\n","\n","///// now...level_1_catreg_local_study_2_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1053532\ttest: 0.1052473\ttest1: 0.0862831\tbest: 0.0862831 (0)\ttotal: 1.59ms\tremaining: 14.3ms\n","1:\tlearn: 0.1036567\ttest: 0.1034685\ttest1: 0.0849073\tbest: 0.0849073 (1)\ttotal: 2.68ms\tremaining: 10.7ms\n","2:\tlearn: 0.1018114\ttest: 0.1016598\ttest1: 0.0835211\tbest: 0.0835211 (2)\ttotal: 3.77ms\tremaining: 8.79ms\n","3:\tlearn: 0.0998235\ttest: 0.0996235\ttest1: 0.0819509\tbest: 0.0819509 (3)\ttotal: 4.84ms\tremaining: 7.26ms\n","4:\tlearn: 0.0982531\ttest: 0.0977605\ttest1: 0.0805368\tbest: 0.0805368 (4)\ttotal: 5.83ms\tremaining: 5.83ms\n","5:\tlearn: 0.0966136\ttest: 0.0960623\ttest1: 0.0791884\tbest: 0.0791884 (5)\ttotal: 6.82ms\tremaining: 4.55ms\n","6:\tlearn: 0.0947104\ttest: 0.0940619\ttest1: 0.0775988\tbest: 0.0775988 (6)\ttotal: 7.79ms\tremaining: 3.34ms\n","7:\tlearn: 0.0929110\ttest: 0.0922503\ttest1: 0.0762036\tbest: 0.0762036 (7)\ttotal: 8.74ms\tremaining: 2.18ms\n","8:\tlearn: 0.0915202\ttest: 0.0907925\ttest1: 0.0749641\tbest: 0.0749641 (8)\ttotal: 9.33ms\tremaining: 1.04ms\n","9:\tlearn: 0.0901741\ttest: 0.0893438\ttest1: 0.0738729\tbest: 0.0738729 (9)\ttotal: 10.3ms\tremaining: 0us\n","\n","bestTest = 0.07387287296\n","bestIteration = 9\n","\n","loss____: 0.09173127691112179\n","\u001b[32m[I 2025-02-19 06:42:34,139]\u001b[0m Trial 0 finished with value: 0.09173127691112179 and parameters: {}. Best is trial 0 with value: 0.09173127691112179.\u001b[0m\n","-------\n","0.09173127691112179\n","-------\n","\n","///// now...level_1_catreg_local_study_3_fold_cv /////\n","★ level_1_catreg_LOCAL_TRIAL_1 strat...\n","0:\tlearn: 0.1032558\ttest: 0.1030377\ttest1: 0.1032523\tbest: 0.1032523 (0)\ttotal: 2.1ms\tremaining: 18.9ms\n","1:\tlearn: 0.1013541\ttest: 0.1010504\ttest1: 0.1015094\tbest: 0.1015094 (1)\ttotal: 3.41ms\tremaining: 13.6ms\n","2:\tlearn: 0.0993091\ttest: 0.0989282\ttest1: 0.0996518\tbest: 0.0996518 (2)\ttotal: 5.11ms\tremaining: 11.9ms\n","3:\tlearn: 0.0974786\ttest: 0.0969331\ttest1: 0.0979159\tbest: 0.0979159 (3)\ttotal: 7.42ms\tremaining: 11.1ms\n","4:\tlearn: 0.0960587\ttest: 0.0950471\ttest1: 0.0962390\tbest: 0.0962390 (4)\ttotal: 8.84ms\tremaining: 8.84ms\n","5:\tlearn: 0.0943926\ttest: 0.0932310\ttest1: 0.0946134\tbest: 0.0946134 (5)\ttotal: 10.1ms\tremaining: 6.74ms\n","6:\tlearn: 0.0925816\ttest: 0.0912939\ttest1: 0.0928289\tbest: 0.0928289 (6)\ttotal: 11.4ms\tremaining: 4.89ms\n","7:\tlearn: 0.0908453\ttest: 0.0894930\ttest1: 0.0912366\tbest: 0.0912366 (7)\ttotal: 13.2ms\tremaining: 3.3ms\n","8:\tlearn: 0.0892174\ttest: 0.0877616\ttest1: 0.0896605\tbest: 0.0896605 (8)\ttotal: 14.8ms\tremaining: 1.64ms\n","9:\tlearn: 0.0877134\ttest: 0.0861389\ttest1: 0.0881935\tbest: 0.0881935 (9)\ttotal: 16.4ms\tremaining: 0us\n","\n","bestTest = 0.08819352333\n","bestIteration = 9\n","\n","loss____: 0.08393811038916005\n","\u001b[32m[I 2025-02-19 06:42:34,234]\u001b[0m Trial 0 finished with value: 0.08393811038916005 and parameters: {}. Best is trial 0 with value: 0.08393811038916005.\u001b[0m\n","-------\n","0.08393811038916005\n","-------\n","設定したn_trials50に達することなくstudyを終了します。\n","11回目のトライアルで終了しました。\n","///// now...level_1_catreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:34,366]\u001b[0m Trial 10 finished with value: 0.08710230030487363 and parameters: {'max_depth': 5, 'learning_rate': 0.029739065775707358, 'l2_leaf_reg': 0.7, 'subsample': 0.9}. Best is trial 0 with value: 0.07027420238720218.\u001b[0m\n","catreg_TRIAL_11 done...\n","///// model with optimal parameters 訓練中... //////\n","\n","///// LV1_catreg_model_with_best_params /////\n","///// 1_fold_cv... /////\n","0:\tlearn: 0.1036813\ttest: 0.1035628\ttest1: 0.1026155\tbest: 0.1026155 (0)\ttotal: 1.71ms\tremaining: 15.4ms\n","1:\tlearn: 0.1018178\ttest: 0.1016532\ttest1: 0.1008568\tbest: 0.1008568 (1)\ttotal: 2.98ms\tremaining: 11.9ms\n","2:\tlearn: 0.0997463\ttest: 0.0995094\ttest1: 0.0987608\tbest: 0.0987608 (2)\ttotal: 4.09ms\tremaining: 9.54ms\n","3:\tlearn: 0.0978122\ttest: 0.0974477\ttest1: 0.0967841\tbest: 0.0967841 (3)\ttotal: 5.02ms\tremaining: 7.53ms\n","4:\tlearn: 0.0959427\ttest: 0.0953989\ttest1: 0.0948396\tbest: 0.0948396 (4)\ttotal: 6.87ms\tremaining: 6.87ms\n","5:\tlearn: 0.0940439\ttest: 0.0934649\ttest1: 0.0929997\tbest: 0.0929997 (5)\ttotal: 7.87ms\tremaining: 5.24ms\n","6:\tlearn: 0.0924219\ttest: 0.0918851\ttest1: 0.0914085\tbest: 0.0914085 (6)\ttotal: 8.8ms\tremaining: 3.77ms\n","7:\tlearn: 0.0908000\ttest: 0.0902100\ttest1: 0.0897884\tbest: 0.0897884 (7)\ttotal: 9.75ms\tremaining: 2.44ms\n","8:\tlearn: 0.0891427\ttest: 0.0884729\ttest1: 0.0881312\tbest: 0.0881312 (8)\ttotal: 10.8ms\tremaining: 1.2ms\n","9:\tlearn: 0.0876000\ttest: 0.0869084\ttest1: 0.0866023\tbest: 0.0866023 (9)\ttotal: 12ms\tremaining: 0us\n","\n","bestTest = 0.08660229064\n","bestIteration = 9\n","\n","///// LV1_catreg_model_with_best_params /////\n","///// 2_fold_cv... /////\n","0:\tlearn: 0.1056481\ttest: 0.1054885\ttest1: 0.0873822\tbest: 0.0873822 (0)\ttotal: 1.33ms\tremaining: 12ms\n","1:\tlearn: 0.1037248\ttest: 0.1035243\ttest1: 0.0858108\tbest: 0.0858108 (1)\ttotal: 2.53ms\tremaining: 10.1ms\n","2:\tlearn: 0.1016435\ttest: 0.1013201\ttest1: 0.0839641\tbest: 0.0839641 (2)\ttotal: 3.73ms\tremaining: 8.7ms\n","3:\tlearn: 0.0995644\ttest: 0.0991763\ttest1: 0.0821748\tbest: 0.0821748 (3)\ttotal: 5.3ms\tremaining: 7.96ms\n","4:\tlearn: 0.0977729\ttest: 0.0973753\ttest1: 0.0807925\tbest: 0.0807925 (4)\ttotal: 6.42ms\tremaining: 6.42ms\n","5:\tlearn: 0.0958702\ttest: 0.0954224\ttest1: 0.0791706\tbest: 0.0791706 (5)\ttotal: 7.37ms\tremaining: 4.92ms\n","6:\tlearn: 0.0941739\ttest: 0.0936495\ttest1: 0.0777278\tbest: 0.0777278 (6)\ttotal: 8.3ms\tremaining: 3.56ms\n","7:\tlearn: 0.0928907\ttest: 0.0922357\ttest1: 0.0764978\tbest: 0.0764978 (7)\ttotal: 8.9ms\tremaining: 2.23ms\n","8:\tlearn: 0.0912989\ttest: 0.0905217\ttest1: 0.0751617\tbest: 0.0751617 (8)\ttotal: 9.87ms\tremaining: 1.1ms\n","9:\tlearn: 0.0896499\ttest: 0.0888037\ttest1: 0.0738100\tbest: 0.0738100 (9)\ttotal: 10.8ms\tremaining: 0us\n","\n","bestTest = 0.07381001088\n","bestIteration = 9\n","\n","///// LV1_catreg_model_with_best_params /////\n","///// 3_fold_cv... /////\n","0:\tlearn: 0.1028509\ttest: 0.1027722\ttest1: 0.1037422\tbest: 0.1037422 (0)\ttotal: 1.3ms\tremaining: 11.7ms\n","1:\tlearn: 0.1010113\ttest: 0.1008584\ttest1: 0.1018920\tbest: 0.1018920 (1)\ttotal: 2.37ms\tremaining: 9.5ms\n","2:\tlearn: 0.0991196\ttest: 0.0989343\ttest1: 0.1000130\tbest: 0.1000130 (2)\ttotal: 3.21ms\tremaining: 7.49ms\n","3:\tlearn: 0.0971726\ttest: 0.0969317\ttest1: 0.0978614\tbest: 0.0978614 (3)\ttotal: 4.05ms\tremaining: 6.07ms\n","4:\tlearn: 0.0955920\ttest: 0.0952295\ttest1: 0.0961900\tbest: 0.0961900 (4)\ttotal: 4.95ms\tremaining: 4.95ms\n","5:\tlearn: 0.0940538\ttest: 0.0936189\ttest1: 0.0946265\tbest: 0.0946265 (5)\ttotal: 6.43ms\tremaining: 4.29ms\n","6:\tlearn: 0.0923080\ttest: 0.0918183\ttest1: 0.0927369\tbest: 0.0927369 (6)\ttotal: 7.29ms\tremaining: 3.12ms\n","7:\tlearn: 0.0908390\ttest: 0.0903259\ttest1: 0.0910522\tbest: 0.0910522 (7)\ttotal: 8.24ms\tremaining: 2.06ms\n","8:\tlearn: 0.0891336\ttest: 0.0885750\ttest1: 0.0892342\tbest: 0.0892342 (8)\ttotal: 9.09ms\tremaining: 1.01ms\n","9:\tlearn: 0.0876736\ttest: 0.0870719\ttest1: 0.0876134\tbest: 0.0876134 (9)\ttotal: 9.98ms\tremaining: 0us\n","\n","bestTest = 0.08761340703\n","bestIteration = 9\n","\n","[757.64378393 727.18539721 727.4134717  727.18539721 727.18539721\n"," 738.27433108 743.94583239 758.09763894 727.18539721 727.18539721\n"," 727.4134717  727.11933254 737.98019191 743.79637717 757.64860877\n"," 726.89125805 727.11933254 726.89125805 726.89125805 738.35937871\n"," 743.94583239 758.09763894 727.18539721 727.18539721 727.4134717\n"," 727.18539721 738.27433108 743.94583239 758.09763894 727.47717729\n"," 727.47717729 726.89735607 726.83365048 738.00763197 743.59408566\n"," 759.0919752  726.83365048 726.83365048 726.83365048 726.83365048\n"," 737.92258434 743.67913329 758.64294503 726.53951131 726.7675858\n"," 726.7675858  726.53951131 737.71349281 743.3595828  758.64294503\n"," 727.06172497 726.83365048 727.06172497 726.83365048 737.92258434\n"," 743.67913329 759.0919752  726.83365048 726.83365048 726.68219805\n"," 726.61849246 737.59707803 743.18353172 758.80056649 726.61849246\n"," 726.61849246 726.61849246 726.61849246 737.5120304  743.26857935\n"," 758.35153632 726.3243533  726.55242779 726.55242779 726.3243533\n"," 737.30293887 742.94902886 758.35153632 726.84656695 726.61849246\n"," 726.84656695 726.61849246 737.5120304  743.26857935 758.80056649\n"," 726.61849246 726.61849246 726.61849246 726.91027254 737.7277949\n"," 743.43739048 758.73294961 726.9207919  726.69271741 726.69271741\n"," 726.69271741 737.6351723  743.30667362 758.73294961 726.69271741\n"," 726.9207919  726.62665274 726.39857825 737.42608077 743.1572184\n"," 758.28391944 726.62665274 726.39857825 726.39857825 726.9207919\n"," 737.6351723  743.39172125 758.73294961 726.69271741 726.9207919\n"," 726.69271741 726.69271741 737.6351723  743.30667362 758.2790946\n"," 726.57050936 726.50680377 726.73487826 726.50680377 737.35821859\n"," 743.0297199  758.7213924  726.50680377 726.50680377 726.50680377\n"," 726.73487826 737.14912705 742.79521705 758.27236223 726.4407391\n"," 726.21266461 726.4407391  726.21266461 737.06407942 743.11476753\n"," 758.7213924  726.73487826 726.50680377 726.50680377 726.73487826\n"," 737.35821859 743.0297199  758.7213924  726.50680377 726.79858385\n"," 726.79858385 726.57050936 737.35821859 743.11476753 758.7213924\n"," 726.50680377 726.50680377 726.50680377 726.50680377 737.35821859\n"," 743.0297199  758.7213924  726.4407391  726.21266461 726.4407391\n"," 726.4407391  737.06407942 742.88026468 758.27236223 726.21266461\n"," 726.73487826 726.50680377 726.73487826 737.35821859 743.0297199\n"," 758.7213924  726.50680377 726.50680377 726.50680377 726.50680377\n"," 737.57398308 743.16043677 758.7213924  726.73487826 726.50680377\n"," 726.50680377 726.50680377 737.35821859 743.0297199  758.7213924\n"," 726.50680377 726.73487826 726.4407391  726.21266461 737.14912705\n"," 742.88026468 758.27236223 726.4407391  726.21266461 726.21266461\n"," 726.73487826 737.35821859 743.11476753 758.7213924  726.50680377\n"," 726.73487826 726.50680377 726.50680377 737.35821859 743.0297199\n"," 758.26753739 726.79858385 726.756423   726.69271741 726.9207919\n"," 737.6351723  743.30667362 758.73294961 726.69271741 726.69271741\n"," 726.69271741 726.69271741 737.72021994 743.1572184  758.28391944\n"," 726.62665274 726.62665274 726.39857825 726.62665274 737.34103314\n"," 743.07217076 758.73294961 726.69271741 726.9207919  726.69271741\n"," 726.69271741 737.72021994 743.30667362 758.73294961 726.69271741\n"," 726.69271741 726.98449749 726.98449749 738.26739252 742.9617329\n"," 758.42547654 727.0457677  727.0457677  727.0457677  727.0457677\n"," 738.13667565 742.9617329  758.42547654 727.27384219 726.97970303\n"," 726.75162854 726.97970303 737.92758412 742.72723005 757.97644637\n"," 726.75162854 726.75162854 727.27384219 727.0457677  738.22172329\n"," 742.9617329  758.42547654 727.27384219 727.0457677  727.0457677\n"," 727.0457677  738.13667565 743.1774974  758.30860082 727.2864983\n"," 727.51457279 727.2864983  727.2864983  738.22895319 743.33617983\n"," 758.76245583 727.2864983  727.2864983  727.51457279 727.22043363\n"," 737.93481403 743.18672461 758.31342566 726.99235914 727.22043363\n"," 726.99235914 726.99235914 738.31400083 743.33617983 758.76245583\n"," 727.2864983  727.2864983  727.51457279 727.2864983  738.22895319\n"," 743.33617983 758.76245583 727.57827838 727.57827838 727.70195063\n"," 727.63824504 738.66574756 743.68792657 757.76811957 727.63824504\n"," 727.63824504 727.63824504 727.63824504 738.58069993 743.7729742\n"," 757.3190894  727.34410587 727.57218036 727.57218036 727.34410587\n"," 738.3716084  743.45342372 757.3190894  727.86631953 727.63824504\n"," 727.86631953 727.63824504 738.58069993 743.7729742  757.76811957\n"," 727.63824504 727.63824504 727.63824504 727.93002512 738.78145088\n"," 743.75796066 757.84544073 727.68450167 727.68450167 727.68450167\n"," 727.68450167 738.65073402 743.75796066 757.84544073 727.91257616\n"," 727.61843699 727.3903625  727.61843699 738.44164249 743.52345781\n"," 757.39641056 727.3903625  727.3903625  727.91257616 727.68450167\n"," 738.73578165 743.75796066 757.84544073 727.91257616 727.68450167\n"," 727.68450167 727.68450167 738.65073402 743.97372515 757.39158572\n"," 727.2491028  727.18539721 727.4134717  727.18539721 738.27433108\n"," 743.94583239 758.09763894 727.18539721 727.18539721 727.18539721\n"," 727.4134717  738.06523955 743.71132954 757.64860877 727.11933254\n"," 726.89125805 727.11933254 726.89125805 737.98019191 744.03088003\n"," 758.09763894 727.4134717  727.18539721 727.18539721 727.4134717\n"," 738.27433108 743.94583239 758.09763894 727.18539721 727.47717729\n"," 727.47717729 726.89735607 737.92258434 743.67913329 759.0919752\n"," 726.83365048 726.83365048 726.83365048 726.83365048 737.92258434\n"," 743.59408566 759.0919752  726.7675858  726.53951131 726.7675858\n"," 726.7675858  737.62844518 743.44463043 758.64294503 726.53951131\n"," 727.06172497 726.83365048 727.06172497 737.92258434 743.59408566\n"," 759.0919752  726.83365048 726.83365048 726.83365048 726.68219805\n"," 737.5120304  743.26857935 758.80056649 726.61849246 726.61849246\n"," 726.61849246 726.61849246 737.5120304  743.18353172 758.80056649\n"," 726.55242779 726.3243533  726.55242779 726.55242779 737.21789124\n"," 743.03407649 758.35153632 726.3243533  726.84656695 726.61849246\n"," 726.84656695 737.5120304  743.18353172 758.80056649 726.61849246\n"," 726.61849246 726.61849246 726.61849246 737.7277949  743.39929621\n"," 758.2790946  726.69271741 726.9207919  726.69271741 726.69271741\n"," 737.6351723  743.30667362 758.73294961 726.69271741 726.69271741\n"," 726.9207919  726.62665274 737.34103314 743.1572184  758.28391944\n"," 726.39857825 726.62665274 726.39857825 726.39857825 737.72021994\n"," 743.30667362 758.73294961 726.69271741 726.69271741 726.9207919\n"," 726.69271741 737.6351723  743.30667362 758.73294961 726.98449749\n"," 726.57050936 726.50680377 726.73487826 737.35821859 743.0297199\n"," 758.7213924  726.50680377 726.50680377 726.50680377 726.50680377\n"," 737.44326622 742.88026468 758.27236223 726.4407391  726.4407391\n"," 726.21266461 726.4407391  737.06407942 742.79521705 758.7213924\n"," 726.50680377 726.73487826 726.50680377 726.50680377 737.44326622\n"," 743.0297199  758.7213924  726.50680377 726.50680377 726.79858385\n"," 726.79858385 737.48893545 743.0297199  758.7213924  726.50680377\n"," 726.50680377 726.50680377 726.50680377 737.35821859 743.0297199\n"," 758.7213924  726.73487826 726.4407391  726.21266461 726.4407391\n"," 737.14912705 742.79521705 758.27236223 726.21266461 726.21266461\n"," 726.73487826 726.50680377 737.44326622 743.0297199  758.7213924\n"," 726.73487826 726.50680377 726.50680377 726.50680377 737.35821859\n"," 743.2454844  758.26753739 726.50680377 726.73487826 726.50680377\n"," 726.50680377 737.35821859 743.0297199  758.7213924  726.50680377\n"," 726.50680377 726.73487826 726.4407391  737.06407942 742.88026468\n"," 758.27236223 726.21266461 726.4407391  726.21266461 726.21266461\n"," 737.44326622 743.0297199  758.7213924  726.50680377 726.50680377\n"," 726.73487826 726.50680377 737.35821859 743.0297199  758.7213924\n"," 726.79858385 726.79858385 726.756423   726.69271741 737.72021994\n"," 743.30667362 758.73294961 726.69271741 726.69271741 726.69271741\n"," 726.69271741 737.6351723  743.39172125 758.28391944 726.39857825\n"," 726.62665274 726.62665274 726.39857825 737.42608077 743.07217076\n"," 758.28391944 726.9207919  726.69271741 726.9207919  726.69271741\n"," 737.6351723  743.39172125 758.73294961 726.69271741 726.69271741\n"," 726.69271741 726.98449749 737.8509368  743.09244977 758.42547654\n"," 727.27384219 727.0457677  727.0457677  727.0457677  738.13667565\n"," 742.9617329  758.42547654 727.0457677  727.27384219 726.97970303\n"," 726.75162854 737.92758412 742.81227768 757.97644637 726.97970303\n"," 726.75162854 726.75162854 727.27384219 738.13667565 743.04678054\n"," 758.42547654 727.0457677  727.27384219 727.0457677  727.0457677\n"," 738.13667565 742.9617329  757.97162153 727.35020389 727.2864983\n"," 727.51457279 727.2864983  738.22895319 743.33617983 758.76245583\n"," 727.2864983  727.2864983  727.2864983  727.51457279 738.01986166\n"," 743.10167698 758.31342566 727.22043363 726.99235914 727.22043363\n"," 726.99235914 737.93481403 743.42122747 758.76245583 727.51457279\n"," 727.2864983  727.2864983  727.51457279 738.22895319 743.33617983\n"," 758.76245583 727.2864983  727.57827838 727.57827838 727.70195063\n"," 738.58069993 743.7729742  757.76811957 727.63824504 727.63824504\n"," 727.63824504 727.63824504 738.58069993 743.68792657 757.76811957\n"," 727.57218036 727.34410587 727.57218036 727.57218036 738.28656076\n"," 743.53847135 757.3190894  727.34410587 727.86631953 727.63824504\n"," 727.86631953 738.58069993 743.68792657 757.76811957 727.63824504\n"," 727.63824504 727.63824504 727.63824504 738.79646442 743.88867752\n"," 757.84544073 727.91257616 727.68450167 727.68450167 727.68450167\n"," 738.65073402 743.75796066 757.84544073 727.68450167 727.91257616\n"," 727.61843699 727.3903625  738.44164249 743.60850544 757.39641056\n"," 727.61843699 727.3903625  727.3903625  727.91257616 738.65073402\n"," 743.84300829 757.84544073 727.68450167 727.91257616 727.68450167\n"," 727.68450167 738.65073402 743.75796066 757.39158572 727.97628175\n"," 727.2491028  727.18539721 727.4134717  738.27433108 743.94583239\n"," 758.09763894 727.18539721 727.18539721 727.18539721 727.18539721\n"," 738.35937871 743.79637717 757.64860877 727.11933254 727.11933254\n"," 726.89125805 727.11933254 737.98019191 743.71132954 758.09763894\n"," 727.18539721 727.4134717  727.18539721 727.18539721 738.35937871\n"," 743.94583239 758.09763894 727.18539721 727.18539721 727.47717729\n"," 727.47717729 738.05330121 743.59408566 759.0919752  726.83365048\n"," 726.83365048 726.83365048 726.83365048 737.92258434 743.59408566\n"," 759.0919752  727.06172497 726.7675858  726.53951131 726.7675858\n"," 737.71349281 743.3595828  758.64294503 726.53951131 726.53951131\n"," 727.06172497 726.83365048 738.00763197 743.59408566 759.0919752\n"," 727.06172497 726.83365048 726.83365048 726.83365048 737.64274727\n"," 743.18353172 758.80056649 726.61849246 726.61849246 726.61849246\n"," 726.61849246 737.5120304  743.18353172 758.80056649 726.84656695\n"," 726.55242779 726.3243533  726.55242779 737.30293887 742.94902886\n"," 758.35153632 726.3243533  726.3243533  726.84656695 726.61849246\n"," 737.59707803 743.18353172 758.80056649 726.84656695 726.61849246\n"," 726.61849246 726.61849246 737.5120304  743.39929621 758.34671148\n"," 726.756423   726.69271741 726.9207919  726.69271741 737.6351723\n"," 743.30667362 758.73294961 726.69271741 726.69271741 726.69271741\n"," 726.9207919  737.42608077 743.07217076 758.28391944 726.62665274\n"," 726.39857825 726.62665274 726.39857825 737.34103314 743.39172125\n"," 758.73294961 726.9207919  726.69271741 726.69271741 726.9207919\n"," 737.6351723  743.30667362 758.73294961 726.69271741 726.98449749\n"," 726.57050936 726.50680377 737.44326622 743.0297199  758.7213924\n"," 726.50680377 726.50680377 726.50680377 726.50680377 737.35821859\n"," 743.11476753 758.27236223 726.21266461 726.4407391  726.4407391\n"," 726.21266461 737.14912705 742.79521705 758.27236223 726.73487826\n"," 726.50680377 726.73487826 726.50680377 737.35821859 743.11476753\n"," 758.7213924  726.50680377 726.50680377 726.50680377 726.79858385\n"," 737.57398308 743.16043677 758.7213924  726.73487826 726.50680377\n"," 726.50680377 726.50680377 737.35821859 743.0297199  758.7213924\n"," 726.50680377 726.73487826 726.4407391  726.21266461 737.14912705\n"," 742.88026468 758.27236223 726.4407391  726.21266461 726.21266461\n"," 726.73487826 737.35821859 743.11476753 758.7213924  726.50680377\n"," 726.73487826 726.50680377 726.50680377 737.35821859 743.0297199\n"," 758.26753739 726.57050936 726.50680377 726.73487826 726.50680377\n"," 737.35821859 743.0297199  758.7213924  726.50680377 726.50680377\n"," 726.50680377 726.73487826 737.14912705 742.79521705 758.27236223\n"," 726.4407391  726.21266461 726.4407391  726.21266461 737.06407942\n"," 743.11476753 758.7213924  726.73487826 726.50680377 726.50680377\n"," 726.73487826 737.35821859 743.0297199  758.7213924  726.50680377\n"," 726.79858385 726.79858385 726.756423   737.6351723  743.39172125\n"," 758.73294961 726.69271741 726.69271741 726.69271741 726.69271741\n"," 737.6351723  743.30667362 758.73294961 726.62665274 726.39857825\n"," 726.62665274 726.62665274 737.34103314 743.1572184  758.28391944\n"," 726.39857825 726.9207919  726.69271741 726.9207919  737.6351723\n"," 743.30667362 758.73294961 726.69271741 726.69271741 726.69271741\n"," 726.69271741 737.8509368  743.52243811 757.97162153 727.0457677\n"," 727.27384219 727.0457677  727.0457677  738.13667565 742.9617329\n"," 758.42547654 727.0457677  727.0457677  727.27384219 726.97970303\n"," 737.84253649 742.81227768 757.97644637 726.75162854 726.97970303\n"," 726.75162854 726.75162854 738.22172329 742.9617329  758.42547654\n"," 727.0457677  727.0457677  727.27384219 727.0457677  738.13667565]\n","\tDone!\n","\n","/////  Lv2 model 訓練中... //////\n","lgbreg を使用しています...\n","\toptimal hyperparameters using Optuna　探索中...\n","///// now... level_2 //////\n","///// search for lgbreg /////\n","\u001b[32m[I 2025-02-19 06:42:35,077]\u001b[0m A new study created in RDB with name: study_ens_level_2_lgbreg\u001b[0m\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_1 strat...\n","\u001b[32m[I 2025-02-19 06:42:35,231]\u001b[0m A new study created in memory with name: no-name-01f1bd51-a880-4728-bcf6-4aee69705232\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:35,232]\u001b[0m A new study created in memory with name: no-name-0ab160e3-e2e1-4d31-a96c-c65468dbd68e\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:35,232]\u001b[0m A new study created in memory with name: no-name-3faae272-e7ec-44d0-a6bd-54ea647e3196\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 102, 'learning_rate': 0.05395030966670229, 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.6, 'colsample_bytree': 0.5}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000057 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 690\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.317186\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4103.71\teval's l2: 5420.45\n","loss____: 0.0667297368415373\n","\u001b[32m[I 2025-02-19 06:42:35,480]\u001b[0m Trial 0 finished with value: 0.0667297368415373 and parameters: {}. Best is trial 0 with value: 0.0667297368415373.\u001b[0m\n","-------\n","0.0667297368415373\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000058 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 601\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 737.650519\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4249.32\teval's l2: 4311.35\n","loss____: 0.07243581492381464\n","\u001b[32m[I 2025-02-19 06:42:35,497]\u001b[0m Trial 0 finished with value: 0.07243581492381464 and parameters: {}. Best is trial 0 with value: 0.07243581492381464.\u001b[0m\n","-------\n","0.07243581492381464\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000046 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 610\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 738.151096\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4124.49\teval's l2: 5554.1\n","loss____: 0.06743867276213487\n","\u001b[32m[I 2025-02-19 06:42:35,509]\u001b[0m Trial 0 finished with value: 0.06743867276213487 and parameters: {}. Best is trial 0 with value: 0.06743867276213487.\u001b[0m\n","-------\n","0.06743867276213487\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:35,588]\u001b[0m Trial 0 finished with value: 0.06886807484249562 and parameters: {'max_depth': 2, 'num_leaves': 102, 'learning_rate': 0.05395030966670229, 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 0 with value: 0.06886807484249562.\u001b[0m\n","lgbreg_TRIAL_1 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_2 strat...\n","\u001b[32m[I 2025-02-19 06:42:35,629]\u001b[0m A new study created in memory with name: no-name-a6be58f5-fd27-425a-856a-06e545eb001e\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:35,629]\u001b[0m A new study created in memory with name: no-name-b84ad910-6ec6-442a-b9d5-00369e3304d4\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:35,630]\u001b[0m A new study created in memory with name: no-name-dbe9f6b9-ad8e-4fcf-9aa1-cab62b5491f5\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 5, 'num_leaves': 58, 'learning_rate': 0.051059032093947576, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 1.0, 'colsample_bytree': 0.6}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000051 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 620\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.468281\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4192.41\teval's l2: 3283.58\n","loss____: 0.06637277368008958\n","\u001b[32m[I 2025-02-19 06:42:35,842]\u001b[0m Trial 0 finished with value: 0.06637277368008958 and parameters: {}. Best is trial 0 with value: 0.06637277368008958.\u001b[0m\n","-------\n","0.06637277368008958\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000045 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 691\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 732.247982\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3874.14\teval's l2: 5725.49\n","loss____: 0.07175804518386698\n","\u001b[32m[I 2025-02-19 06:42:35,855]\u001b[0m Trial 0 finished with value: 0.07175804518386698 and parameters: {}. Best is trial 0 with value: 0.07175804518386698.\u001b[0m\n","-------\n","0.07175804518386698\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000052 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 690\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.662053\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4113.75\teval's l2: 3732.16\n","loss____: 0.06691605026441971\n","\u001b[32m[I 2025-02-19 06:42:35,869]\u001b[0m Trial 0 finished with value: 0.06691605026441971 and parameters: {}. Best is trial 0 with value: 0.06691605026441971.\u001b[0m\n","-------\n","0.06691605026441971\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:35,958]\u001b[0m Trial 1 finished with value: 0.06834895637612542 and parameters: {'max_depth': 5, 'num_leaves': 58, 'learning_rate': 0.051059032093947576, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 1.0, 'colsample_bytree': 0.6}. Best is trial 1 with value: 0.06834895637612542.\u001b[0m\n","lgbreg_TRIAL_2 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_3 strat...\n","\u001b[32m[I 2025-02-19 06:42:36,008]\u001b[0m A new study created in memory with name: no-name-2f6619df-2865-43e6-bb23-1fd3dacae59e\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:36,009]\u001b[0m A new study created in memory with name: no-name-4957896e-7b8e-4b8c-8c5c-ba9c3199d949\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:36,009]\u001b[0m A new study created in memory with name: no-name-72f9c64d-88e2-440f-8243-79928a7d892d\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 29, 'learning_rate': 0.02014847788415866, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.8}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 696\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.711649\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7048.21\teval's l2: 6587.1\n","loss____: 0.08746139857633388\n","\u001b[32m[I 2025-02-19 06:42:36,226]\u001b[0m Trial 0 finished with value: 0.08746139857633388 and parameters: {}. Best is trial 0 with value: 0.08746139857633388.\u001b[0m\n","-------\n","0.08746139857633388\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000051 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 688\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.004614\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7067.53\teval's l2: 6539.52\n","loss____: 0.09354847493378782\n","\u001b[32m[I 2025-02-19 06:42:36,239]\u001b[0m Trial 0 finished with value: 0.09354847493378782 and parameters: {}. Best is trial 0 with value: 0.09354847493378782.\u001b[0m\n","-------\n","0.09354847493378782\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000059 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 603\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.462514\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6944.52\teval's l2: 7205.42\n","loss____: 0.08675194424846693\n","\u001b[32m[I 2025-02-19 06:42:36,251]\u001b[0m Trial 0 finished with value: 0.08675194424846693 and parameters: {}. Best is trial 0 with value: 0.08675194424846693.\u001b[0m\n","-------\n","0.08675194424846693\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:36,337]\u001b[0m Trial 2 finished with value: 0.08925393925286289 and parameters: {'max_depth': 2, 'num_leaves': 29, 'learning_rate': 0.02014847788415866, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.06834895637612542.\u001b[0m\n","lgbreg_TRIAL_3 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_4 strat...\n","\u001b[32m[I 2025-02-19 06:42:36,412]\u001b[0m A new study created in memory with name: no-name-642aac5f-d658-473b-9613-f620de7330b4\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:36,413]\u001b[0m A new study created in memory with name: no-name-e2359552-63f9-4080-a5d4-bb0dd6a2f363\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:36,413]\u001b[0m A new study created in memory with name: no-name-80a59c9d-756c-4021-af54-56ef74c201e0\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 35, 'learning_rate': 0.023246728489504348, 'reg_alpha': 0.5, 'reg_lambda': 0.6000000000000001, 'subsample': 0.6, 'colsample_bytree': 0.8}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 621\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.994233\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6799.34\teval's l2: 5889.06\n","loss____: 0.08573664451105223\n","\u001b[32m[I 2025-02-19 06:42:36,610]\u001b[0m Trial 0 finished with value: 0.08573664451105223 and parameters: {}. Best is trial 0 with value: 0.08573664451105223.\u001b[0m\n","-------\n","0.08573664451105223\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000046 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 616\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.580161\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6592.46\teval's l2: 7196.11\n","loss____: 0.09170257830497412\n","\u001b[32m[I 2025-02-19 06:42:36,622]\u001b[0m Trial 0 finished with value: 0.09170257830497412 and parameters: {}. Best is trial 0 with value: 0.09170257830497412.\u001b[0m\n","-------\n","0.09170257830497412\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 690\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.782007\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 6531.02\teval's l2: 7702.64\n","loss____: 0.08472863511280128\n","\u001b[32m[I 2025-02-19 06:42:36,633]\u001b[0m Trial 0 finished with value: 0.08472863511280128 and parameters: {}. Best is trial 0 with value: 0.08472863511280128.\u001b[0m\n","-------\n","0.08472863511280128\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:36,725]\u001b[0m Trial 3 finished with value: 0.08738928597627588 and parameters: {'max_depth': 2, 'num_leaves': 35, 'learning_rate': 0.023246728489504348, 'reg_alpha': 0.5, 'reg_lambda': 0.6000000000000001, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.06834895637612542.\u001b[0m\n","lgbreg_TRIAL_4 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_5 strat...\n","\u001b[32m[I 2025-02-19 06:42:36,773]\u001b[0m A new study created in memory with name: no-name-63fa3521-f482-4ace-8bd3-14f888c6af01\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:36,773]\u001b[0m A new study created in memory with name: no-name-a48d7d22-cd6b-4ec2-9c25-b9c51716b617\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:36,773]\u001b[0m A new study created in memory with name: no-name-cb83c5ab-521f-4c01-92c4-83efff6b53ec\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 23, 'learning_rate': 0.04050837781329675, 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 1.0, 'colsample_bytree': 1.0}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000063 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 619\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 733.384083\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4413.58\teval's l2: 4835.98\n","loss____: 0.06996300494946664\n","\u001b[32m[I 2025-02-19 06:42:37,000]\u001b[0m Trial 0 finished with value: 0.06996300494946664 and parameters: {}. Best is trial 0 with value: 0.06996300494946664.\u001b[0m\n","-------\n","0.06996300494946664\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000059 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 535\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 737.222607\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4616.28\teval's l2: 3496.89\n","loss____: 0.07633280766124013\n","\u001b[32m[I 2025-02-19 06:42:37,012]\u001b[0m Trial 0 finished with value: 0.07633280766124013 and parameters: {}. Best is trial 0 with value: 0.07633280766124013.\u001b[0m\n","-------\n","0.07633280766124013\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 601\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.828143\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 4573.7\teval's l2: 3865.47\n","loss____: 0.07034729746820373\n","\u001b[32m[I 2025-02-19 06:42:37,026]\u001b[0m Trial 0 finished with value: 0.07034729746820373 and parameters: {}. Best is trial 0 with value: 0.07034729746820373.\u001b[0m\n","-------\n","0.07034729746820373\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:37,120]\u001b[0m Trial 4 finished with value: 0.0722143700263035 and parameters: {'max_depth': 3, 'num_leaves': 23, 'learning_rate': 0.04050837781329675, 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 1.0, 'colsample_bytree': 1.0}. Best is trial 1 with value: 0.06834895637612542.\u001b[0m\n","lgbreg_TRIAL_5 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_6 strat...\n","\u001b[32m[I 2025-02-19 06:42:37,175]\u001b[0m A new study created in memory with name: no-name-fa09ef57-7859-4d4b-a430-0e72197a5c4c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:37,175]\u001b[0m A new study created in memory with name: no-name-363c28f0-b9db-457e-8f95-5cf720a1535d\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:37,176]\u001b[0m A new study created in memory with name: no-name-7cfbc2db-22f1-4c91-adf5-0fefa61c8432\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'num_leaves': 35, 'learning_rate': 0.012521954287060391, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.7}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000049 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 688\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.083045\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7795.12\teval's l2: 7140.17\n","loss____: 0.09227742455364787\n","\u001b[32m[I 2025-02-19 06:42:37,444]\u001b[0m Trial 0 finished with value: 0.09227742455364787 and parameters: {}. Best is trial 0 with value: 0.09227742455364787.\u001b[0m\n","-------\n","0.09227742455364787\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000045 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 688\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.535179\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7630.16\teval's l2: 8369.72\n","loss____: 0.09931100461945254\n","\u001b[32m[I 2025-02-19 06:42:37,456]\u001b[0m Trial 0 finished with value: 0.09931100461945254 and parameters: {}. Best is trial 0 with value: 0.09931100461945254.\u001b[0m\n","-------\n","0.09931100461945254\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 620\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.499423\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7632.9\teval's l2: 8245.41\n","loss____: 0.09169811241281862\n","\u001b[32m[I 2025-02-19 06:42:37,469]\u001b[0m Trial 0 finished with value: 0.09169811241281862 and parameters: {}. Best is trial 0 with value: 0.09169811241281862.\u001b[0m\n","-------\n","0.09169811241281862\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:37,541]\u001b[0m Trial 5 finished with value: 0.09442884719530635 and parameters: {'max_depth': 4, 'num_leaves': 35, 'learning_rate': 0.012521954287060391, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.5, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 1 with value: 0.06834895637612542.\u001b[0m\n","lgbreg_TRIAL_6 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_7 strat...\n","\u001b[32m[I 2025-02-19 06:42:37,583]\u001b[0m A new study created in memory with name: no-name-5cb4ac62-6bfd-4f31-bab0-bef41133e64f\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:37,584]\u001b[0m A new study created in memory with name: no-name-5158e3dc-6571-4ef7-a847-94b81d0b62dd\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:37,584]\u001b[0m A new study created in memory with name: no-name-fba1b5d2-101c-40f4-9354-93af8c28a954\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 95, 'learning_rate': 0.01814596135349025, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.4, 'subsample': 0.8, 'colsample_bytree': 0.8}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 621\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 739.262976\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7248.26\teval's l2: 7061.21\n","loss____: 0.0898194024999943\n","\u001b[32m[I 2025-02-19 06:42:37,808]\u001b[0m Trial 0 finished with value: 0.0898194024999943 and parameters: {}. Best is trial 0 with value: 0.0898194024999943.\u001b[0m\n","-------\n","0.0898194024999943\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000046 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 692\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.679354\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7244.83\teval's l2: 6921.69\n","loss____: 0.09547875528032868\n","\u001b[32m[I 2025-02-19 06:42:37,819]\u001b[0m Trial 0 finished with value: 0.09547875528032868 and parameters: {}. Best is trial 0 with value: 0.09547875528032868.\u001b[0m\n","-------\n","0.09547875528032868\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000046 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 625\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.907728\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7237.86\teval's l2: 6898.57\n","loss____: 0.0882502189118177\n","\u001b[32m[I 2025-02-19 06:42:37,831]\u001b[0m Trial 0 finished with value: 0.0882502189118177 and parameters: {}. Best is trial 0 with value: 0.0882502189118177.\u001b[0m\n","-------\n","0.0882502189118177\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:37,909]\u001b[0m Trial 6 finished with value: 0.09118279223071357 and parameters: {'max_depth': 2, 'num_leaves': 95, 'learning_rate': 0.01814596135349025, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.4, 'subsample': 0.8, 'colsample_bytree': 0.8}. Best is trial 1 with value: 0.06834895637612542.\u001b[0m\n","lgbreg_TRIAL_7 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_8 strat...\n","\u001b[32m[I 2025-02-19 06:42:37,954]\u001b[0m A new study created in memory with name: no-name-db73bd3a-b133-4fc6-8d7d-b85fa7fb2c2a\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:37,954]\u001b[0m A new study created in memory with name: no-name-33d06c96-3bb9-4136-8775-4c402b95fce7\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:37,954]\u001b[0m A new study created in memory with name: no-name-4a22948d-5c93-4311-8e21-3c65b86a18f6\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 105, 'learning_rate': 0.05958443469672518, 'reg_alpha': 0.7, 'reg_lambda': 0.7, 'subsample': 0.8, 'colsample_bytree': 1.0}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000054 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 618\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.648212\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3471.93\teval's l2: 3614.28\n","loss____: 0.060748898465343336\n","\u001b[32m[I 2025-02-19 06:42:38,173]\u001b[0m Trial 0 finished with value: 0.060748898465343336 and parameters: {}. Best is trial 0 with value: 0.060748898465343336.\u001b[0m\n","-------\n","0.060748898465343336\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000049 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 617\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.521338\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3420.61\teval's l2: 3939.18\n","loss____: 0.06611680839255919\n","\u001b[32m[I 2025-02-19 06:42:38,187]\u001b[0m Trial 0 finished with value: 0.06611680839255919 and parameters: {}. Best is trial 0 with value: 0.06611680839255919.\u001b[0m\n","-------\n","0.06611680839255919\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000066 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 695\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.613610\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 3532.35\teval's l2: 3213.16\n","loss____: 0.06126831531008156\n","\u001b[32m[I 2025-02-19 06:42:38,201]\u001b[0m Trial 0 finished with value: 0.06126831531008156 and parameters: {}. Best is trial 0 with value: 0.06126831531008156.\u001b[0m\n","-------\n","0.06126831531008156\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:38,281]\u001b[0m Trial 7 finished with value: 0.06271134072266137 and parameters: {'max_depth': 2, 'num_leaves': 105, 'learning_rate': 0.05958443469672518, 'reg_alpha': 0.7, 'reg_lambda': 0.7, 'subsample': 0.8, 'colsample_bytree': 1.0}. Best is trial 7 with value: 0.06271134072266137.\u001b[0m\n","lgbreg_TRIAL_8 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_9 strat...\n","\u001b[32m[I 2025-02-19 06:42:38,354]\u001b[0m A new study created in memory with name: no-name-ad4c184c-0120-4962-84f7-e3114c892238\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:38,355]\u001b[0m A new study created in memory with name: no-name-9507c2a9-1fbd-4d1a-9ec3-70950bd37009\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:38,355]\u001b[0m A new study created in memory with name: no-name-9ff38f10-eec5-4de3-b9e4-371c0a9fba34\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 30, 'learning_rate': 0.011097554561103107, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.7, 'colsample_bytree': 0.9}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000049 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 690\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.294118\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 8033.49\teval's l2: 6815.97\n","loss____: 0.09345639103705909\n","\u001b[32m[I 2025-02-19 06:42:38,551]\u001b[0m Trial 0 finished with value: 0.09345639103705909 and parameters: {}. Best is trial 0 with value: 0.09345639103705909.\u001b[0m\n","-------\n","0.09345639103705909\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000051 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 693\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 733.707036\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7821.89\teval's l2: 8453.68\n","loss____: 0.09994189756055553\n","\u001b[32m[I 2025-02-19 06:42:38,563]\u001b[0m Trial 0 finished with value: 0.09994189756055553 and parameters: {}. Best is trial 0 with value: 0.09994189756055553.\u001b[0m\n","-------\n","0.09994189756055553\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 536\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.717416\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 7865.13\teval's l2: 8079.39\n","loss____: 0.09278151296314378\n","\u001b[32m[I 2025-02-19 06:42:38,575]\u001b[0m Trial 0 finished with value: 0.09278151296314378 and parameters: {}. Best is trial 0 with value: 0.09278151296314378.\u001b[0m\n","-------\n","0.09278151296314378\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:38,686]\u001b[0m Trial 8 finished with value: 0.09539326718691947 and parameters: {'max_depth': 2, 'num_leaves': 30, 'learning_rate': 0.011097554561103107, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 7 with value: 0.06271134072266137.\u001b[0m\n","lgbreg_TRIAL_9 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_10 strat...\n","\u001b[32m[I 2025-02-19 06:42:38,738]\u001b[0m A new study created in memory with name: no-name-0a20c3c3-29bf-43b2-b241-9d75e7cae94c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:38,739]\u001b[0m A new study created in memory with name: no-name-694bcc4d-f93b-4ac1-b0fd-914b95710d09\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:38,739]\u001b[0m A new study created in memory with name: no-name-53935481-1caf-4590-a9fe-ea6a637dc377\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 2, 'num_leaves': 34, 'learning_rate': 0.03488960745139221, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 0.6, 'colsample_bytree': 1.0}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000054 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 623\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.934256\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5203.93\teval's l2: 5586.87\n","loss____: 0.07574067564461007\n","\u001b[32m[I 2025-02-19 06:42:38,985]\u001b[0m Trial 0 finished with value: 0.07574067564461007 and parameters: {}. Best is trial 0 with value: 0.07574067564461007.\u001b[0m\n","-------\n","0.07574067564461007\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000051 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 693\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 733.453287\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5080.22\teval's l2: 6885.71\n","loss____: 0.08153419599097379\n","\u001b[32m[I 2025-02-19 06:42:38,997]\u001b[0m Trial 0 finished with value: 0.08153419599097379 and parameters: {}. Best is trial 0 with value: 0.08153419599097379.\u001b[0m\n","-------\n","0.08153419599097379\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000052 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 606\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.846597\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 5263.7\teval's l2: 5378.15\n","loss____: 0.07553711720252188\n","\u001b[32m[I 2025-02-19 06:42:39,009]\u001b[0m Trial 0 finished with value: 0.07553711720252188 and parameters: {}. Best is trial 0 with value: 0.07553711720252188.\u001b[0m\n","-------\n","0.07553711720252188\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:39,085]\u001b[0m Trial 9 finished with value: 0.07760399627936858 and parameters: {'max_depth': 2, 'num_leaves': 34, 'learning_rate': 0.03488960745139221, 'reg_alpha': 0.3, 'reg_lambda': 0.7, 'subsample': 0.6, 'colsample_bytree': 1.0}. Best is trial 7 with value: 0.06271134072266137.\u001b[0m\n","lgbreg_TRIAL_10 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_11 strat...\n","\u001b[32m[I 2025-02-19 06:42:39,128]\u001b[0m A new study created in memory with name: no-name-7aa15a5f-3300-48b3-acd5-77199363171f\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:39,128]\u001b[0m A new study created in memory with name: no-name-f2fef737-13a6-4157-8747-bc77adfe9cb2\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:39,128]\u001b[0m A new study created in memory with name: no-name-857ee192-54ea-43bb-a437-baf5d5e2fcc9\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 68, 'learning_rate': 0.09322601292624808, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 617\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.144175\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1786.6\teval's l2: 1363.21\n","loss____: 0.04336198778545878\n","\u001b[32m[I 2025-02-19 06:42:39,390]\u001b[0m Trial 0 finished with value: 0.04336198778545878 and parameters: {}. Best is trial 0 with value: 0.04336198778545878.\u001b[0m\n","-------\n","0.04336198778545878\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000051 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 692\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.410611\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1716.48\teval's l2: 2068.82\n","loss____: 0.04752361138654244\n","\u001b[32m[I 2025-02-19 06:42:39,402]\u001b[0m Trial 0 finished with value: 0.04752361138654244 and parameters: {}. Best is trial 0 with value: 0.04752361138654244.\u001b[0m\n","-------\n","0.04752361138654244\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 600\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.696655\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1750.32\teval's l2: 1905.23\n","loss____: 0.04437702181086822\n","\u001b[32m[I 2025-02-19 06:42:39,415]\u001b[0m Trial 0 finished with value: 0.04437702181086822 and parameters: {}. Best is trial 0 with value: 0.04437702181086822.\u001b[0m\n","-------\n","0.04437702181086822\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:39,483]\u001b[0m Trial 10 finished with value: 0.04508754032762315 and parameters: {'max_depth': 3, 'num_leaves': 68, 'learning_rate': 0.09322601292624808, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}. Best is trial 10 with value: 0.04508754032762315.\u001b[0m\n","lgbreg_TRIAL_11 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_12 strat...\n","\u001b[32m[I 2025-02-19 06:42:39,525]\u001b[0m A new study created in memory with name: no-name-22783edc-84c2-49d9-8ba6-061edf78cc2f\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:39,525]\u001b[0m A new study created in memory with name: no-name-85a27f8f-62d0-4b47-b6dc-b93a5a8e42e4\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:39,525]\u001b[0m A new study created in memory with name: no-name-140fb86f-442c-4a5a-99d8-40e6b45136e0\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 70, 'learning_rate': 0.09944100570862976, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000080 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 692\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.297578\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1574.15\teval's l2: 1788\n","loss____: 0.041772996680411296\n","\u001b[32m[I 2025-02-19 06:42:39,780]\u001b[0m Trial 0 finished with value: 0.041772996680411296 and parameters: {}. Best is trial 0 with value: 0.041772996680411296.\u001b[0m\n","-------\n","0.041772996680411296\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 615\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.910035\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1579.13\teval's l2: 1580.48\n","loss____: 0.045130113246682785\n","\u001b[32m[I 2025-02-19 06:42:39,793]\u001b[0m Trial 0 finished with value: 0.045130113246682785 and parameters: {}. Best is trial 0 with value: 0.045130113246682785.\u001b[0m\n","-------\n","0.045130113246682785\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000059 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 604\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.130334\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1527.57\teval's l2: 2103.4\n","loss____: 0.042323249733024114\n","\u001b[32m[I 2025-02-19 06:42:39,806]\u001b[0m Trial 0 finished with value: 0.042323249733024114 and parameters: {}. Best is trial 0 with value: 0.042323249733024114.\u001b[0m\n","-------\n","0.042323249733024114\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:39,868]\u001b[0m Trial 11 finished with value: 0.043075453220039396 and parameters: {'max_depth': 3, 'num_leaves': 70, 'learning_rate': 0.09944100570862976, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 1.0}. Best is trial 11 with value: 0.043075453220039396.\u001b[0m\n","lgbreg_TRIAL_12 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_13 strat...\n","\u001b[32m[I 2025-02-19 06:42:39,913]\u001b[0m A new study created in memory with name: no-name-50b5f653-4609-4404-9b0c-96a394871927\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:39,914]\u001b[0m A new study created in memory with name: no-name-c28cb6fe-20b3-4660-b965-b530231a8a94\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:39,914]\u001b[0m A new study created in memory with name: no-name-30d96cdc-01e1-4d4d-bd12-3a9bdeedf20d\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 67, 'learning_rate': 0.09890395348990107, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 624\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.467128\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1546.57\teval's l2: 2172.31\n","loss____: 0.04205929842199722\n","\u001b[32m[I 2025-02-19 06:42:40,169]\u001b[0m Trial 0 finished with value: 0.04205929842199722 and parameters: {}. Best is trial 0 with value: 0.04205929842199722.\u001b[0m\n","-------\n","0.04205929842199722\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 609\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.701269\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1591.96\teval's l2: 1482.11\n","loss____: 0.04512747315006907\n","\u001b[32m[I 2025-02-19 06:42:40,181]\u001b[0m Trial 0 finished with value: 0.04512747315006907 and parameters: {}. Best is trial 0 with value: 0.04512747315006907.\u001b[0m\n","-------\n","0.04512747315006907\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000051 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 603\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.697809\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1531\teval's l2: 2152.47\n","loss____: 0.042122735034549774\n","\u001b[32m[I 2025-02-19 06:42:40,193]\u001b[0m Trial 0 finished with value: 0.042122735034549774 and parameters: {}. Best is trial 0 with value: 0.042122735034549774.\u001b[0m\n","-------\n","0.042122735034549774\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:40,306]\u001b[0m Trial 12 finished with value: 0.04310316886887202 and parameters: {'max_depth': 3, 'num_leaves': 67, 'learning_rate': 0.09890395348990107, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 11 with value: 0.043075453220039396.\u001b[0m\n","lgbreg_TRIAL_13 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_14 strat...\n","\u001b[32m[I 2025-02-19 06:42:40,358]\u001b[0m A new study created in memory with name: no-name-9b6a58ac-d4b5-4df9-aa24-1434938fe728\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:40,359]\u001b[0m A new study created in memory with name: no-name-348f3909-05a6-4198-820e-357fa99ed179\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:40,359]\u001b[0m A new study created in memory with name: no-name-4c7bf69b-4946-4cf4-863d-86c4a6051fb0\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 4, 'num_leaves': 75, 'learning_rate': 0.0896977020441668, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000060 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 687\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 733.843137\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1835.22\teval's l2: 1654\n","loss____: 0.04443918026639204\n","\u001b[32m[I 2025-02-19 06:42:40,607]\u001b[0m Trial 0 finished with value: 0.04443918026639204 and parameters: {}. Best is trial 0 with value: 0.04443918026639204.\u001b[0m\n","-------\n","0.04443918026639204\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 621\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 734.948097\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1823.22\teval's l2: 1624.91\n","loss____: 0.048066042517901884\n","\u001b[32m[I 2025-02-19 06:42:40,622]\u001b[0m Trial 0 finished with value: 0.048066042517901884 and parameters: {}. Best is trial 0 with value: 0.048066042517901884.\u001b[0m\n","-------\n","0.048066042517901884\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000050 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 687\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 738.221453\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 1875.01\teval's l2: 1264.23\n","loss____: 0.04576034417156481\n","\u001b[32m[I 2025-02-19 06:42:40,635]\u001b[0m Trial 0 finished with value: 0.04576034417156481 and parameters: {}. Best is trial 0 with value: 0.04576034417156481.\u001b[0m\n","-------\n","0.04576034417156481\n","-------\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:40,706]\u001b[0m Trial 13 finished with value: 0.046088522318619575 and parameters: {'max_depth': 4, 'num_leaves': 75, 'learning_rate': 0.0896977020441668, 'reg_alpha': 0.7, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 11 with value: 0.043075453220039396.\u001b[0m\n","lgbreg_TRIAL_14 done...\n","\n","◎ level_2_lgbreg_GLOBAL_TRIAL_15 strat...\n","\u001b[32m[I 2025-02-19 06:42:40,750]\u001b[0m A new study created in memory with name: no-name-a86eed2d-4c77-4813-9eb1-a053239230c1\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:40,751]\u001b[0m A new study created in memory with name: no-name-781d9a84-aee5-4473-b628-b0725bbb3f2c\u001b[0m\n","\u001b[32m[I 2025-02-19 06:42:40,751]\u001b[0m A new study created in memory with name: no-name-47139a3d-4337-4c71-9407-449fea9678b1\u001b[0m\n","lgbreg\n","///// now...level_2_lgbreg_local_study_start ! /////\n","trial.params: {'max_depth': 3, 'num_leaves': 50, 'learning_rate': 0.07306871808189483, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}\n","\n","///// now...level_2_lgbreg_local_study_1_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 620\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.626298\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 2491.41\teval's l2: 2583.77\n","loss____: 0.052224546155206414\n","\u001b[32m[I 2025-02-19 06:42:40,993]\u001b[0m Trial 0 finished with value: 0.052224546155206414 and parameters: {}. Best is trial 0 with value: 0.052224546155206414.\u001b[0m\n","-------\n","0.052224546155206414\n","-------\n","\n","///// now...level_2_lgbreg_local_study_2_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000049 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 693\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 737.603230\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 2445.64\teval's l2: 2660.61\n","loss____: 0.057474377398984235\n","\u001b[32m[I 2025-02-19 06:42:41,005]\u001b[0m Trial 0 finished with value: 0.057474377398984235 and parameters: {}. Best is trial 0 with value: 0.057474377398984235.\u001b[0m\n","-------\n","0.057474377398984235\n","-------\n","\n","///// now...level_2_lgbreg_local_study_3_fold_cv /////\n","★ level_2_lgbreg_LOCAL_TRIAL_1 strat...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 595\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.838524\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 2437.18\teval's l2: 3026.05\n","loss____: 0.05281275745967327\n","\u001b[32m[I 2025-02-19 06:42:41,017]\u001b[0m Trial 0 finished with value: 0.05281275745967327 and parameters: {}. Best is trial 0 with value: 0.05281275745967327.\u001b[0m\n","-------\n","0.05281275745967327\n","-------\n","設定したn_trials50に達することなくstudyを終了します。\n","15回目のトライアルで終了しました。\n","///// now...level_2_lgbreg_local_study_all_done ! /////\n","\u001b[32m[I 2025-02-19 06:42:41,107]\u001b[0m Trial 14 finished with value: 0.054170560337954636 and parameters: {'max_depth': 3, 'num_leaves': 50, 'learning_rate': 0.07306871808189483, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.6000000000000001, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 11 with value: 0.043075453220039396.\u001b[0m\n","lgbreg_TRIAL_15 done...\n","\tmodel with optimal parameters 訓練中...\n","\n","///// LV2_lgbreg_model_with_best_params /////\n","///// 1_fold_cv... /////\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 621\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.622837\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 2447.15\teval's l2: 2899.48\n","///// LV2_lgbreg_model_with_best_params /////\n","///// 2_fold_cv... /////\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000055 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 694\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 735.226067\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 2537.22\teval's l2: 2128.52\n","///// LV2_lgbreg_model_with_best_params /////\n","///// 3_fold_cv... /////\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 534\n","[LightGBM] [Info] Number of data points in the train set: 867, number of used features: 3\n","[LightGBM] [Info] Start training from score 736.344867\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Training until validation scores don't improve for 10 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Did not meet early stopping. Best iteration is:\n","[10]\ttrain's l2: 2445.26\teval's l2: 2851.33\n","実行は成功しました！\n"]}]},{"cell_type":"code","source":["# base_path = Path('/content/drive/MyDrive/nishika/mansion_autumn_2024/')\n","run_id = 7\n","with open(base_path / f\"output/exp/exp_1/run_{run_id}/tr_df.pickle\", 'rb') as f:\n","    s_tr_df = pickle.load(f)\n","\n","with open(base_path / f\"output/exp/exp_1/run_{run_id}/test_df.pickle\", 'rb') as f:\n","    s_test_df = pickle.load(f)\n","\n","with open(base_path / f\"output/exp/exp_1/run_{run_id}/output_path.pickle\", 'rb') as f:\n","    output_path = pickle.load(f)"],"metadata":{"id":"R9ouxKJV1o9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error\n","\n","def eval_mae_score(tr_df, trgt_y):\n","    score = mean_absolute_error(tr_df[f\"{trgt_y}\"].values, tr_df[f\"{trgt_y}_pred\"].values)\n","    return score\n","\n","\n","trgt_y = \"num_sold\"\n","eval_mae_score(s_tr_df, trgt_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDLnT3Z-unsQ","executionInfo":{"status":"ok","timestamp":1737971392492,"user_tz":-540,"elapsed":804,"user":{"displayName":"金城雄太","userId":"17611660587199306335"}},"outputId":"56d92d9e-5108-4e60-adde-2fee012e1e54"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.120566126467949"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# submit"],"metadata":{"id":"NHRougI7bdkr"}},{"cell_type":"code","source":["def save_submit(output_path, trgt_df, trgt_y, row_sample_df):\n","    submit_df = row_sample_df.copy()\n","    submit_df = submit_df.drop(1, axis=1)\n","    exp_id = re.findall(\"exp_(\\d)\", str(output_path))[0]\n","    run_id = re.findall(\"run_(\\d{1,3})\", str(output_path))[0]\n","    save_path = output_path / f\"submit_exp_{exp_id}_{run_id}.csv\"\n","\n","    df = trgt_df[[\"id\", f\"{trgt_y}_pred\"]]\n","    df = df.rename(columns={\"id\": 0, f\"{trgt_y}_pred\": 1})\n","    df[0] = df[0].apply(lambda x: str(x))\n","    submit_df = pd.merge(submit_df, df, on=0, how=\"left\")\n","    submit_df[1][0] = \"num_sold\"\n","    submit_df.to_csv(save_path, encoding=\"shift_jis\", index=False, header=False)\n","    return submit_df\n","\n","save_submit(output_path, s_test_df, trgt_y, row_sample_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":768},"id":"GC5bSmLpx3BP","executionInfo":{"status":"ok","timestamp":1737973541681,"user_tz":-540,"elapsed":708,"user":{"displayName":"金城雄太","userId":"17611660587199306335"}},"outputId":"07a0fb70-e147-4d16-e4c6-8bbf4d4af5cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-47-ab8a2f8369aa>:16: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n","You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n","A typical example is when you are setting values in a column of a DataFrame, like:\n","\n","df[\"col\"][row_indexer] = value\n","\n","Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","\n","  submit_df[1][0] = \"num_sold\"\n","<ipython-input-47-ab8a2f8369aa>:16: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  submit_df[1][0] = \"num_sold\"\n","<ipython-input-47-ab8a2f8369aa>:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'num_sold' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n","  submit_df[1][0] = \"num_sold\"\n"]},{"output_type":"execute_result","data":{"text/plain":["            0            1\n","0          id     num_sold\n","1      230130   878.743408\n","2      230131   669.486145\n","3      230132   690.044434\n","4      230133   239.717728\n","...       ...          ...\n","98546  328675    303.93515\n","98547  328676  2258.800781\n","98548  328677  2027.406616\n","98549  328678    768.71698\n","98550  328679   1436.03772\n","\n","[98551 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-4d7f9234-d56c-40e2-98f1-7c382f72dba1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id</td>\n","      <td>num_sold</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>230130</td>\n","      <td>878.743408</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>230131</td>\n","      <td>669.486145</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>230132</td>\n","      <td>690.044434</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>230133</td>\n","      <td>239.717728</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>98546</th>\n","      <td>328675</td>\n","      <td>303.93515</td>\n","    </tr>\n","    <tr>\n","      <th>98547</th>\n","      <td>328676</td>\n","      <td>2258.800781</td>\n","    </tr>\n","    <tr>\n","      <th>98548</th>\n","      <td>328677</td>\n","      <td>2027.406616</td>\n","    </tr>\n","    <tr>\n","      <th>98549</th>\n","      <td>328678</td>\n","      <td>768.71698</td>\n","    </tr>\n","    <tr>\n","      <th>98550</th>\n","      <td>328679</td>\n","      <td>1436.03772</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>98551 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d7f9234-d56c-40e2-98f1-7c382f72dba1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4d7f9234-d56c-40e2-98f1-7c382f72dba1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4d7f9234-d56c-40e2-98f1-7c382f72dba1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b24d8df3-fe30-4698-9d0b-61622e61fe5e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b24d8df3-fe30-4698-9d0b-61622e61fe5e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b24d8df3-fe30-4698-9d0b-61622e61fe5e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"save_submit(output_path, s_test_df, trgt_y, row_sample_df)\",\n  \"rows\": 98551,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 98551,\n        \"samples\": [\n          \"320731\",\n          \"313682\",\n          \"287923\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 98362,\n        \"samples\": [\n          698.34521484375,\n          865.51611328125,\n          300.9200134277344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":47}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"toc_visible":true,"mount_file_id":"1tZgnue6AArq2ccwdv7M6lOoWNDKXhfIS","authorship_tag":"ABX9TyM5oqHWSz5lVX+XpKfvME41"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}